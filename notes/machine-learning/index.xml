<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on</title><link>/notes/machine-learning/</link><description>Recent content in Machine Learning on</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 06 Oct 2020 08:49:15 +0000</lastBuildDate><atom:link href="/notes/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>K-Nearest Neighbors (KNN)</title><link>/notes/machine-learning/k-nearest-neighbors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/notes/machine-learning/k-nearest-neighbors/</guid><description> A non-parametric supervised learning algorithm used for both classification and regression Steps Choose K value - typically an odd number Initialization - given point Calculate distance (Euclidean) between given point and all data points in the training set Sort the results in increasing order Type Classification - Majority Vote Regression - Average Best K value Use Cross Validation and Learning Curve K-Value overfitting/underfitting Small K - Low Bias, High Variance (Overfitting) High K - High Bias, Low Variance (Underfitting) Pros Cons Easy to understand and simple to implement Suffers from Curse of Dimensionality Suitable for small datasets Requires high memory storage</description></item><item><title>Linear Regression</title><link>/notes/machine-learning/linear-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/notes/machine-learning/linear-regression/</guid><description>A supervised learning algorithm used for regression problems Model an output variable as a linear combination of input features, finds a line (or surface) that best fits the data Formula: y_hat = W^T·X y_hat, dependent/response variable, target W^T, weights or coefficients X, independent/predictor variable(s), features Polynomial Regression: add polynomial features Assumptions Linear Relationship - a linear relationship between each predictor variable and the response variable No Multicollinearity - none of the predictor variables are highly correlated with each other Independence - each observation in the dataset is independent Homoscedasticity - residuals have constant variance at every point in the linear model Multivariate Normality - residuals of the model are normally distributed Pros Cons Highly interpretable Sensitive to outliers Fast to train Can underfit with small, high-dimensional data Approaches</description></item><item><title>Logistic Regression</title><link>/notes/machine-learning/logistic-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/notes/machine-learning/logistic-regression/</guid><description> A supervised learning algorithm used for classification problems Sigmoid Function: values range from 0 - 1 Assumptions Response Variable is Binary Independence - observations in the dataset are independent of each other No Multicollinearity - none of the predictor variables are highly correlated with each other No Extreme Outliers Linear Relationship between Explanatory Variables and Logit of Response Variable Sample Size is Sufficiently Large Pros Cons Highly interpretable Can overfit with small, high-dimensional data Applicable for multi-class predictions Multiclass Classification: One-vs-Rest (OvR) Approaches Gradient Descent Binary Cross Entropy / Log Loss Cost(hθ(x),y) = −y log(hθ(x))−(1−y) log(1−hθ(x)) Model Performance Evaluation Confusion Matrix, Accuracy, Precision, Recall, F1-Score, ROC Curve</description></item></channel></rss>