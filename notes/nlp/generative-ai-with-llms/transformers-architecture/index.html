<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-500.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><link rel=stylesheet href=/main.5a6da61787dd33a1f4266a84356b6bddb654307deead4955e92a35222a9ea082df264aa3d900cbd28a47bf139cbac4d6504566a6d330e46d959e92faa5df6f86.css integrity="sha512-Wm2mF4fdM6H0JmqENWtr3bZUMH3urUlV6So1IiqeoILfJkqj2QDL0opHvxOcusTWUEVmptMw5G2VnpL6pd9vhg==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Week 1 Part 1 - Transformers Architecture | iliyaML</title><meta name=description content="Week 1 Part 1 - Transformers Architecture"><link rel=canonical href=/notes/nlp/generative-ai-with-llms/transformers-architecture/><meta property="og:locale" content><meta property="og:type" content="article"><meta property="og:title" content="Week 1 Part 1 - Transformers Architecture"><meta property="og:description" content="Week 1 Part 1 - Transformers Architecture"><meta property="og:url" content="/notes/nlp/generative-ai-with-llms/transformers-architecture/"><meta property="og:site_name" content="iliyaML"><meta property="article:published_time" content="2023-05-23T13:59:39+01:00"><meta property="article:modified_time" content="2023-05-23T13:59:39+01:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content><meta name=twitter:creator content><meta name=twitter:title content="Week 1 Part 1 - Transformers Architecture"><meta name=twitter:description content="Week 1 Part 1 - Transformers Architecture"><meta name=twitter:card content="summary"><meta name=twitter:image:alt content="Week 1 Part 1 - Transformers Architecture"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"/#/schema/person/1","name":"","url":"/","sameAs":[],"image":{"@type":"ImageObject","@id":"/#/schema/image/1","url":"/\u003cnil\u003e","width":null,"height":null,"caption":""}},{"@type":"WebSite","@id":"/#/schema/website/1","url":"/","name":"iliyaML","description":"Hi, Iâ€™m Iliya ðŸ‘‹","publisher":{"@id":"/#/schema/person/1"}},{"@type":"WebPage","@id":"/notes/nlp/generative-ai-with-llms/transformers-architecture/","url":"/notes/nlp/generative-ai-with-llms/transformers-architecture/","name":"Week 1 Part 1 - Transformers Architecture","description":"Week 1 Part 1 - Transformers Architecture","isPartOf":{"@id":"/#/schema/website/1"},"about":{"@id":"/#/schema/person/1"},"datePublished":"2023-05-23T13:59:39CET","dateModified":"2023-05-23T13:59:39CET","breadcrumb":{"@id":"/notes/nlp/generative-ai-with-llms/transformers-architecture/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"/notes/nlp/generative-ai-with-llms/transformers-architecture/#/schema/image/2"},"inLanguage":"","potentialAction":[{"@type":"ReadAction","target":["/notes/nlp/generative-ai-with-llms/transformers-architecture/"]}]},{"@type":"BreadcrumbList","@id":"/notes/nlp/generative-ai-with-llms/transformers-architecture/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"/","url":"/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@id":"/notesnlpgenerative-ai-with-llmstransformers-architecture/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"/notes/nlp/generative-ai-with-llms/transformers-architecture/#/schema/image/2","url":null,"contentUrl":null,"caption":"Week 1 Part 1 - Transformers Architecture"}]}]}</script><meta name=theme-color content="#fff"><link rel=icon href=/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=/site.webmanifest></head><body class="notes single"><div class=sticky-top><div class=header-bar></div><header class="navbar navbar-expand-lg navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/ aria-label=iliyaML>iliyaML</a>
<button class="btn btn-link order-0 ms-auto d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasExample aria-controls=offcanvasExample><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-more-horizontal"><circle cx="12" cy="12" r="1"/><circle cx="19" cy="12" r="1"/><circle cx="5" cy="12" r="1"/></svg></button><div class="offcanvas offcanvas-start d-lg-none" tabindex=-1 id=offcanvasExample aria-labelledby=offcanvasExampleLabel><div class=header-bar></div><div class=offcanvas-header><h5 class=offcanvas-title id=offcanvasExampleLabel>Browse notes</h5><button type=button class=btn-close data-bs-dismiss=offcanvas aria-label=Close></button></div><div class=offcanvas-body><aside class="doks-sidebar mt-n3"><nav id=doks-docs-nav aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-091fa9121c047db1dd48c3e2ab5f3c91 aria-expanded=false>
Machine Learning</button><div class=collapse id=section-091fa9121c047db1dd48c3e2ab5f3c91><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/machine-learning/k-nearest-neighbors/>K-Nearest Neighbors (KNN)</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/linear-regression/>Linear Regression</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/logistic-regression/>Logistic Regression</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a68b6412b3d8a605c374d3c59e02694 aria-expanded=false>
Deep Learning</button><div class=collapse id=section-6a68b6412b3d8a605c374d3c59e02694><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/deep-learning/neural-networks/>Neural Networks</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-85587c49afa2ea2adacd4bbcdb57d064 aria-expanded=true>
NLP</button><div class="collapse show" id=section-85587c49afa2ea2adacd4bbcdb57d064><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/papers/>Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/attention-is-all-you-need/>Attention is All You Need (2017)</a></li><li><a class="docs-link rounded" href=/notes/nlp/state-of-gpt-2023/>State of GPT (2023)</a></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-d275029ac7b5f661dae7a2b707f60c0e aria-expanded=true>
Generative AI with Large Langauge Models</button><div class="collapse show" id=section-d275029ac7b5f661dae7a2b707f60c0e><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/introduction/>Introduction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-1/>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</a></li><li><a class="docs-link rounded active" href=/notes/nlp/generative-ai-with-llms/transformers-architecture/>Week 1 Part 1 - Transformers Architecture</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-2/>Week 1 Part 2 - Pre-training and Scaling Laws</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-research-papers/>Week 1 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/>Week 2 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-1/>Week 2 Part 1 - Fine-tuning LLMs with Instruction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-2/>Week 2 Part 2 - Parameter Efficient Fine-tuning</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-1/>Week 3 Part 1 - Reinforcement Learning from Human Feedback</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-2/>Week 3 Part 2 - LLM-powered Applications</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/>Week 3 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/course-conclusion/>Course Conclusion</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c33e404a441c6ba9648f88af3c68a1ca aria-expanded=false>
Statistics</button><div class=collapse id=section-c33e404a441c6ba9648f88af3c68a1ca><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/statistics/introduction/>Introduction</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-89320dbd4e0a792bc1676fde2d5bccfc aria-expanded=false>
DSA</button><div class=collapse id=section-89320dbd4e0a792bc1676fde2d5bccfc><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/dsa/data-structures-and-algorithms/>Data Structures & Algorithms</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-98bf5c1a28c1981bf8d74d4bc73ceaab aria-expanded=false>
System Design</button><div class=collapse id=section-98bf5c1a28c1981bf8d74d4bc73ceaab><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0460583622f03a52d7693094d6fa2452 aria-expanded=false>
Concepts</button><div class=collapse id=section-0460583622f03a52d7693094d6fa2452><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/concepts/basics/>Basics</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/20-concepts/>20 Concepts</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/25-golden-rules/>25 Golden Rules</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/key-steps/>Key Steps</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-1afa74da05ca145d3418aad9af510109 aria-expanded=false>
Design</button><div class=collapse id=section-1afa74da05ca145d3418aad9af510109><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/design/url-shortener/>URL Shortener</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/short-media-sharing-platform/>Short Media Sharing Platform</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/video-streaming-platform/>Video Streaming Platform</a></li></ul></div></li></ul></div></li></ul></nav></aside></div></div><button class="btn btn-menu order-2 d-block d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-end border-0 py-lg-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-lg-none"></div><div class="offcanvas-header d-lg-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>iliyaML</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body p-4 p-lg-0"><ul class="nav flex-column flex-lg-row align-items-lg-center mt-2 mt-lg-0 ms-lg-2 me-lg-auto"><li class=nav-item><a class="nav-link ps-0 py-1" href=/about/>About</a></li><li class=nav-item><a class="nav-link ps-0 py-1 active" href=/notes/nlp/generative-ai-with-llms/introduction/>Notes</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><form class="doks-search position-relative flex-grow-1 ms-lg-auto me-lg-2"><input id=search class="form-control is-search" type=search placeholder=Search... aria-label=Search... autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><hr class="text-black-50 my-4 d-lg-none"><ul class="nav flex-column flex-lg-row"><li class=nav-item><a class="nav-link social-link" href=https://github.com/iliyaML><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-lg-none">GitHub</small></a></li><li class=nav-item><a class="nav-link social-link" href=https://www.linkedin.com/in/iliya-mohamad-lokman><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-linkedin"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg><small class="ms-2 d-lg-none">LinkedIn</small></a></li></ul><hr class="text-black-50 my-4 d-lg-none"><button id=mode class="btn btn-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></div></div></nav></header></div><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-091fa9121c047db1dd48c3e2ab5f3c91 aria-expanded=false>
Machine Learning</button><div class=collapse id=section-091fa9121c047db1dd48c3e2ab5f3c91><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/machine-learning/k-nearest-neighbors/>K-Nearest Neighbors (KNN)</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/linear-regression/>Linear Regression</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/logistic-regression/>Logistic Regression</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a68b6412b3d8a605c374d3c59e02694 aria-expanded=false>
Deep Learning</button><div class=collapse id=section-6a68b6412b3d8a605c374d3c59e02694><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/deep-learning/neural-networks/>Neural Networks</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-85587c49afa2ea2adacd4bbcdb57d064 aria-expanded=true>
NLP</button><div class="collapse show" id=section-85587c49afa2ea2adacd4bbcdb57d064><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/papers/>Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/attention-is-all-you-need/>Attention is All You Need (2017)</a></li><li><a class="docs-link rounded" href=/notes/nlp/state-of-gpt-2023/>State of GPT (2023)</a></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-d275029ac7b5f661dae7a2b707f60c0e aria-expanded=true>
Generative AI with Large Langauge Models</button><div class="collapse show" id=section-d275029ac7b5f661dae7a2b707f60c0e><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/introduction/>Introduction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-1/>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</a></li><li><a class="docs-link rounded active" href=/notes/nlp/generative-ai-with-llms/transformers-architecture/>Week 1 Part 1 - Transformers Architecture</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-2/>Week 1 Part 2 - Pre-training and Scaling Laws</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-research-papers/>Week 1 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/>Week 2 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-1/>Week 2 Part 1 - Fine-tuning LLMs with Instruction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-2/>Week 2 Part 2 - Parameter Efficient Fine-tuning</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-1/>Week 3 Part 1 - Reinforcement Learning from Human Feedback</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-2/>Week 3 Part 2 - LLM-powered Applications</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/>Week 3 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/course-conclusion/>Course Conclusion</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c33e404a441c6ba9648f88af3c68a1ca aria-expanded=false>
Statistics</button><div class=collapse id=section-c33e404a441c6ba9648f88af3c68a1ca><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/statistics/introduction/>Introduction</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-89320dbd4e0a792bc1676fde2d5bccfc aria-expanded=false>
DSA</button><div class=collapse id=section-89320dbd4e0a792bc1676fde2d5bccfc><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/dsa/data-structures-and-algorithms/>Data Structures & Algorithms</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-98bf5c1a28c1981bf8d74d4bc73ceaab aria-expanded=false>
System Design</button><div class=collapse id=section-98bf5c1a28c1981bf8d74d4bc73ceaab><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0460583622f03a52d7693094d6fa2452 aria-expanded=false>
Concepts</button><div class=collapse id=section-0460583622f03a52d7693094d6fa2452><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/concepts/basics/>Basics</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/20-concepts/>20 Concepts</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/25-golden-rules/>25 Golden Rules</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/key-steps/>Key Steps</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-1afa74da05ca145d3418aad9af510109 aria-expanded=false>
Design</button><div class=collapse id=section-1afa74da05ca145d3418aad9af510109><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/design/url-shortener/>URL Shortener</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/short-media-sharing-platform/>Short Media Sharing Platform</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/video-streaming-platform/>Video Streaming Platform</a></li></ul></div></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-091fa9121c047db1dd48c3e2ab5f3c91 aria-expanded=false>
Machine Learning</button><div class=collapse id=section-091fa9121c047db1dd48c3e2ab5f3c91><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/machine-learning/k-nearest-neighbors/>K-Nearest Neighbors (KNN)</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/linear-regression/>Linear Regression</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/logistic-regression/>Logistic Regression</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a68b6412b3d8a605c374d3c59e02694 aria-expanded=false>
Deep Learning</button><div class=collapse id=section-6a68b6412b3d8a605c374d3c59e02694><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/deep-learning/neural-networks/>Neural Networks</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-85587c49afa2ea2adacd4bbcdb57d064 aria-expanded=true>
NLP</button><div class="collapse show" id=section-85587c49afa2ea2adacd4bbcdb57d064><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/papers/>Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/attention-is-all-you-need/>Attention is All You Need (2017)</a></li><li><a class="docs-link rounded" href=/notes/nlp/state-of-gpt-2023/>State of GPT (2023)</a></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-d275029ac7b5f661dae7a2b707f60c0e aria-expanded=true>
Generative AI with Large Langauge Models</button><div class="collapse show" id=section-d275029ac7b5f661dae7a2b707f60c0e><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/introduction/>Introduction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-1/>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</a></li><li><a class="docs-link rounded active" href=/notes/nlp/generative-ai-with-llms/transformers-architecture/>Week 1 Part 1 - Transformers Architecture</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-2/>Week 1 Part 2 - Pre-training and Scaling Laws</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-research-papers/>Week 1 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/>Week 2 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-1/>Week 2 Part 1 - Fine-tuning LLMs with Instruction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-2/>Week 2 Part 2 - Parameter Efficient Fine-tuning</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-1/>Week 3 Part 1 - Reinforcement Learning from Human Feedback</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-2/>Week 3 Part 2 - LLM-powered Applications</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/>Week 3 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/course-conclusion/>Course Conclusion</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c33e404a441c6ba9648f88af3c68a1ca aria-expanded=false>
Statistics</button><div class=collapse id=section-c33e404a441c6ba9648f88af3c68a1ca><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/statistics/introduction/>Introduction</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-89320dbd4e0a792bc1676fde2d5bccfc aria-expanded=false>
DSA</button><div class=collapse id=section-89320dbd4e0a792bc1676fde2d5bccfc><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/dsa/data-structures-and-algorithms/>Data Structures & Algorithms</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-98bf5c1a28c1981bf8d74d4bc73ceaab aria-expanded=false>
System Design</button><div class=collapse id=section-98bf5c1a28c1981bf8d74d4bc73ceaab><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0460583622f03a52d7693094d6fa2452 aria-expanded=false>
Concepts</button><div class=collapse id=section-0460583622f03a52d7693094d6fa2452><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/concepts/basics/>Basics</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/20-concepts/>20 Concepts</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/25-golden-rules/>25 Golden Rules</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/key-steps/>Key Steps</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-1afa74da05ca145d3418aad9af510109 aria-expanded=false>
Design</button><div class=collapse id=section-1afa74da05ca145d3418aad9af510109><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/design/url-shortener/>URL Shortener</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/short-media-sharing-platform/>Short Media Sharing Platform</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/video-streaming-platform/>Video Streaming Platform</a></li></ul></div></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#transformers-architecture>Transformers Architecture</a><ul><li><a href=#self-attention>Self-Attention</a></li><li><a href=#simplified-diagram-of-the-transformer-architecture>Simplified Diagram of the Transformer Architecture</a></li><li><a href=#tokenizer>Tokenizer</a></li><li><a href=#embedding>Embedding</a></li><li><a href=#visualization-embedding-in-3d>Visualization: Embedding in 3D</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#self-attention-layer>Self-Attention Layer</a></li><li><a href=#multi-headed-attention-layer>Multi-Headed Attention Layer</a></li><li><a href=#fully-connected-feed-forward-network>Fully-Connected Feed-Forward Network</a></li><li><a href=#softmax-layer>Softmax Layer</a></li></ul></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#transformers-architecture>Transformers Architecture</a><ul><li><a href=#self-attention>Self-Attention</a></li><li><a href=#simplified-diagram-of-the-transformer-architecture>Simplified Diagram of the Transformer Architecture</a></li><li><a href=#tokenizer>Tokenizer</a></li><li><a href=#embedding>Embedding</a></li><li><a href=#visualization-embedding-in-3d>Visualization: Embedding in 3D</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#self-attention-layer>Self-Attention Layer</a></li><li><a href=#multi-headed-attention-layer>Multi-Headed Attention Layer</a></li><li><a href=#fully-connected-feed-forward-network>Fully-Connected Feed-Forward Network</a></li><li><a href=#softmax-layer>Softmax Layer</a></li></ul></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>Week 1 Part 1 - Transformers Architecture</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#transformers-architecture>Transformers Architecture</a><ul><li><a href=#self-attention>Self-Attention</a></li><li><a href=#simplified-diagram-of-the-transformer-architecture>Simplified Diagram of the Transformer Architecture</a></li><li><a href=#tokenizer>Tokenizer</a></li><li><a href=#embedding>Embedding</a></li><li><a href=#visualization-embedding-in-3d>Visualization: Embedding in 3D</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#self-attention-layer>Self-Attention Layer</a></li><li><a href=#multi-headed-attention-layer>Multi-Headed Attention Layer</a></li><li><a href=#fully-connected-feed-forward-network>Fully-Connected Feed-Forward Network</a></li><li><a href=#softmax-layer>Softmax Layer</a></li></ul></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#transformers-architecture>Transformers Architecture</a><ul><li><a href=#self-attention>Self-Attention</a></li><li><a href=#simplified-diagram-of-the-transformer-architecture>Simplified Diagram of the Transformer Architecture</a></li><li><a href=#tokenizer>Tokenizer</a></li><li><a href=#embedding>Embedding</a></li><li><a href=#visualization-embedding-in-3d>Visualization: Embedding in 3D</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#self-attention-layer>Self-Attention Layer</a></li><li><a href=#multi-headed-attention-layer>Multi-Headed Attention Layer</a></li><li><a href=#fully-connected-feed-forward-network>Fully-Connected Feed-Forward Network</a></li><li><a href=#softmax-layer>Softmax Layer</a></li></ul></li></ul></nav></div></nav><p>These notes were developed using lectures/material/transcripts from the <a href=https://www.deeplearning.ai/courses/generative-ai-with-llms/>DeepLearning.AI & AWS - Generative AI with Large Language Models</a> course</p><h2 id=notes>Notes <a href=#notes class=anchor aria-hidden=true>#</a></h2><ul><li>Self-Attention - ability to learn the relevance and context of all the words in a sentence<ul><li>Attention weights are learned during training<ul><li>These layers reflect the importance of each word in that input sequence to all other words in the sequence</li></ul></li></ul></li><li>Transformer Architecture: Encoder-Decoder<ul><li>Translation: sequence-to-sequence task</li></ul></li><li>Tokenizer - converts words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with<ul><li>Each token is mapped to a token ID</li><li>Subword tokenization - gets the best of word-level tokenization and character-level tokenization</li><li>Note: must use the same tokenizer when generating text</li></ul></li><li>Embedding<ul><li>Each token ID is mapped to a vector</li><li>Original transformer has embedding size of 512</li><li>Weights are learned during training - these vectors learn to encode the meaning and context of individual tokens in the input sequence</li></ul></li><li>Positional Encoding<ul><li>Model processes each of the input tokens in parallel</li><li>Adding positional encoding preserves the information about the word order</li></ul></li><li>Self-Attention Layer<ul><li>Analyzes the relationships between the tokens</li><li>Attend to different parts of the input sequence to better capture the contextual dependencies between the words</li><li>Weights are learned during training - reflect the importance of each word in that input sequence to all other words in the sequence</li></ul></li><li>Multi-Headed Attention Layer<ul><li>Multiple sets of self-attention weights or heads are learned in parallel independently of each other</li></ul></li><li>Fully Connected Feed-Forward Network<ul><li>Output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary</li></ul></li><li>Softmax Layer<ul><li>Logits are normalized into a probability score for each word in the vocabulary</li><li>The most likely predicted token will have the highest score</li></ul></li><li>Text Generation Strategies</li></ul><hr><h2 id=transformers-architecture>Transformers Architecture <a href=#transformers-architecture class=anchor aria-hidden=true>#</a></h2><h3 id=self-attention>Self-Attention <a href=#self-attention class=anchor aria-hidden=true>#</a></h3><ul><li>The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence<ul><li>Not just as you see here, to each word next to its neighbor, but to every other word in a sentence and to apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input</li></ul></li><li>These attention weights are learned during LLM training</li><li>This is called self-attention and the ability to learn attention in this way across the whole input significantly approves the model&rsquo;s ability to encode language</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled.png alt=Untitled></p><h3 id=simplified-diagram-of-the-transformer-architecture>Simplified Diagram of the Transformer Architecture <a href=#simplified-diagram-of-the-transformer-architecture class=anchor aria-hidden=true>#</a></h3><ul><li>The transformer architecture is split into two distinct parts<ol><li>the encoder and</li><li>the decoder</li></ol></li><li>These components work in conjunction with each other and they share a number of similarities</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%201.png alt=Untitled></p><h3 id=tokenizer>Tokenizer <a href=#tokenizer class=anchor aria-hidden=true>#</a></h3><ul><li>Before passing texts into the model to process, you must first tokenize the words<ul><li>This converts the words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with</li></ul></li><li>Multiple tokenization methods<ol><li>Word tokenization,</li><li>Character-level tokenization,</li><li>Subword tokenization</li></ol></li><li>What&rsquo;s important is that once you&rsquo;ve selected a tokenizer to train the model, you must use the same tokenizer when you generate text</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%202.png alt=Untitled></p><h3 id=embedding>Embedding <a href=#embedding class=anchor aria-hidden=true>#</a></h3><ul><li>Now that your input is represented as numbers, you can pass it to the embedding layer</li><li>This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space</li><li>Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence</li><li>Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like word2vec use this concept</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%203.png alt=Untitled></p><ul><li>Looking back at the sample sequence, you can see that in this simple case, each word has been matched to a token ID, and each token is mapped into a vector</li><li>In the original transformer paper, the vector size was actually 512</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%204.png alt=Untitled></p><h3 id=visualization-embedding-in-3d>Visualization: Embedding in 3D <a href=#visualization-embedding-in-3d class=anchor aria-hidden=true>#</a></h3><ul><li>For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words.</li><li>You can see now how you can relate words that are located close to each other in the embedding space, and how you can calculate the distance between the words as an angle, which gives the model the ability to mathematically understand language</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%205.png alt=Untitled></p><h3 id=positional-encoding>Positional Encoding <a href=#positional-encoding class=anchor aria-hidden=true>#</a></h3><ul><li>As you add the token vectors into the base of the encoder or the decoder, you also add positional encoding</li><li>The model processes each of the input tokens in parallel</li><li>So by adding the positional encoding, you preserve the information about the word order and don&rsquo;t lose the relevance of the position of the word in the sentence.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%206.png alt=Untitled></p><h3 id=self-attention-layer>Self-Attention Layer <a href=#self-attention-layer class=anchor aria-hidden=true>#</a></h3><ul><li>Once you&rsquo;ve summed the input tokens and the positional encodings, you pass the resulting vectors to the self-attention layer</li><li>The model analyzes the relationships between the tokens in your input sequence.</li><li>As you saw earlier, this allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words.</li><li>The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%207.png alt=Untitled></p><h3 id=multi-headed-attention-layer>Multi-Headed Attention Layer <a href=#multi-headed-attention-layer class=anchor aria-hidden=true>#</a></h3><ul><li>But this does not happen just once, the transformer architecture actually has multi-headed self-attention. This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other. The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common. The intuition here is that each self-attention head will learn a different aspect of language.<ul><li>For example, one head may see the relationship between the people entities in our sentence.</li><li>Whilst another head may focus on the activity of the sentence.</li><li>Whilst yet another head may focus on some other properties such as if the words rhyme.</li></ul></li><li>Note: you don&rsquo;t dictate ahead of time what aspects of language the attention heads will learn.<ul><li>The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language.</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%208.png alt=Untitled></p><ul><li>While some attention maps are easy to interpret, like the examples discussed here, others may not be.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%209.png alt=Untitled></p><h3 id=fully-connected-feed-forward-network>Fully-Connected Feed-Forward Network <a href=#fully-connected-feed-forward-network class=anchor aria-hidden=true>#</a></h3><ul><li>Now that all of the attention weights have been applied to your input data, the output is processed through a fully-connected feed-forward network.</li><li>The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%2010.png alt=Untitled></p><h3 id=softmax-layer>Softmax Layer <a href=#softmax-layer class=anchor aria-hidden=true>#</a></h3><ul><li>Pass these logits to a final softmax layer, where they are normalized into a probability score for each word. This output includes a probability for every single word in the vocabulary, so there&rsquo;s likely to be thousands of scores here. One single token will have a score higher than the rest. This is the most likely predicted token.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-1-part-1/transformers-architecture/Untitled%2011.png alt=Untitled></p><p>There are a number of methods that you can use to vary the final selection from this vector of probabilities (Text Generation Strategies)</p><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"></div><div class="docs-navigation d-flex justify-content-between"><a href=/notes/nlp/generative-ai-with-llms/week-1-part-1/><div class="card my-1"><div class="card-body py-2">&larr; Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</div></div></a><a class=ms-auto href=/notes/nlp/generative-ai-with-llms/week-1-part-2/><div class="card my-1"><div class="card-body py-2">Week 1 Part 2 - Pre-training and Scaling Laws &rarr;</div></div></a></div></main></div></div></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-X0X8EQ5BBE"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-X0X8EQ5BBE")</script><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Â© 2023 <a class=text-muted href=/>iliyaML</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=/js/bootstrap.min.650aeec64c81d69d4c0850fc73c93da3f0330cec0a27772feed7f90f60baa5f47f1c45687d71914bdafd1c4e860d40f6dc08ede27a2f08431ff929c9a2d24621.js integrity="sha512-ZQruxkyB1p1MCFD8c8k9o/AzDOwKJ3cv7tf5D2C6pfR/HEVofXGRS9r9HE6GDUD23Ajt4novCEMf+SnJotJGIQ==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.3f0a703c54cbed82ca277187e23cf2d272da28c15ce7e33cde685d40b53d741893d5b74d35bb2d20a81f56c289084f245bdd0c9145d39d7094d3dfbc62d1326a.js integrity="sha512-PwpwPFTL7YLKJ3GH4jzy0nLaKMFc5+M83mhdQLU9dBiT1bdNNbstIKgfVsKJCE8kW90MkUXTnXCU09+8YtEyag==" crossorigin=anonymous defer></script>
<script src=/main.min.cb2e2ebbf2e4002f3117addc33582923b2b3ae5265c22944cd117ebec7abe61c170417c4506d7a0f8f0fc9053dfdf441421d53601ac467042ff3d06ec0ba07fa.js integrity="sha512-yy4uu/LkAC8xF63cM1gpI7KzrlJlwilEzRF+vser5hwXBBfEUG16D48PyQU9/fRBQh1TYBrEZwQv89BuwLoH+g==" crossorigin=anonymous defer></script>
<script src=/index.min.787928e0a959255366c62acb9b16d4f1f63336c0a411a3ec7059e7a5a0b8b9ab24a81b179d154bba8a6722d844ce2a02f730b853a8498e70bdeb258d1cbc7fde.js integrity="sha512-eHko4KlZJVNmxirLmxbU8fYzNsCkEaPscFnnpaC4uaskqBsXnRVLuopnIthEzioC9zC4U6hJjnC96yWNHLx/3g==" crossorigin=anonymous defer></script></body></html>