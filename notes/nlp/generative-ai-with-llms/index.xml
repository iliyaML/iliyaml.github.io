<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Generative AI with Large Langauge Models on</title><link>/notes/nlp/generative-ai-with-llms/</link><description>Recent content in Generative AI with Large Langauge Models on</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 06 Oct 2020 08:49:15 +0000</lastBuildDate><atom:link href="/notes/nlp/generative-ai-with-llms/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction</title><link>/notes/nlp/generative-ai-with-llms/introduction/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/introduction/</guid><description>Generative AI with Large Language Models course developed by DeepLearning.AI and AWS
Notes #
Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle Week 1 Part 1 - Transformers Architecture Week 1 Part 2 - Pre-training and Scaling Laws Week 1 - Research Papers Week 2 Part 1 - Fine-tuning LLMs with Instruction Week 2 Part 2 - Parameter Efficient Fine-tuning Week 2 - Research Papers Week 3 Part 1 - Reinforecement Learning from Human Feedback Week 3 Part 2 - LLM-powered Applications Week 3 - Research Papers Course Conclusion Credits #
The in-line diagrams are taken from Coursera, unless specified otherwise These notes were developed using lectures/material/transcripts from the course</description></item><item><title>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</title><link>/notes/nlp/generative-ai-with-llms/week-1-part-1/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/week-1-part-1/</guid><description>These notes were developed using lectures/material/transcripts from the DeepLearning.AI &amp;amp; AWS - Generative AI with Large Language Models course
Notes #
Generative AI &amp;amp; LLMs Gen AI - subset of machine learning Models learn statistical patterns in massive datasets of content LLM Use Cases and Tasks Augmenting LLMs by connecting them to external data sources or using them to invoke external APIs Provide the model with information it doesn&amp;rsquo;t know from its pre-training Text Generation Before Transformers Transformers Architecture Transformers Architecture Generating Text with Transformers How the Overall Prediction Process Works Encoder-Decoder Architecture Encoder - encodes input sequences into a deep representation of the structure and meaning of the input Decoder - working from input token triggers, uses the encoder&amp;rsquo;s contextual understanding to generate new tokens, does this in a loop until a stop condition is reached Variants Encoder-only - BERT Encoder-Decoder - BART Decoder-only - GPT, BLOOM, LLaMA Reading: Transformers: Attention is All You Need (2017) https://arxiv.</description></item><item><title>Week 1 Part 1 - Transformers Architecture</title><link>/notes/nlp/generative-ai-with-llms/transformers-architecture/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/transformers-architecture/</guid><description>These notes were developed using lectures/material/transcripts from the DeepLearning.AI &amp;amp; AWS - Generative AI with Large Language Models course
Notes #
Self-Attention - ability to learn the relevance and context of all the words in a sentence Attention weights are learned during training These layers reflect the importance of each word in that input sequence to all other words in the sequence Transformer Architecture: Encoder-Decoder Translation: sequence-to-sequence task Tokenizer - converts words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with Each token is mapped to a token ID Subword tokenization - gets the best of word-level tokenization and character-level tokenization Note: must use the same tokenizer when generating text Embedding Each token ID is mapped to a vector Original transformer has embedding size of 512 Weights are learned during training - these vectors learn to encode the meaning and context of individual tokens in the input sequence Positional Encoding Model processes each of the input tokens in parallel Adding positional encoding preserves the information about the word order Self-Attention Layer Analyzes the relationships between the tokens Attend to different parts of the input sequence to better capture the contextual dependencies between the words Weights are learned during training - reflect the importance of each word in that input sequence to all other words in the sequence Multi-Headed Attention Layer Multiple sets of self-attention weights or heads are learned in parallel independently of each other Fully Connected Feed-Forward Network Output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary Softmax Layer Logits are normalized into a probability score for each word in the vocabulary The most likely predicted token will have the highest score Text Generation Strategies Transformers Architecture #
Self-Attention #
The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence Not just as you see here, to each word next to its neighbor, but to every other word in a sentence and to apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input These attention weights are learned during LLM training This is called self-attention and the ability to learn attention in this way across the whole input significantly approves the model&amp;rsquo;s ability to encode language Simplified Diagram of the Transformer Architecture #
The transformer architecture is split into two distinct parts the encoder and the decoder These components work in conjunction with each other and they share a number of similarities Tokenizer #
Before passing texts into the model to process, you must first tokenize the words This converts the words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with Multiple tokenization methods Word tokenization, Character-level tokenization, Subword tokenization What&amp;rsquo;s important is that once you&amp;rsquo;ve selected a tokenizer to train the model, you must use the same tokenizer when you generate text Embedding #
Now that your input is represented as numbers, you can pass it to the embedding layer This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like word2vec use this concept Looking back at the sample sequence, you can see that in this simple case, each word has been matched to a token ID, and each token is mapped into a vector In the original transformer paper, the vector size was actually 512 Visualization: Embedding in 3D #
For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words.</description></item><item><title>Week 1 Part 2 - Pre-training and Scaling Laws</title><link>/notes/nlp/generative-ai-with-llms/week-1-part-2/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/week-1-part-2/</guid><description>These notes were developed using lectures/material/transcripts from the DeepLearning.AI &amp;amp; AWS - Generative AI with Large Language Models course
Notes #
Pre-training Large Language Models Stage 1 of Generative AI Project Lifecycle - Select Work with an existing Foundation model or train your own model Model cards - model details, uses, bias, risks, limitations, training details and evaluation Model Architectures and Pre-training Objectives LLMs encode a deep statistical representation of language The model’s pre-training phase - the model learns from vast amounts of unstructured textual data (petabytes of text) In this self-supervised learning step, the model internalizes the patterns and structures present in the language Transformer Variants Autoencoding (encoder only) models - trained using Masked Language Modeling (MLM), denoising objective Build bi-directional representations of the input sequence, meaning that the model has an understanding of the full context of a token and not just of the words that come before Tasks: Sentiment Analysis, Named Entity Recognition (NER), Sentence/Word/Token Classification Models: BERT, RoBERTa Autoregressive (decoder only) models - trained using Causal Language Modeling (CLM) Mask the input sequence and can only see the input tokens leading up to the token in question, the model has no knowledge of the end of the sentence.</description></item><item><title>Week 1 - Research Papers</title><link>/notes/nlp/generative-ai-with-llms/week-1-research-papers/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/week-1-research-papers/</guid><description>These notes were developed using lectures/material/transcripts from the DeepLearning.AI &amp;amp; AWS - Generative AI with Large Language Models course
Transformer Architecture #
Attention is All You Need - This paper introduced the Transformer architecture, with the core “self-attention” mechanism. This article was the foundation for LLMs. BLOOM: BigScience 176B Model - BLOOM is a open-source LLM with 176B parameters (similar to GPT-3) trained in an open and transparent way. In this paper, the authors present a detailed discussion of the dataset and process used to train the model.</description></item><item><title>Week 2 - Research Papers</title><link>/notes/nlp/generative-ai-with-llms/week-2-research-papers/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/week-2-research-papers/</guid><description>These notes were developed using lectures/material/transcripts from the DeepLearning.AI &amp;amp; AWS - Generative AI with Large Language Models course
Multi-task, Instruction Fine-tuning #
Scaling Instruction-Finetuned Language Models - Scaling fine-tuning with a focus on task, model size and chain-of-thought data. Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning - This blog (and article) explores instruction fine-tuning, which aims to make language models better at performing NLP tasks with zero-shot inference.</description></item><item><title>Week 2 Part 1 - Fine-tuning LLMs with Instruction</title><link>/notes/nlp/generative-ai-with-llms/week-2-part-1/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/week-2-part-1/</guid><description>These notes were developed using lectures/material/transcripts from the DeepLearning.AI &amp;amp; AWS - Generative AI with Large Language Models course
Notes #
Instruction Fine-Tuning
Stage 2 of Generative AI Project Lifecycle - Adapt and Align Model Prompt Engineering Fine-tuning Align with Human Feedback Evaluate Limitation of In-Context Learning (ICL) Does not work for smaller models Examples take up valuable space in the context window Pre-training Recap Train LLM using vast amount (GB, TB, PB) on unstructured textual data via self-supervised learning Fine-tuning - supervised learning process where you use a dataset (GB, TB) of labeled examples (prompt completion pairs) Full Fine-tuning - updates all of the model’s weights Requires enough memory and compute budget to store and process all the gradients, optimizers and other components that are being updated during training Instruction Fine-tuning - trains the model using examples that demonstrate how it should respond to a specific instruction Data Preparation - prepare instruction dataset Prompt template libraries Classification Text Generation Text Summarization LLM Fine-tuning Process Divide into training, validation and test splits Validation accuracy - Measure LLM performance using the validation dataset Test accuracy - Final performance evaluation using the holdout test dataset Select prompts from your training data set and pass them to the LLM, which then generates completions Compare the distribution of the completion and that of the training label and use the standard Cross-Entropy function to calculate loss between the two token distributions Use the calculated loss to update the model weights using Backpropagation This is repeated for many batches of prompt completion pairs and over several epochs, update the weights so that the model’s performance on the task improves Results in a new version of the base model called an instruct model Fine-tuning on a Single Task</description></item><item><title>Week 2 Part 2 - Parameter Efficient Fine-tuning</title><link>/notes/nlp/generative-ai-with-llms/week-2-part-2/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/week-2-part-2/</guid><description>These notes were developed using lectures/material/transcripts from the DeepLearning.AI &amp;amp; AWS - Generative AI with Large Language Models course
Notes #
Full Fine-tuning - every model weight is updated during supervised learning Requires memory not just to store the model, but various other parameters that are required during the training process Weights Optimizer States Gradients Forward Activations Temporary Memory used Can lead to Catastrophic Forgetting Results in a new version of the model for every task you train on, thus can create an expensive storage problem if you’re fine-tuning for multiple tasks Parameter Efficient Fine-tuning (PEFT) - only update a small subset of the model parameters Freezes most (if not all) of the original model weights and focus on fine-tuning a subset of (existing or additional) model parameters The number of trained parameters is much smaller than the number of parameters in the original LLM In some cases, just 15-20% of the original LLM weights More robust against Catastrophic Forgetting PEFT Saves Space and is Flexible Train only a small number of weights, which results in a much smaller footprint overall (MB, GB) New parameters are combined with the original LLM weights for inference The PEFT weights are trained for each task and can be easily swapped out for inference, allowing efficient adaptation of the original model to multiple tasks PEFT Trade-offs Parameter Efficiency Training Speed Inference Costs Model Performance Memory Efficiency Three Main Classes of PEFT Methods Selective - fine-tune only a subset of the original LLM parameters Reparameterization - work with the original LLM parameters, but reduce the number of parameters to train by creating new low rank transformations of the original network weights LoRA Additive - keep all of the LLM weights frozen and introduce new trainable components Adapters Soft Prompts - Prompt Tuning PEFT Techniques 1: LoRA (Low Rank Adaptation of LLMs) - a strategy that reduces the number of parameters to be trained during fine-tuning by freezing all of the original model parameters and then injecting a pair of rank decomposition matrices alongside the original weights Training Freeze most of the original LLM weights Inject 2 Rank Decomposition Matrices (Rank r is small simension, typically 4, 8, …, 64) Train the weights of the smaller matrices Inference Matrix multiple the low rank matrices Add to original weights Latency - little to no impact as the model has the same number of parameters as the original Apply LoRA to the Self-Attention layers since most of the parameters of LLMs are in these layers Though it can also be applied to other layers as well like FFN layers Can often perform LoRA with a single GPU and avoid the need for a distributed cluster of GPUs Train on Different Tasks Train different rank decomposition matrices for different tasks Update weights before inference The memory required to store these LoRA matrices is very small, therefore can use LoRA to train for many tasks Choosing the LoRA Rank (r) The smaller the rank, the smaller the number of trainable parameters, the bigger the savings on compute Ranks in the range of 4-32 can provide you with a good trade-off between reducing trainable parameters and preserving performance Can combine with Quantization techniques to further reduce memory footprint: QLoRA PEFT Techniques 2: Soft Prompts Trainable tokens are added to the prompt and the model weights are left frozen Lab 2 - Fine-tune a Generative AI Model For Dialogue Summarization Model: flan-t5-base Task: Dialogue Summarization Metrics: ROUGE Comparison of model performance Original model Fully Fine-tuned model PEFT model Parameter Efficient Fine-Tuning #
Training LLMs is computationally intensive Full fine-tuning requires memory not just to store the model, but various other parameters that are required during the training process.</description></item><item><title>Week 3 Part 1 - Reinforcement Learning from Human Feedback</title><link>/notes/nlp/generative-ai-with-llms/week-3-part-1/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/week-3-part-1/</guid><description>These notes were developed using lectures/material/transcripts from the DeepLearning.AI &amp;amp; AWS - Generative AI with Large Language Models course
Notes #
Aligning Models with Human Values Stage 3 of Generative AI Project Lifecycle - Adapt and Align Model Fine-tuning - train your models so that they better understand human like prompts and generate more human-like responses Models Behaving Badly Toxic language Aggressive responses Providing dangerous information These problems exist because large models are trained on vast amounts of text data from the Internet where such language appears frequently Helpfulness, Honesty and Harmlessness (HHH) - a set of principles that guide developers in the responsible use of AI Reinforcement Learning from Human Feedback (RLHF) A model fine-tuned on human feedback produced better responses than a pretrained model, an instruct fine-tuned model and even the reference human baseline RLHF fine-tunes the LLM with human feedback data, resulting in a model that is better aligned with human preferences Benefits Produces outputs that maximize usefulness and relevance to the input prompt Minimize the potential for harm Train the model to give caveats that acknowledge their limitations and to avoid toxic language and topics Potential Personalization of LLMs, where models learn the preferences of each individual user through a continuous feedback process Reinforcement Learning Overview Agent learns to make decisions to a specific goal by taking Actions in an Environment, with the objective of maximizing some notion of a cumulative Reward By iterating through this process, the Agent gradually refines its strategy or Policy to make better decisions and increase its chances of success The goal of Reinforcement Learning is for the Agent to learn the Optimal Policy for a given Environment that maximizes their Rewards Reinforcement Learning: Fine-tune LLMs Components The agent&amp;rsquo;s policy that guides the actions is the LLM Its objective is to generate text that is perceived as being aligned with the human preferences The environment is the context window of the model The state that the model considers before taking an action is the current context The action here is the act of generating text The action space is the token vocabulary The sequence of actions and states is called a rollout, instead of the term playout that&amp;rsquo;s used in classic reinforcement learning Reward Model to classify the outputs of the LLM and evaluate the degree of alignment with human preferences Central component of the Reinforcement Learning process Encodes all of the preferences that have been learned from human feedback, and it plays a central role in how the model updates its weights over many iterations Once trained, use the reward model to assess the output of the LLM and assign a reward value, which in turn gets used to update the weights off the LLM and train a new human aligned version RLHF: Obtaining Feedback from Humans Prepare Dataset for Human Feedback Find it easier to start with an instruct model (Instruct LLM) that has already been fine tuned across many tasks and has some general capabilities Use this LLM along with a prompt data set to generate a number of different responses for each prompt The prompt dataset is comprised of multiple prompts, each of which gets processed by the LLM to produce a set of completions Collect Human Feedback Collect feedback from human labelers on the completions generated by the LLM Decide what criterion you want the humans to assess the completions on.</description></item><item><title>Week 3 Part 2 - LLM-powered Applications</title><link>/notes/nlp/generative-ai-with-llms/week-3-part-2/</link><pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate><guid>/notes/nlp/generative-ai-with-llms/week-3-part-2/</guid><description>These notes were developed using lectures/material/transcripts from the DeepLearning.AI &amp;amp; AWS - Generative AI with Large Language Models course
Notes #
Model Optimizations for Deployment Generative AI Project Lifecycle Stage 4 - Application Integration Questions to ask: How the LLM will function in deployment What additional resources that the LLM may need How the model will be consumed Model Optimizations to Improve Application Performance LLM inference challenges Compute requirements Storage requirements Latency Challenge is to reduce model size while maintaining model performance Techniques to reduce model size to improve model performance during inference without impacting accuracy Distillation - uses a larger model to train a smaller model, the smaller model will be used for inference In practice, not as effective for generative decoder models, more effective for encoder-only models such as BERT that have a lot of representation redundancy Post-Training Quantization (PTQ) - transforms a model&amp;rsquo;s weights to a lower precision representation such as 16-bit floating point or 8-bit integer Pruning - eliminate weights (that are very close or equal to zero) that are not contributing much to overall model performance Generative AI Project Lifecycle Cheat Sheet Pre-training Prompt Engineering Prompt Tuning and Fine-tuning Reinforcement Learning Compression/Optimization/Deployment Using the LLM in Applications LLM-powered Applications Problems Information may be out of date Struggle with complex math problems Hallucination Ways to overcome - connect them to external data sources and applications Retrieval Augmented Generation - published by Facebook in 2020 Retriever - retrieves relevant information from an external corpus or knowledge base Encoder - encodes the query in the same format as the external documents External Data Source Data Preparation Data must fit inside context window - text documents are broken into smaller chunks, each of which will fit in the context window of LLM Data must be in format that allows its relevance to be assessed at inference time: embedding vectors - allow the LLM to identify semantically related words through measures such as Cosine Similarity Vector Database Store both the text representation as well as the embeddings Enable a fast and efficient kind of relevant search based on similarity Each vector is identified by a key Enables a citation to be included in completion Interacting with External Applications Helping LLMs Reason and Plan with Chain of Thought (CoT) It works by including a series of intermediate reasoning steps into any examples that you use for one or few-shot inference By structuring the examples in this way, you&amp;rsquo;re essentially teaching the model how to reason through the task to reach a solution It is a powerful technique that improves the ability of the model to reason through problems Program-aided Language Models (PAL) Presented by researchers at Carnegie Mellon University in 2022 Pairs an LLM with an external code interpreter to carry out calculations The method makes use of Chain-of-Thought (CoT) prompting to generate executable Python scripts ReAct: Combining Reasoning and Action ReAct - a prompting strategy that combines Chain-of-Thought (CoT) reasoning with action planning Proposed by researches at Princeton and Google in 2022 LangChain framework - provides you with modular pieces that contain the components necessary to work with LLMs Significance of Scale: Application Building Larger models are more capable Start with a larger model, collect a lot of user data in deployment, and use it to train and fine-tune a smaller model that you can switch to at a later time Reading: ReAct - Reasoning and Action https://arxiv.</description></item></channel></rss>