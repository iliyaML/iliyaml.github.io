<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-500.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><link rel=stylesheet href=/main.5a6da61787dd33a1f4266a84356b6bddb654307deead4955e92a35222a9ea082df264aa3d900cbd28a47bf139cbac4d6504566a6d330e46d959e92faa5df6f86.css integrity="sha512-Wm2mF4fdM6H0JmqENWtr3bZUMH3urUlV6So1IiqeoILfJkqj2QDL0opHvxOcusTWUEVmptMw5G2VnpL6pd9vhg==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Week 3 Part 2 - LLM-powered Applications | iliyaML</title><meta name=description content="Week 3 Part 2 - LLM-powered Applications"><link rel=canonical href=/notes/nlp/generative-ai-with-llms/week-3-part-2/><meta property="og:locale" content><meta property="og:type" content="article"><meta property="og:title" content="Week 3 Part 2 - LLM-powered Applications"><meta property="og:description" content="Week 3 Part 2 - LLM-powered Applications"><meta property="og:url" content="/notes/nlp/generative-ai-with-llms/week-3-part-2/"><meta property="og:site_name" content="iliyaML"><meta property="article:published_time" content="2023-05-23T13:59:39+01:00"><meta property="article:modified_time" content="2023-05-23T13:59:39+01:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content><meta name=twitter:creator content><meta name=twitter:title content="Week 3 Part 2 - LLM-powered Applications"><meta name=twitter:description content="Week 3 Part 2 - LLM-powered Applications"><meta name=twitter:card content="summary"><meta name=twitter:image:alt content="Week 3 Part 2 - LLM-powered Applications"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"/#/schema/person/1","name":"","url":"/","sameAs":[],"image":{"@type":"ImageObject","@id":"/#/schema/image/1","url":"/\u003cnil\u003e","width":null,"height":null,"caption":""}},{"@type":"WebSite","@id":"/#/schema/website/1","url":"/","name":"iliyaML","description":"Hi, Iâ€™m Iliya ðŸ‘‹","publisher":{"@id":"/#/schema/person/1"}},{"@type":"WebPage","@id":"/notes/nlp/generative-ai-with-llms/week-3-part-2/","url":"/notes/nlp/generative-ai-with-llms/week-3-part-2/","name":"Week 3 Part 2 - LLM-powered Applications","description":"Week 3 Part 2 - LLM-powered Applications","isPartOf":{"@id":"/#/schema/website/1"},"about":{"@id":"/#/schema/person/1"},"datePublished":"2023-05-23T13:59:39CET","dateModified":"2023-05-23T13:59:39CET","breadcrumb":{"@id":"/notes/nlp/generative-ai-with-llms/week-3-part-2/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"/notes/nlp/generative-ai-with-llms/week-3-part-2/#/schema/image/2"},"inLanguage":"","potentialAction":[{"@type":"ReadAction","target":["/notes/nlp/generative-ai-with-llms/week-3-part-2/"]}]},{"@type":"BreadcrumbList","@id":"/notes/nlp/generative-ai-with-llms/week-3-part-2/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"/","url":"/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@id":"/notesnlpgenerative-ai-with-llmsweek-3-part-2/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"/notes/nlp/generative-ai-with-llms/week-3-part-2/#/schema/image/2","url":null,"contentUrl":null,"caption":"Week 3 Part 2 - LLM-powered Applications"}]}]}</script><meta name=theme-color content="#fff"><link rel=icon href=/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=/site.webmanifest></head><body class="notes single"><div class=sticky-top><div class=header-bar></div><header class="navbar navbar-expand-lg navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/ aria-label=iliyaML>iliyaML</a>
<button class="btn btn-link order-0 ms-auto d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasExample aria-controls=offcanvasExample><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-more-horizontal"><circle cx="12" cy="12" r="1"/><circle cx="19" cy="12" r="1"/><circle cx="5" cy="12" r="1"/></svg></button><div class="offcanvas offcanvas-start d-lg-none" tabindex=-1 id=offcanvasExample aria-labelledby=offcanvasExampleLabel><div class=header-bar></div><div class=offcanvas-header><h5 class=offcanvas-title id=offcanvasExampleLabel>Browse notes</h5><button type=button class=btn-close data-bs-dismiss=offcanvas aria-label=Close></button></div><div class=offcanvas-body><aside class="doks-sidebar mt-n3"><nav id=doks-docs-nav aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-091fa9121c047db1dd48c3e2ab5f3c91 aria-expanded=false>
Machine Learning</button><div class=collapse id=section-091fa9121c047db1dd48c3e2ab5f3c91><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/machine-learning/k-nearest-neighbors/>K-Nearest Neighbors (KNN)</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/linear-regression/>Linear Regression</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/logistic-regression/>Logistic Regression</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a68b6412b3d8a605c374d3c59e02694 aria-expanded=false>
Deep Learning</button><div class=collapse id=section-6a68b6412b3d8a605c374d3c59e02694><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/deep-learning/neural-networks/>Neural Networks</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-85587c49afa2ea2adacd4bbcdb57d064 aria-expanded=true>
NLP</button><div class="collapse show" id=section-85587c49afa2ea2adacd4bbcdb57d064><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/papers/>Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/attention-is-all-you-need/>Attention is All You Need (2017)</a></li><li><a class="docs-link rounded" href=/notes/nlp/state-of-gpt-2023/>State of GPT (2023)</a></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-d275029ac7b5f661dae7a2b707f60c0e aria-expanded=true>
Generative AI with Large Langauge Models</button><div class="collapse show" id=section-d275029ac7b5f661dae7a2b707f60c0e><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/introduction/>Introduction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-1/>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/transformers-architecture/>Week 1 Part 1 - Transformers Architecture</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-2/>Week 1 Part 2 - Pre-training and Scaling Laws</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-research-papers/>Week 1 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-1/>Week 2 Part 1 - Fine-tuning LLMs with Instruction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-2/>Week 2 Part 2 - Parameter Efficient Fine-tuning</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/>Week 2 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-1/>Week 3 Part 1 - Reinforcement Learning from Human Feedback</a></li><li><a class="docs-link rounded active" href=/notes/nlp/generative-ai-with-llms/week-3-part-2/>Week 3 Part 2 - LLM-powered Applications</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/>Week 3 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/course-conclusion/>Course Conclusion</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c33e404a441c6ba9648f88af3c68a1ca aria-expanded=false>
Statistics</button><div class=collapse id=section-c33e404a441c6ba9648f88af3c68a1ca><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/statistics/introduction/>Introduction</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-89320dbd4e0a792bc1676fde2d5bccfc aria-expanded=false>
DSA</button><div class=collapse id=section-89320dbd4e0a792bc1676fde2d5bccfc><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/dsa/data-structures-and-algorithms/>Data Structures & Algorithms</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-98bf5c1a28c1981bf8d74d4bc73ceaab aria-expanded=false>
System Design</button><div class=collapse id=section-98bf5c1a28c1981bf8d74d4bc73ceaab><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0460583622f03a52d7693094d6fa2452 aria-expanded=false>
Concepts</button><div class=collapse id=section-0460583622f03a52d7693094d6fa2452><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/concepts/basics/>Basics</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/20-concepts/>20 Concepts</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/25-golden-rules/>25 Golden Rules</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/key-steps/>Key Steps</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-1afa74da05ca145d3418aad9af510109 aria-expanded=false>
Design</button><div class=collapse id=section-1afa74da05ca145d3418aad9af510109><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/design/url-shortener/>URL Shortener</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/short-media-sharing-platform/>Short Media Sharing Platform</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/video-streaming-platform/>Video Streaming Platform</a></li></ul></div></li></ul></div></li></ul></nav></aside></div></div><button class="btn btn-menu order-2 d-block d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-end border-0 py-lg-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-lg-none"></div><div class="offcanvas-header d-lg-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>iliyaML</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body p-4 p-lg-0"><ul class="nav flex-column flex-lg-row align-items-lg-center mt-2 mt-lg-0 ms-lg-2 me-lg-auto"><li class=nav-item><a class="nav-link ps-0 py-1" href=/about/>About</a></li><li class=nav-item><a class="nav-link ps-0 py-1 active" href=/notes/nlp/generative-ai-with-llms/introduction/>Notes</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><form class="doks-search position-relative flex-grow-1 ms-lg-auto me-lg-2"><input id=search class="form-control is-search" type=search placeholder=Search... aria-label=Search... autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><hr class="text-black-50 my-4 d-lg-none"><ul class="nav flex-column flex-lg-row"><li class=nav-item><a class="nav-link social-link" href=https://github.com/iliyaML><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-lg-none">GitHub</small></a></li><li class=nav-item><a class="nav-link social-link" href=https://www.linkedin.com/in/iliya-mohamad-lokman><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-linkedin"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg><small class="ms-2 d-lg-none">LinkedIn</small></a></li></ul><hr class="text-black-50 my-4 d-lg-none"><button id=mode class="btn btn-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></div></div></nav></header></div><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-091fa9121c047db1dd48c3e2ab5f3c91 aria-expanded=false>
Machine Learning</button><div class=collapse id=section-091fa9121c047db1dd48c3e2ab5f3c91><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/machine-learning/k-nearest-neighbors/>K-Nearest Neighbors (KNN)</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/linear-regression/>Linear Regression</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/logistic-regression/>Logistic Regression</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a68b6412b3d8a605c374d3c59e02694 aria-expanded=false>
Deep Learning</button><div class=collapse id=section-6a68b6412b3d8a605c374d3c59e02694><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/deep-learning/neural-networks/>Neural Networks</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-85587c49afa2ea2adacd4bbcdb57d064 aria-expanded=true>
NLP</button><div class="collapse show" id=section-85587c49afa2ea2adacd4bbcdb57d064><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/papers/>Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/attention-is-all-you-need/>Attention is All You Need (2017)</a></li><li><a class="docs-link rounded" href=/notes/nlp/state-of-gpt-2023/>State of GPT (2023)</a></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-d275029ac7b5f661dae7a2b707f60c0e aria-expanded=true>
Generative AI with Large Langauge Models</button><div class="collapse show" id=section-d275029ac7b5f661dae7a2b707f60c0e><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/introduction/>Introduction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-1/>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/transformers-architecture/>Week 1 Part 1 - Transformers Architecture</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-2/>Week 1 Part 2 - Pre-training and Scaling Laws</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-research-papers/>Week 1 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-1/>Week 2 Part 1 - Fine-tuning LLMs with Instruction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-2/>Week 2 Part 2 - Parameter Efficient Fine-tuning</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/>Week 2 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-1/>Week 3 Part 1 - Reinforcement Learning from Human Feedback</a></li><li><a class="docs-link rounded active" href=/notes/nlp/generative-ai-with-llms/week-3-part-2/>Week 3 Part 2 - LLM-powered Applications</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/>Week 3 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/course-conclusion/>Course Conclusion</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c33e404a441c6ba9648f88af3c68a1ca aria-expanded=false>
Statistics</button><div class=collapse id=section-c33e404a441c6ba9648f88af3c68a1ca><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/statistics/introduction/>Introduction</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-89320dbd4e0a792bc1676fde2d5bccfc aria-expanded=false>
DSA</button><div class=collapse id=section-89320dbd4e0a792bc1676fde2d5bccfc><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/dsa/data-structures-and-algorithms/>Data Structures & Algorithms</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-98bf5c1a28c1981bf8d74d4bc73ceaab aria-expanded=false>
System Design</button><div class=collapse id=section-98bf5c1a28c1981bf8d74d4bc73ceaab><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0460583622f03a52d7693094d6fa2452 aria-expanded=false>
Concepts</button><div class=collapse id=section-0460583622f03a52d7693094d6fa2452><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/concepts/basics/>Basics</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/20-concepts/>20 Concepts</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/25-golden-rules/>25 Golden Rules</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/key-steps/>Key Steps</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-1afa74da05ca145d3418aad9af510109 aria-expanded=false>
Design</button><div class=collapse id=section-1afa74da05ca145d3418aad9af510109><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/design/url-shortener/>URL Shortener</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/short-media-sharing-platform/>Short Media Sharing Platform</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/video-streaming-platform/>Video Streaming Platform</a></li></ul></div></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-091fa9121c047db1dd48c3e2ab5f3c91 aria-expanded=false>
Machine Learning</button><div class=collapse id=section-091fa9121c047db1dd48c3e2ab5f3c91><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/machine-learning/k-nearest-neighbors/>K-Nearest Neighbors (KNN)</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/linear-regression/>Linear Regression</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/logistic-regression/>Logistic Regression</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a68b6412b3d8a605c374d3c59e02694 aria-expanded=false>
Deep Learning</button><div class=collapse id=section-6a68b6412b3d8a605c374d3c59e02694><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/deep-learning/neural-networks/>Neural Networks</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-85587c49afa2ea2adacd4bbcdb57d064 aria-expanded=true>
NLP</button><div class="collapse show" id=section-85587c49afa2ea2adacd4bbcdb57d064><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/papers/>Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/attention-is-all-you-need/>Attention is All You Need (2017)</a></li><li><a class="docs-link rounded" href=/notes/nlp/state-of-gpt-2023/>State of GPT (2023)</a></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-d275029ac7b5f661dae7a2b707f60c0e aria-expanded=true>
Generative AI with Large Langauge Models</button><div class="collapse show" id=section-d275029ac7b5f661dae7a2b707f60c0e><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/introduction/>Introduction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-1/>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/transformers-architecture/>Week 1 Part 1 - Transformers Architecture</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-2/>Week 1 Part 2 - Pre-training and Scaling Laws</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-research-papers/>Week 1 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-1/>Week 2 Part 1 - Fine-tuning LLMs with Instruction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-2/>Week 2 Part 2 - Parameter Efficient Fine-tuning</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/>Week 2 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-1/>Week 3 Part 1 - Reinforcement Learning from Human Feedback</a></li><li><a class="docs-link rounded active" href=/notes/nlp/generative-ai-with-llms/week-3-part-2/>Week 3 Part 2 - LLM-powered Applications</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/>Week 3 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/course-conclusion/>Course Conclusion</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c33e404a441c6ba9648f88af3c68a1ca aria-expanded=false>
Statistics</button><div class=collapse id=section-c33e404a441c6ba9648f88af3c68a1ca><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/statistics/introduction/>Introduction</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-89320dbd4e0a792bc1676fde2d5bccfc aria-expanded=false>
DSA</button><div class=collapse id=section-89320dbd4e0a792bc1676fde2d5bccfc><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/dsa/data-structures-and-algorithms/>Data Structures & Algorithms</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-98bf5c1a28c1981bf8d74d4bc73ceaab aria-expanded=false>
System Design</button><div class=collapse id=section-98bf5c1a28c1981bf8d74d4bc73ceaab><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0460583622f03a52d7693094d6fa2452 aria-expanded=false>
Concepts</button><div class=collapse id=section-0460583622f03a52d7693094d6fa2452><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/concepts/basics/>Basics</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/20-concepts/>20 Concepts</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/25-golden-rules/>25 Golden Rules</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/key-steps/>Key Steps</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-1afa74da05ca145d3418aad9af510109 aria-expanded=false>
Design</button><div class=collapse id=section-1afa74da05ca145d3418aad9af510109><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/design/url-shortener/>URL Shortener</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/short-media-sharing-platform/>Short Media Sharing Platform</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/video-streaming-platform/>Video Streaming Platform</a></li></ul></div></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#model-optimizations-for-deployment>Model Optimizations for Deployment</a><ul><li><a href=#generative-ai-project-lifecycle-stage-4---application-integration>Generative AI Project Lifecycle Stage 4 - Application Integration.</a></li><li><a href=#model-optimizations-to-improve-application-performance>Model Optimizations to Improve Application Performance</a></li><li><a href=#three-llm-optimization-techniques>Three LLM Optimization Techniques</a></li><li><a href=#distillation>Distillation</a></li><li><a href=#post-training-quantization-ptq>Post-Training Quantization (PTQ)</a></li><li><a href=#pruning>Pruning</a></li></ul></li><li><a href=#generative-ai-project-lifecycle-cheat-sheet>Generative AI Project Lifecycle Cheat Sheet</a><ul><li><a href=#pre-training>Pre-training</a></li><li><a href=#prompt-engineering>Prompt Engineering</a></li><li><a href=#prompt-tuning-and-fine-tuning>Prompt Tuning and Fine-tuning</a></li><li><a href=#reinforcement-learning>Reinforcement Learning</a></li><li><a href=#compressionoptimizationdeployment>Compression/Optimization/Deployment</a></li></ul></li><li><a href=#using-the-llm-in-applications>Using the LLM in Applications</a><ul><li><a href=#retrieval-augmented-generation>Retrieval Augmented Generation</a></li><li><a href=#rag-example-searching-legal-documents>RAG Example: Searching Legal Documents</a></li><li><a href=#rag-integrates-with-many-types-of-data-sources>RAG Integrates with Many Types of Data Sources</a></li><li><a href=#data-preparation>Data Preparation</a></li><li><a href=#vector-database-search>Vector Database Search</a></li></ul></li><li><a href=#interacting-with-external-applications>Interacting with External Applications</a><ul><li><a href=#llm-powered-applications>LLM-powered Applications</a></li><li><a href=#requirements-for-using-llms-to-power-applications>Requirements for Using LLMs to Power Applications</a></li></ul></li><li><a href=#helping-llms-reason-and-plan-with-chain-of-thought-cot>Helping LLMs Reason and Plan with Chain of Thought (CoT)</a><ul><li><a href=#llms-can-struggle-with-complex-reasoning-problems>LLMs Can Struggle with Complex Reasoning Problems</a></li></ul></li><li><a href=#program-aided-language-pal-models>Program-aided Language (PAL) Models</a><ul><li><a href=#llms-can-struggle-with-mathematics>LLMs Can Struggle with Mathematics</a></li><li><a href=#pal-models>PAL Models</a></li><li><a href=#pal-example>PAL Example</a></li><li><a href=#llm-powered-applications-2>LLM-powered Applications 2</a></li><li><a href=#pal-architecture>PAL Architecture</a></li><li><a href=#react-combining-reasoning-and-action>ReAct: Combining Reasoning and Action</a></li><li><a href=#react-synergizing-reasoning-and-action-in-llms>ReAct: Synergizing Reasoning and Action in LLMs</a></li><li><a href=#react-instructions-define-the-action-space>ReAct Instructions Define the Action Space</a></li><li><a href=#building-up-the-react-prompt>Building Up the ReAct Prompt</a></li><li><a href=#langchain>LangChain</a></li><li><a href=#the-significance-of-scale-application-building>The Significance of Scale: Application Building</a></li></ul></li><li><a href=#reading-react-reasoning-and-action>Reading: ReAct: Reasoning and Action</a></li><li><a href=#llm-application-architectures>LLM Application Architectures</a><ul><li><a href=#infrastructure>Infrastructure</a></li><li><a href=#llm-models>LLM Models</a></li><li><a href=#information-sources>Information Sources</a></li><li><a href=#gather-outputs--feedback>Gather Outputs & Feedback</a></li><li><a href=#llm-tools--frameworks>LLM Tools & Frameworks</a></li><li><a href=#application-interfaces>Application Interfaces</a></li><li><a href=#summary>Summary</a></li></ul></li><li><a href=#optional-aws-sagemaker-jumpstart>Optional: AWS Sagemaker Jumpstart</a></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#model-optimizations-for-deployment>Model Optimizations for Deployment</a><ul><li><a href=#generative-ai-project-lifecycle-stage-4---application-integration>Generative AI Project Lifecycle Stage 4 - Application Integration.</a></li><li><a href=#model-optimizations-to-improve-application-performance>Model Optimizations to Improve Application Performance</a></li><li><a href=#three-llm-optimization-techniques>Three LLM Optimization Techniques</a></li><li><a href=#distillation>Distillation</a></li><li><a href=#post-training-quantization-ptq>Post-Training Quantization (PTQ)</a></li><li><a href=#pruning>Pruning</a></li></ul></li><li><a href=#generative-ai-project-lifecycle-cheat-sheet>Generative AI Project Lifecycle Cheat Sheet</a><ul><li><a href=#pre-training>Pre-training</a></li><li><a href=#prompt-engineering>Prompt Engineering</a></li><li><a href=#prompt-tuning-and-fine-tuning>Prompt Tuning and Fine-tuning</a></li><li><a href=#reinforcement-learning>Reinforcement Learning</a></li><li><a href=#compressionoptimizationdeployment>Compression/Optimization/Deployment</a></li></ul></li><li><a href=#using-the-llm-in-applications>Using the LLM in Applications</a><ul><li><a href=#retrieval-augmented-generation>Retrieval Augmented Generation</a></li><li><a href=#rag-example-searching-legal-documents>RAG Example: Searching Legal Documents</a></li><li><a href=#rag-integrates-with-many-types-of-data-sources>RAG Integrates with Many Types of Data Sources</a></li><li><a href=#data-preparation>Data Preparation</a></li><li><a href=#vector-database-search>Vector Database Search</a></li></ul></li><li><a href=#interacting-with-external-applications>Interacting with External Applications</a><ul><li><a href=#llm-powered-applications>LLM-powered Applications</a></li><li><a href=#requirements-for-using-llms-to-power-applications>Requirements for Using LLMs to Power Applications</a></li></ul></li><li><a href=#helping-llms-reason-and-plan-with-chain-of-thought-cot>Helping LLMs Reason and Plan with Chain of Thought (CoT)</a><ul><li><a href=#llms-can-struggle-with-complex-reasoning-problems>LLMs Can Struggle with Complex Reasoning Problems</a></li></ul></li><li><a href=#program-aided-language-pal-models>Program-aided Language (PAL) Models</a><ul><li><a href=#llms-can-struggle-with-mathematics>LLMs Can Struggle with Mathematics</a></li><li><a href=#pal-models>PAL Models</a></li><li><a href=#pal-example>PAL Example</a></li><li><a href=#llm-powered-applications-2>LLM-powered Applications 2</a></li><li><a href=#pal-architecture>PAL Architecture</a></li><li><a href=#react-combining-reasoning-and-action>ReAct: Combining Reasoning and Action</a></li><li><a href=#react-synergizing-reasoning-and-action-in-llms>ReAct: Synergizing Reasoning and Action in LLMs</a></li><li><a href=#react-instructions-define-the-action-space>ReAct Instructions Define the Action Space</a></li><li><a href=#building-up-the-react-prompt>Building Up the ReAct Prompt</a></li><li><a href=#langchain>LangChain</a></li><li><a href=#the-significance-of-scale-application-building>The Significance of Scale: Application Building</a></li></ul></li><li><a href=#reading-react-reasoning-and-action>Reading: ReAct: Reasoning and Action</a></li><li><a href=#llm-application-architectures>LLM Application Architectures</a><ul><li><a href=#infrastructure>Infrastructure</a></li><li><a href=#llm-models>LLM Models</a></li><li><a href=#information-sources>Information Sources</a></li><li><a href=#gather-outputs--feedback>Gather Outputs & Feedback</a></li><li><a href=#llm-tools--frameworks>LLM Tools & Frameworks</a></li><li><a href=#application-interfaces>Application Interfaces</a></li><li><a href=#summary>Summary</a></li></ul></li><li><a href=#optional-aws-sagemaker-jumpstart>Optional: AWS Sagemaker Jumpstart</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>Week 3 Part 2 - LLM-powered Applications</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#model-optimizations-for-deployment>Model Optimizations for Deployment</a><ul><li><a href=#generative-ai-project-lifecycle-stage-4---application-integration>Generative AI Project Lifecycle Stage 4 - Application Integration.</a></li><li><a href=#model-optimizations-to-improve-application-performance>Model Optimizations to Improve Application Performance</a></li><li><a href=#three-llm-optimization-techniques>Three LLM Optimization Techniques</a></li><li><a href=#distillation>Distillation</a></li><li><a href=#post-training-quantization-ptq>Post-Training Quantization (PTQ)</a></li><li><a href=#pruning>Pruning</a></li></ul></li><li><a href=#generative-ai-project-lifecycle-cheat-sheet>Generative AI Project Lifecycle Cheat Sheet</a><ul><li><a href=#pre-training>Pre-training</a></li><li><a href=#prompt-engineering>Prompt Engineering</a></li><li><a href=#prompt-tuning-and-fine-tuning>Prompt Tuning and Fine-tuning</a></li><li><a href=#reinforcement-learning>Reinforcement Learning</a></li><li><a href=#compressionoptimizationdeployment>Compression/Optimization/Deployment</a></li></ul></li><li><a href=#using-the-llm-in-applications>Using the LLM in Applications</a><ul><li><a href=#retrieval-augmented-generation>Retrieval Augmented Generation</a></li><li><a href=#rag-example-searching-legal-documents>RAG Example: Searching Legal Documents</a></li><li><a href=#rag-integrates-with-many-types-of-data-sources>RAG Integrates with Many Types of Data Sources</a></li><li><a href=#data-preparation>Data Preparation</a></li><li><a href=#vector-database-search>Vector Database Search</a></li></ul></li><li><a href=#interacting-with-external-applications>Interacting with External Applications</a><ul><li><a href=#llm-powered-applications>LLM-powered Applications</a></li><li><a href=#requirements-for-using-llms-to-power-applications>Requirements for Using LLMs to Power Applications</a></li></ul></li><li><a href=#helping-llms-reason-and-plan-with-chain-of-thought-cot>Helping LLMs Reason and Plan with Chain of Thought (CoT)</a><ul><li><a href=#llms-can-struggle-with-complex-reasoning-problems>LLMs Can Struggle with Complex Reasoning Problems</a></li></ul></li><li><a href=#program-aided-language-pal-models>Program-aided Language (PAL) Models</a><ul><li><a href=#llms-can-struggle-with-mathematics>LLMs Can Struggle with Mathematics</a></li><li><a href=#pal-models>PAL Models</a></li><li><a href=#pal-example>PAL Example</a></li><li><a href=#llm-powered-applications-2>LLM-powered Applications 2</a></li><li><a href=#pal-architecture>PAL Architecture</a></li><li><a href=#react-combining-reasoning-and-action>ReAct: Combining Reasoning and Action</a></li><li><a href=#react-synergizing-reasoning-and-action-in-llms>ReAct: Synergizing Reasoning and Action in LLMs</a></li><li><a href=#react-instructions-define-the-action-space>ReAct Instructions Define the Action Space</a></li><li><a href=#building-up-the-react-prompt>Building Up the ReAct Prompt</a></li><li><a href=#langchain>LangChain</a></li><li><a href=#the-significance-of-scale-application-building>The Significance of Scale: Application Building</a></li></ul></li><li><a href=#reading-react-reasoning-and-action>Reading: ReAct: Reasoning and Action</a></li><li><a href=#llm-application-architectures>LLM Application Architectures</a><ul><li><a href=#infrastructure>Infrastructure</a></li><li><a href=#llm-models>LLM Models</a></li><li><a href=#information-sources>Information Sources</a></li><li><a href=#gather-outputs--feedback>Gather Outputs & Feedback</a></li><li><a href=#llm-tools--frameworks>LLM Tools & Frameworks</a></li><li><a href=#application-interfaces>Application Interfaces</a></li><li><a href=#summary>Summary</a></li></ul></li><li><a href=#optional-aws-sagemaker-jumpstart>Optional: AWS Sagemaker Jumpstart</a></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#model-optimizations-for-deployment>Model Optimizations for Deployment</a><ul><li><a href=#generative-ai-project-lifecycle-stage-4---application-integration>Generative AI Project Lifecycle Stage 4 - Application Integration.</a></li><li><a href=#model-optimizations-to-improve-application-performance>Model Optimizations to Improve Application Performance</a></li><li><a href=#three-llm-optimization-techniques>Three LLM Optimization Techniques</a></li><li><a href=#distillation>Distillation</a></li><li><a href=#post-training-quantization-ptq>Post-Training Quantization (PTQ)</a></li><li><a href=#pruning>Pruning</a></li></ul></li><li><a href=#generative-ai-project-lifecycle-cheat-sheet>Generative AI Project Lifecycle Cheat Sheet</a><ul><li><a href=#pre-training>Pre-training</a></li><li><a href=#prompt-engineering>Prompt Engineering</a></li><li><a href=#prompt-tuning-and-fine-tuning>Prompt Tuning and Fine-tuning</a></li><li><a href=#reinforcement-learning>Reinforcement Learning</a></li><li><a href=#compressionoptimizationdeployment>Compression/Optimization/Deployment</a></li></ul></li><li><a href=#using-the-llm-in-applications>Using the LLM in Applications</a><ul><li><a href=#retrieval-augmented-generation>Retrieval Augmented Generation</a></li><li><a href=#rag-example-searching-legal-documents>RAG Example: Searching Legal Documents</a></li><li><a href=#rag-integrates-with-many-types-of-data-sources>RAG Integrates with Many Types of Data Sources</a></li><li><a href=#data-preparation>Data Preparation</a></li><li><a href=#vector-database-search>Vector Database Search</a></li></ul></li><li><a href=#interacting-with-external-applications>Interacting with External Applications</a><ul><li><a href=#llm-powered-applications>LLM-powered Applications</a></li><li><a href=#requirements-for-using-llms-to-power-applications>Requirements for Using LLMs to Power Applications</a></li></ul></li><li><a href=#helping-llms-reason-and-plan-with-chain-of-thought-cot>Helping LLMs Reason and Plan with Chain of Thought (CoT)</a><ul><li><a href=#llms-can-struggle-with-complex-reasoning-problems>LLMs Can Struggle with Complex Reasoning Problems</a></li></ul></li><li><a href=#program-aided-language-pal-models>Program-aided Language (PAL) Models</a><ul><li><a href=#llms-can-struggle-with-mathematics>LLMs Can Struggle with Mathematics</a></li><li><a href=#pal-models>PAL Models</a></li><li><a href=#pal-example>PAL Example</a></li><li><a href=#llm-powered-applications-2>LLM-powered Applications 2</a></li><li><a href=#pal-architecture>PAL Architecture</a></li><li><a href=#react-combining-reasoning-and-action>ReAct: Combining Reasoning and Action</a></li><li><a href=#react-synergizing-reasoning-and-action-in-llms>ReAct: Synergizing Reasoning and Action in LLMs</a></li><li><a href=#react-instructions-define-the-action-space>ReAct Instructions Define the Action Space</a></li><li><a href=#building-up-the-react-prompt>Building Up the ReAct Prompt</a></li><li><a href=#langchain>LangChain</a></li><li><a href=#the-significance-of-scale-application-building>The Significance of Scale: Application Building</a></li></ul></li><li><a href=#reading-react-reasoning-and-action>Reading: ReAct: Reasoning and Action</a></li><li><a href=#llm-application-architectures>LLM Application Architectures</a><ul><li><a href=#infrastructure>Infrastructure</a></li><li><a href=#llm-models>LLM Models</a></li><li><a href=#information-sources>Information Sources</a></li><li><a href=#gather-outputs--feedback>Gather Outputs & Feedback</a></li><li><a href=#llm-tools--frameworks>LLM Tools & Frameworks</a></li><li><a href=#application-interfaces>Application Interfaces</a></li><li><a href=#summary>Summary</a></li></ul></li><li><a href=#optional-aws-sagemaker-jumpstart>Optional: AWS Sagemaker Jumpstart</a></li></ul></nav></div></nav><p>These notes were developed using lectures/material/transcripts from the <a href=https://www.deeplearning.ai/courses/generative-ai-with-llms/>DeepLearning.AI & AWS - Generative AI with Large Language Models</a> course</p><h2 id=notes>Notes <a href=#notes class=anchor aria-hidden=true>#</a></h2><ul><li>Model Optimizations for Deployment<ul><li>Generative AI Project Lifecycle Stage 4 - Application Integration<ul><li>Questions to ask:<ol><li>How the LLM will function in deployment</li><li>What additional resources that the LLM may need</li><li>How the model will be consumed</li></ol></li></ul></li><li>Model Optimizations to Improve Application Performance<ul><li>LLM inference challenges<ul><li>Compute requirements</li><li>Storage requirements</li><li>Latency</li></ul></li><li>Challenge is to reduce model size while maintaining model performance</li><li>Techniques to reduce model size to improve model performance during inference without impacting accuracy<ol><li>Distillation - uses a larger model to train a smaller model, the smaller model will be used for inference<ul><li>In practice, not as effective for generative decoder models, more effective for encoder-only models such as BERT that have a lot of representation redundancy</li></ul></li><li>Post-Training Quantization (PTQ) - transforms a model&rsquo;s weights to a lower precision representation such as 16-bit floating point or 8-bit integer</li><li>Pruning - eliminate weights (that are very close or equal to zero) that are not contributing much to overall model performance</li></ol></li></ul></li></ul></li><li>Generative AI Project Lifecycle Cheat Sheet<ol><li>Pre-training</li><li>Prompt Engineering</li><li>Prompt Tuning and Fine-tuning</li><li>Reinforcement Learning</li><li>Compression/Optimization/Deployment</li></ol></li><li>Using the LLM in Applications<ul><li>LLM-powered Applications<ul><li>Problems<ul><li>Information may be out of date</li><li>Struggle with complex math problems</li><li>Hallucination</li></ul></li><li>Ways to overcome - connect them to external data sources and applications<ul><li>Retrieval Augmented Generation - published by Facebook in 2020<ul><li>Retriever - retrieves relevant information from an external corpus or knowledge base<ul><li>Encoder - encodes the query in the same format as the external documents</li><li>External Data Source</li></ul></li><li>Data Preparation<ol><li>Data must fit inside context window - text documents are broken into smaller chunks, each of which will fit in the context window of LLM</li><li>Data must be in format that allows its relevance to be assessed at inference time: embedding vectors - allow the LLM to identify semantically related words through measures such as Cosine Similarity</li></ol></li><li>Vector Database<ul><li>Store both the text representation as well as the embeddings</li><li>Enable a fast and efficient kind of relevant search based on similarity</li><li>Each vector is identified by a key</li><li>Enables a citation to be included in completion</li></ul></li></ul></li></ul></li></ul></li></ul></li><li>Interacting with External Applications</li><li>Helping LLMs Reason and Plan with Chain of Thought (CoT)<ul><li>It works by including a series of intermediate reasoning steps into any examples that you use for one or few-shot inference</li><li>By structuring the examples in this way, you&rsquo;re essentially teaching the model how to reason through the task to reach a solution</li><li>It is a powerful technique that improves the ability of the model to reason through problems</li></ul></li><li>Program-aided Language Models (PAL)<ul><li>Presented by researchers at Carnegie Mellon University in 2022</li><li>Pairs an LLM with an external code interpreter to carry out calculations</li><li>The method makes use of Chain-of-Thought (CoT) prompting to generate executable Python scripts</li></ul></li><li>ReAct: Combining Reasoning and Action<ul><li>ReAct - a prompting strategy that combines Chain-of-Thought (CoT) reasoning with action planning<ul><li>Proposed by researches at Princeton and Google in 2022</li></ul></li><li>LangChain framework - provides you with modular pieces that contain the components necessary to work with LLMs</li><li>Significance of Scale: Application Building<ul><li>Larger models are more capable</li><li>Start with a larger model, collect a lot of user data in deployment, and use it to train and fine-tune a smaller model that you can switch to at a later time</li></ul></li></ul></li><li>Reading: ReAct - Reasoning and Action<ul><li><a href=https://arxiv.org/abs/2210.03629>https://arxiv.org/abs/2210.03629</a></li></ul></li><li>LLM Application Architectures<ul><li>Infrastructure - provides the compute, storage, and network to serve up your LLMs, as well as to host your application components</li><li>LLM Models</li><li>Information Sources - required by RAG</li><li>Gather Outputs & Feedback - capture and store the outputs, gather feedback that may be useful for additional fine-tuning, alignment, or evaluation</li><li>LLM Tools & Frameworks - LangChain, Model Hubs, etc.</li><li>Application Interfaces - Web interface or REST API</li></ul></li></ul><hr><h2 id=model-optimizations-for-deployment>Model Optimizations for Deployment <a href=#model-optimizations-for-deployment class=anchor aria-hidden=true>#</a></h2><h3 id=generative-ai-project-lifecycle-stage-4---application-integration>Generative AI Project Lifecycle Stage 4 - Application Integration. <a href=#generative-ai-project-lifecycle-stage-4---application-integration class=anchor aria-hidden=true>#</a></h3><p>There are a number of important questions to ask at this stage.</p><ul><li>The first set is related to how your LLM will function in deployment.<ul><li>So how fast do you need your model to generate completions?</li><li>What compute budget do you have available?</li><li>And are you willing to trade off model performance for improved inference speed or lower storage?</li></ul></li><li>The second set of questions is tied to additional resources that your model may need.<ul><li>Do you intend for your model to interact with external data or other applications?</li><li>And if so, how will you connect to those resources?</li></ul></li><li>Lastly, there&rsquo;s the question of how your model will be consumed.<ul><li>What will the intended application or API interface that your model will be consumed through look like?</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled.png alt=Untitled></p><p>Let&rsquo;s start by exploring a few methods that can be used to optimize your model before deploying it for inference</p><h3 id=model-optimizations-to-improve-application-performance>Model Optimizations to Improve Application Performance <a href=#model-optimizations-to-improve-application-performance class=anchor aria-hidden=true>#</a></h3><ul><li>Large language models present inference challenges in terms of<ul><li>computing and storage requirements, as well as ensuring</li><li>low latency for consuming applications</li></ul></li><li>These challenges persist whether you&rsquo;re deploying on premises or to the cloud, and become even more of an issue when deploying to edge devices.</li><li>One of the primary ways to improve application performance is to reduce the size of the LLM.<ul><li>This can allow for quicker loading of the model, which reduces inference latency.</li><li>However, the challenge is to reduce the size of the model while still maintaining model performance.</li></ul></li><li>Some techniques work better than others for generative models, and there are tradeoffs between accuracy and performance.</li></ul><h3 id=three-llm-optimization-techniques>Three LLM Optimization Techniques <a href=#three-llm-optimization-techniques class=anchor aria-hidden=true>#</a></h3><p>Three Techniques to reduce model size while maintaining model performance</p><ol><li><strong>Distillation</strong> uses a larger model, the teacher model, to train a smaller model, the student model. You then use the smaller model for inference to lower your storage and compute budget.</li><li>Similar to Quantization Aware Training (QAT), <strong>Post Training Quantization</strong> transforms a model&rsquo;s weights to a lower precision representation, such as a 16- bit floating point or eight bit integer. This reduces the memory footprint of your model.</li><li><strong>Model Pruning</strong>, removes redundant model parameters that contribute little to the model&rsquo;s performance</li></ol><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%201.png alt=Untitled></p><h3 id=distillation>Distillation <a href=#distillation class=anchor aria-hidden=true>#</a></h3><ul><li>A technique that focuses on having a larger teacher model train a smaller student model</li><li>The student model learns to statistically mimic the behavior of the teacher model, either just in the final prediction layer or in the model&rsquo;s hidden layers as well</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%202.png alt=Untitled></p><ul><li>You start with your fine-tuned LLM as your teacher model and create a smaller LLM for your student model</li><li>You freeze the teacher model&rsquo;s weights and use it to generate completions for your training data</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%203.png alt=Untitled></p><ul><li>At the same time, you generate completions for the training data using your student model</li><li>The knowledge distillation between teacher and student model is achieved by minimizing a loss function called the distillation loss</li><li>To calculate this loss, distillation uses the probability distribution over tokens that is produced by the teacher model&rsquo;s softmax layer</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%204.png alt=Untitled></p><ul><li>Now, the teacher model is already fine-tuned on the training data</li><li>So the probability distribution likely closely matches the ground truth data and won&rsquo;t have much variation in tokens</li><li>That&rsquo;s why Distillation applies a little trick adding a temperature parameter to the softmax function</li><li>As you learned in lesson one, a higher temperature increases the creativity of the language the model generates</li><li>With a temperature parameter greater than one, the probability distribution becomes broader and less strongly peaked</li><li>This softer distribution provides you with a set of tokens that are similar to the ground truth tokens</li><li>In the context of Distillation, the teacher model&rsquo;s output is often referred to as soft labels and the student model&rsquo;s predictions as soft predictions</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%205.png alt=Untitled></p><ul><li>In parallel, you train the student model to generate the correct predictions based on your ground truth training data</li><li>Here, you don&rsquo;t vary the temperature setting and instead use the standard softmax function</li><li>Distillation refers to the student model outputs as the hard predictions and hard labels<ul><li>The loss between these two is the student loss</li></ul></li><li>The combined distillation and student losses are used to update the weights of the student model via back propagation</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%206.png alt=Untitled></p><ul><li>The key benefit of distillation methods is that the smaller student model can be used for inference in deployment instead of the teacher model</li><li>In practice, distillation is not as effective for generative decoder models<ul><li>It&rsquo;s typically more effective for encoder-only models, such as BERT that have a lot of representation redundancy</li></ul></li><li>Note that with Distillation, you&rsquo;re training a second, smaller model to use during inference.<ul><li>You aren&rsquo;t reducing the model size of the initial LLM in any way</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%207.png alt=Untitled></p><h3 id=post-training-quantization-ptq>Post-Training Quantization (PTQ) <a href=#post-training-quantization-ptq class=anchor aria-hidden=true>#</a></h3><ul><li>You were introduced to the second method, quantization, back in week one in the context of training, specifically Quantization Aware Training (QAT).</li><li>However, after a model is trained, you can perform Post Training Quantization (PTQ) to optimize it for deployment.</li><li>PTQ transforms a model&rsquo;s weights to a lower precision representation, such as 16-bit floating point or 8-bit integer.</li><li>To reduce the model size and memory footprint, as well as the compute resources needed for model serving, quantization can be applied to just the model weights or to both weights and activation layers.</li><li>In general, quantization approaches that include the activations can have a higher impact on model performance.</li><li>Quantization also requires an extra calibration step to statistically capture the dynamic range of the original parameter values</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%208.png alt=Untitled></p><ul><li>As with other methods, there are tradeoffs because sometimes Quantization results in a small percentage reduction in model evaluation metrics</li><li>However, that reduction can often be worth the cost savings and performance gains</li></ul><h3 id=pruning>Pruning <a href=#pruning class=anchor aria-hidden=true>#</a></h3><ul><li>At a high level, the goal is to reduce model size for inference by eliminating weights that are not contributing much to overall model performance</li><li>These are the weights with values very close to or equal to zero</li><li>Note that some pruning methods require full retraining of the model, while others fall into the category of parameter efficient fine-tuning, such as LoRA.</li><li>There are also methods that focus on Post-training Pruning</li><li>In theory, this reduces the size of the model and improves performance</li><li>In practice, however, there may not be much impact on the size and performance if only a small percentage of the model weights are close to zero</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%209.png alt=Untitled></p><ul><li>Quantization, Distillation and Pruning all aim to reduce model size to improve model performance during inference without impacting accuracy</li><li>Optimizing your model for deployment will help ensure that your application functions well and provides your users with the best possible experience sense</li></ul><hr><h2 id=generative-ai-project-lifecycle-cheat-sheet>Generative AI Project Lifecycle Cheat Sheet <a href=#generative-ai-project-lifecycle-cheat-sheet class=anchor aria-hidden=true>#</a></h2><h3 id=pre-training>Pre-training <a href=#pre-training class=anchor aria-hidden=true>#</a></h3><ul><li>Pre-training a large language model can be a huge effort. This stage is the most complex you&rsquo;ll face because of<ul><li>the model architecture decisions,</li><li>the large amount of training data required, and</li><li>the expertise needed.</li></ul></li><li>Remember though, that in general, you will start your development work with an existing foundation model. You&rsquo;ll probably be able to skip this stage.</li></ul><h3 id=prompt-engineering>Prompt Engineering <a href=#prompt-engineering class=anchor aria-hidden=true>#</a></h3><ul><li>If you&rsquo;re working with a foundation model, you&rsquo;ll likely start to assess the model&rsquo;s performance through prompt engineering, which requires less technical expertise, and no additional training of the model.</li></ul><h3 id=prompt-tuning-and-fine-tuning>Prompt Tuning and Fine-tuning <a href=#prompt-tuning-and-fine-tuning class=anchor aria-hidden=true>#</a></h3><ul><li>If your model isn&rsquo;t performing as you need, you&rsquo;ll next think about prompt tuning and fine tuning. Depending on your use case, performance goals, and compute budget, the methods you&rsquo;ll try could range from full fine-tuning to parameter efficient fine tuning techniques like LoRA or prompt tuning. Some level of technical expertise is required for this work. But since fine-tuning can be very successful with a relatively small training dataset, this phase could potentially be completed in a single day.</li></ul><h3 id=reinforcement-learning>Reinforcement Learning <a href=#reinforcement-learning class=anchor aria-hidden=true>#</a></h3><ul><li>Aligning your model using Reinforcement Learning from Human Feedback (RLHF) can be done quickly, once you have your trained reward model. You&rsquo;ll likely see if you can use an existing reward model for this work, as you saw in this week&rsquo;s lab. However, if you have to train a reward model from scratch, it could take a long time because of the effort involved to gather human feedback.</li></ul><h3 id=compressionoptimizationdeployment>Compression/Optimization/Deployment <a href=#compressionoptimizationdeployment class=anchor aria-hidden=true>#</a></h3><ul><li>Finally, optimization techniques, typically fall in the middle in terms of complexity and effort, but can proceed quite quickly assuming the changes to the model don&rsquo;t impact performance too much. After working through all of these steps, you have hopefully trained and tuned a great LLM that is working well for your specific use case, and is optimized for deployment.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2010.png alt=Untitled></p><hr><h2 id=using-the-llm-in-applications>Using the LLM in Applications <a href=#using-the-llm-in-applications class=anchor aria-hidden=true>#</a></h2><ul><li>There are some broader challenges with large language models that can&rsquo;t be solved by training alone</li><li>Problems<ul><li>Information may be out of date</li><li>Struggle with complex math problems</li><li>Hallucination</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2011.png alt=Untitled></p><ul><li>Learn about some techniques that you can use to help your LLM overcome these issues by connecting to external data sources and applications</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2012.png alt=Untitled></p><ul><li>Your application must manage the passing of user input to the large language model and the return of completions</li><li>This is often done through some type of orchestration library</li><li>This layer can enable some powerful technologies that augment and enhance the performance of the LLM at runtime</li><li>By providing access to external data sources or connecting to existing APIs of other applications<ul><li>One implementation example is LangChain</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2013.png alt=Untitled></p><p>Let&rsquo;s start by considering how to connect LLMs to external data sources</p><h3 id=retrieval-augmented-generation>Retrieval Augmented Generation <a href=#retrieval-augmented-generation class=anchor aria-hidden=true>#</a></h3><ul><li>Retrieval Augmented Generation (RAG) is a framework for building LLM powered systems that make use of external data sources and applications to overcome some of the limitations of these models.</li><li>RAG is a great way to overcome the knowledge cutoff issue and help the model update its understanding of the world.</li><li>While you could retrain the model on new data, this would quickly become very expensive and require repeated retraining to regularly update the model with new knowledge.</li><li>A more flexible and less expensive way to overcome knowledge cutoffs is to give your model access to additional external data at inference time.</li><li>RAG is useful in any case where you want the language model to have access to data that it may not have seen.</li><li>This could be new information documents not included in the original training data, or proprietary knowledge stored in your organization&rsquo;s private databases.</li><li>Providing your model with external information, can improve both the relevance and accuracy of its completions</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2014.png alt=Untitled></p><ul><li>Retrieval Augmented Generation isn&rsquo;t a specific set of technologies, but rather a framework for providing LLMs access to data they did not see during training.</li><li>A number of different implementations exist, and the one you choose will depend on the details of your task and the format of the data you have to work with.</li><li>Here you&rsquo;ll walk through the implementation discussed in one of the earliest papers on RAG by researchers at Facebook, originally published in 2020.<ul><li>At the heart of this implementation is a model component called the Retriever, which consists of a query encoder and an external data source.</li><li>The encoder takes the user&rsquo;s input prompt and encodes it into a form that can be used to query the data source.</li><li>In the Facebook paper, the external data is a vector store, which we&rsquo;ll discuss in more detail shortly. But it could instead be a SQL database, CSV files, or other data storage format.</li><li>These two components are trained together to find documents within the external data that are most relevant to the input query.</li><li>The Retriever returns the best single or group of documents from the data source and combines the new information with the original user query.</li><li>The new expanded prompt is then passed to the language model, which generates a completion that makes use of the data</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2015.png alt=Untitled></p><h3 id=rag-example-searching-legal-documents>RAG Example: Searching Legal Documents <a href=#rag-example-searching-legal-documents class=anchor aria-hidden=true>#</a></h3><ul><li>Imagine you are a lawyer using a large language model to help you in the discovery phase of a case.</li><li>A RAG architecture can help you ask questions of a corpus of documents, for example, previous court filings.</li><li>Here you ask the model about the plaintiff named in a specific case number<ul><li>The prompt is passed to the query encoder, which encodes the data in the same format as the external documents.</li><li>And then searches for a relevant entry in the corpus of documents.</li><li>Having found a piece of text that contains the requested information, the Retriever then combines the new text with the original prompt.</li><li>The expanded prompt that now contains information about the specific case of interest is then passed to the LLM.</li><li>The model uses the information in the context of the prompt to generate a completion that contains the correct answer.</li><li>The use case you have seen here is quite simple and only returns a single piece of information that could be found by other means</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2016.png alt=Untitled></p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2017.png alt=Untitled></p><ul><li>But imagine the power of RAG to be able to generate summaries of filings or identify specific people, places and organizations within the full corpus of the legal documents.</li><li>Allowing the model to access information contained in this external data set greatly increases its utility for this specific use case</li></ul><h3 id=rag-integrates-with-many-types-of-data-sources>RAG Integrates with Many Types of Data Sources <a href=#rag-integrates-with-many-types-of-data-sources class=anchor aria-hidden=true>#</a></h3><ul><li>In addition to overcoming knowledge cutoffs, RAG also helps you avoid the problem of the model hallucinating when it doesn&rsquo;t know the answer</li><li>RAG architectures can be used to integrate multiple types of external information sources.</li><li>You can augment large language models with access to local documents, including private wikis and expert systems.</li><li>RAG can also enable access to the Internet to extract information posted on web pages, for example, Wikipedia.</li><li>By encoding the user input prompt as a SQL query, RAG can also interact with databases.</li><li>Another important data storage strategy is a Vector Store, which contains vector representations of text.</li><li>This is a particularly useful data format for language models, since internally they work with vector representations of language to generate text.</li><li>Vector stores enable a fast and efficient kind of relevant search based on similarity</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2018.png alt=Untitled></p><p>Note: implementing RAG is a little more complicated than simply adding text into the LLM</p><h3 id=data-preparation>Data Preparation <a href=#data-preparation class=anchor aria-hidden=true>#</a></h3><ol><li>Data must fit inside context window<ul><li>Most text sources are too long to fit into the limited context window of the model, which is still at most just a few thousand tokens.</li><li>Instead, the external data sources are chopped up into many chunks, each of which will fit in the context window.</li><li>Packages like LangChain can handle this work for you</li></ul></li></ol><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2019.png alt=Untitled></p><ol><li>Data must be in format that allows its relevance to be assessed at inference time - embedding vectors<ul><li>Recall that LLMs don&rsquo;t work directly with text, but instead create vector representations of each token in an embedding space.</li><li>These embedding vectors allow the LLM to identify semantically related words through measures such as cosine similarity</li></ul></li></ol><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2020.png alt=Untitled></p><ul><li>RAG methods take the small chunks of external data and process them through the large language model, to create embedding vectors for each.</li><li>These new representations of the data can be stored in structures called vector stores, which allow for fast searching of datasets and efficient identification of semantically related text</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2021.png alt=Untitled></p><h3 id=vector-database-search>Vector Database Search <a href=#vector-database-search class=anchor aria-hidden=true>#</a></h3><ul><li>Vector databases are a particular implementation of a vector store where each vector is also identified by a key.</li><li>This can allow, for instance, the text generated by RAG to also include a citation for the document from which it was received</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2022.png alt=Untitled></p><ul><li>You&rsquo;ve seen how access to external data sources can help a model overcome limits to its internal knowledge.</li><li>By providing up to date relevant information and avoiding hallucinations, you can greatly improve the experience of using your application for your users.</li><li>Next, we&rsquo;ll explore a technique that can improve a model&rsquo;s ability to reason and make plans important steps when using an LLM to power an application</li></ul><p>Question: How does Retrieval Augmented Generation (RAG) enhance generation-based models?</p><p><strong>By making external knowledge available to the model</strong></p><p>Correct</p><p>The <strong>retriever</strong> component retrieves relevant information from an external corpus or knowledge base, which is then used by the model to generate more informed and contextually relevant responses. This incorporation of external knowledge enhances the quality and relevance of the generated content.</p><hr><h2 id=interacting-with-external-applications>Interacting with External Applications <a href=#interacting-with-external-applications class=anchor aria-hidden=true>#</a></h2><ul><li>In the previous section, you saw how LLMs can interact with external datasets.</li><li>Now let&rsquo;s take a look at how they can interact with external applications.</li><li>To motivate the types of problems and use cases that require this kind of augmentation of the LLM, you&rsquo;ll revisit the customer service bot example you saw earlier in the course.</li><li>During this walkthrough of one customer&rsquo;s interaction with ShopBot, you&rsquo;ll take a look at the integrations that you&rsquo;d need to allow the app to process a return requests from end to end.</li><li>In this conversation, the customer has expressed that they want to return some genes that they purchased.</li><li>ShopBot responds by asking for the order number, which the customer then provides</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2023.png alt=Untitled></p><ul><li>ShopBot then looks up the order number in the transaction database.</li><li>One way it could do this is by using a rag implementation of the kind you saw earlier in the previous video.</li><li>In this case here, you would likely be retrieving data through a SQL query to a back-end order database rather than retrieving data from a corpus of documents.</li><li>Once ShopBot has retrieved the customers order, the next step is to confirm the items that will be returned.</li><li>The bot ask the customer if they&rsquo;d like to return anything other than the genes.</li><li>After the user states their answer, the bot initiates a request to the company&rsquo;s shipping partner for a return label.</li><li>The body uses the shippers Python API to request the label ShopBot is going to email the shipping label to the customer.</li><li>It also asks them to confirm their email address</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2024.png alt=Untitled></p><ul><li>The customer responds with their email address and the bot includes this information in the API call to the shipper.</li><li>Once the API request is completed, the Bartlett&rsquo;s the customer know that the label has been sent by email, and the conversation comes to an end.</li><li>This short example illustrates just one possible set of interactions that you might need an LLM to be capable of to power and application</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2025.png alt=Untitled></p><h3 id=llm-powered-applications>LLM-powered Applications <a href=#llm-powered-applications class=anchor aria-hidden=true>#</a></h3><ul><li>In general, connecting LLMs to external applications allows the model to interact with the broader world, extending their utility beyond language tasks.</li><li>As the shop bought example showed, LLMs can be used to trigger actions when given the ability to interact with APIs.</li><li>LLMs can also connect to other programming resources.</li><li>For example, a Python interpreter that can enable models to incorporate accurate calculations into their outputs.</li><li>It&rsquo;s important to note that prompts and completions are at the very heart of these workflows.</li><li>The actions that the app will take in response to user requests will be determined by the LLM, which serves as the application&rsquo;s reasoning engine</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2026.png alt=Untitled></p><h3 id=requirements-for-using-llms-to-power-applications>Requirements for Using LLMs to Power Applications <a href=#requirements-for-using-llms-to-power-applications class=anchor aria-hidden=true>#</a></h3><ul><li>In order to trigger actions, the completions generated by the LLM must contain certain important information.</li><li>First, the model needs to be able to generate a set of instructions so that the application knows what actions to take.</li><li>These instructions need to be understandable and correspond to allowed actions</li><li>In the ShopBot example for instance, the important steps were;<ul><li>checking the order ID,</li><li>requesting a shipping label,</li><li>verifying the user email, and</li><li>emailing the user the label.</li></ul></li><li>Second, the completion needs to be formatted in a way that the broader application can understand. This could be as simple as a specific sentence structure or as complex as writing a script in Python or generating a SQL command.<ul><li>For example, here is a SQL query that would determine whether an order is present in the database of all orders</li></ul></li><li>Lastly, the model may need to collect information that allows it to validate an action.<ul><li>For example, in the ShopBot conversation, the application needed to verify the email address the customer used to make the original order.</li><li>Any information that is required for validation needs to be obtained from the user and contained in the completion so it can be passed through to the application.</li></ul></li><li>Structuring the prompts in the correct way is important for all of these tasks and can make a huge difference in the quality of a plan generated or the adherence to a desired output format specification</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2027.png alt=Untitled></p><hr><h2 id=helping-llms-reason-and-plan-with-chain-of-thought-cot>Helping LLMs Reason and Plan with Chain of Thought (CoT) <a href=#helping-llms-reason-and-plan-with-chain-of-thought-cot class=anchor aria-hidden=true>#</a></h2><ul><li>It is important that LLMs can reason through the steps that an application must take, to satisfy a user request.</li><li>Unfortunately, complex reasoning can be challenging for LLMs, especially for problems that involve multiple steps or mathematics.</li><li>These problems exist even in large models that show good performance at many other tasks</li></ul><h3 id=llms-can-struggle-with-complex-reasoning-problems>LLMs Can Struggle with Complex Reasoning Problems <a href=#llms-can-struggle-with-complex-reasoning-problems class=anchor aria-hidden=true>#</a></h3><ul><li>Here&rsquo;s one example where an LLM has difficulty completing the task.<ul><li>You&rsquo;re asking the model to solve a simple multi-step math problem, to determine how many apples a cafeteria has after using some to make lunch, and then buying some more.</li><li>Your prompt includes a similar example problem, complete with the solution, to help the model understand the task through one-shot inference.</li><li>If you like, you can pause the video here for a moment and solve the problem yourself.</li><li>After processing the prompt, the model generates the completion shown here, stating that the answer is 27.</li><li>This answer is incorrect, as you found out if you solve the problem.</li><li>The cafeteria actually only has nine apples remaining</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2028.png alt=Untitled></p><ul><li>Researchers have been exploring ways to improve the performance of large language models on reasoning tasks, like the one you just saw.</li><li>One strategy that has demonstrated some success is prompting the model to think more like a human, by breaking the problem down into steps.</li><li>What do I mean by thinking more like a human?</li><li>Well, here is the one-shot example problem from the prompt on the previous slide.</li><li>The task here is to calculate how many tennis balls Roger has after buying some new ones.</li><li>One way that a human might tackle this problem is as follows.<ul><li>Begin by determining the number of tennis balls Roger has at the start.</li><li>Then note that Roger buys two cans of tennis balls. Each can contains three balls, so he has a total of six new tennis balls.</li><li>Next, add these 6 new balls to the original 5, for a total of 11 balls.</li><li>Then finish by stating the answer.</li></ul></li><li>These intermediate calculations form the reasoning steps that a human might take, and the full sequence of steps illustrates the chain of thought that went into solving the problem</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2029.png alt=Untitled></p><ul><li><p>Asking the model to mimic this behavior is known as Chain-of-Thought (CoT) prompting.</p></li><li><p>It works by including a series of intermediate reasoning steps into any examples that you use for one or few-shot inference.</p></li><li><p>By structuring the examples in this way, you&rsquo;re essentially teaching the model how to reason through the task to reach a solution</p></li><li><p>Here&rsquo;s the same apples problem you saw a couple of slides ago, now reworked as a chain of thought prompt.</p><ul><li>The story of Roger buying the tennis balls is still used as the one-shot example.</li><li>But this time you include intermediate reasoning steps in the solution text.</li><li>These steps are basically equivalent to the ones a human might take, that you saw just a few minutes ago.</li><li>You then send this chain of thought prompt to the large language model, which generates a completion.</li><li>Notice that the model has now produced a more robust and transparent response that explains its reasoning steps, following a similar structure as the one-shot example.</li><li>The model now correctly determines that nine apples are left.</li><li>Thinking through the problem has helped the model come to the correct answer.</li><li>One thing to note is that while the input prompt is shown here in a condensed format to save space, the entire prompt is actually included in the output</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2030.png alt=Untitled></p><ul><li>You can use chain of thought prompting to help LLMs improve their reasoning of other types of problems too, in addition to arithmetic</li><li>Here&rsquo;s an example of a simple physics problem, where the model is being asked to determine if a gold ring would sink to the bottom of a swimming pool.<ul><li>The chain of thought prompt included as the one-shot example here, shows the model how to work through this problem, by reasoning that a pair would flow because it&rsquo;s less dense than water.</li><li>When you pass this prompt to the LLM, it generates a similarly structured completion.</li><li>The model correctly identifies the density of gold, which it learned from its training data, and then reasons that the ring would sink because gold is much more dense than water</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2031.png alt=Untitled></p><ul><li>Chain-of-Thought prompting is a powerful technique that improves the ability of your model to reason through problems.</li><li>While this can greatly improve the performance of your model, the limited math skills of LLMs can still cause problems if your task requires accurate calculations, like totaling sales on an e-commerce site, calculating tax, or applying a discount</li><li>In the next lesson, you&rsquo;ll explore a technique that can help you overcome this problem, by letting your LLM talk to a program that is much better at math</li></ul><hr><h2 id=program-aided-language-pal-models>Program-aided Language (PAL) Models <a href=#program-aided-language-pal-models class=anchor aria-hidden=true>#</a></h2><ul><li>The ability of LLMs to carry out arithmetic and other mathematical operations is limited.</li><li>While you can try using Chain-of-Thought (CoT) prompting to overcome this, it will only get you so far.</li><li>Even if the model correctly reasons through a problem, it may still get the individual math operations wrong, especially with larger numbers or complex operations</li></ul><h3 id=llms-can-struggle-with-mathematics>LLMs Can Struggle with Mathematics <a href=#llms-can-struggle-with-mathematics class=anchor aria-hidden=true>#</a></h3><ul><li>Here&rsquo;s the example you saw earlier where the LLM tries to act like a calculator but gets the answer wrong.</li><li>Remember, the model isn&rsquo;t actually doing any real math here.</li><li>It is simply trying to predict the most probable tokens that complete the prompt.</li><li>The model getting math wrong can have many negative consequences depending on your use case, like charging customers the wrong total or getting the measurements for a recipe incorrect</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2032.png alt=Untitled></p><h3 id=pal-models>PAL Models <a href=#pal-models class=anchor aria-hidden=true>#</a></h3><ul><li>You can overcome this limitation by allowing your model to interact with external applications that are good at math, like a Python interpreter.</li><li>One interesting framework for augmenting LLMs in this way is called program-aided language models, or PAL for short.</li><li>This work first presented by Luyu Gao and collaborators at Carnegie Mellon University in 2022, pairs an LLM with an external code interpreter to carry out calculations.</li><li>The method makes use of Chain-of-Though (CoT) prompting to generate executable Python scripts.</li><li>The scripts that the model generates are passed to an interpreter to execute.</li><li>The image on the right here is taken from the paper and show some example prompts and completions</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2033.png alt=Untitled></p><ul><li>The strategy behind PAL is to have the LLM generate completions where reasoning steps are accompanied by computer code. This code is then passed to an interpreter to carry out the calculations necessary to solve the problem. You specify the output format for the model by including examples for one or few short inference in the prompt</li></ul><h3 id=pal-example>PAL Example <a href=#pal-example class=anchor aria-hidden=true>#</a></h3><ul><li>Let&rsquo;s take a closer look at how these example prompts are structured.</li><li>You&rsquo;ll continue to work with the story of Roger buying tennis balls as the one-shot example. The setup here should now look familiar. This is a chain of thought example.</li><li>You can see the reasoning steps written out in words on the lines highlighted in blue.</li><li>What differs from the prompts you saw before is the inclusion of lines of Python code shown in pink.</li><li>These lines translate any reasoning steps that involve calculations into code. Variables are declared based on the text in each reasoning step.</li><li>Their values are assigned either directly, as in the first line of code here, or as calculations using numbers present in the reasoning text as you see in the second Python line.</li><li>The model can also work with variables it creates in other steps, as you see in the third line</li><li>Note that the text of each reasoning step begins with a pound sign, so that the line can be skipped as a comment by the Python interpreter</li><li>The prompt here ends with the new problem to be solved. In this case, the objective is to determine how many loaves of bread a bakery has left after a day of sales and after some loaves are returned from a grocery store partner</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2034.png alt=Untitled></p><ul><li>On the right, you can see the completion generated by the LLM.<ul><li>Again, the chain of thought reasoning steps are shown in blue and the Python code is shown in pink.</li><li>As you can see, the model creates a number of variables to track the loaves baked, the loaves sold in each part of the day, and the loaves returned by the grocery store.</li><li>The answer is then calculated by carrying out arithmetic operations on these variables.</li><li>The model correctly identifies whether terms should be added or subtracted to reach the correct total</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2035.png alt=Untitled></p><ul><li>Now that you know how to structure examples that will tell the LLM to write Python scripts based on its reasoning steps, let&rsquo;s go over how the PAL framework enables an LLM to interact with an external interpreter.</li><li>To prepare for inference with PAL, you&rsquo;ll format your prompt to contain one or more examples.</li><li>Each example should contain a question followed by reasoning steps in lines of Python code that solve the problem</li><li>Next, you will append the new question that you&rsquo;d like to answer to the prompt template.</li><li>Your resulting PAL formatted prompt now contains both the example and the problem to solve.</li><li>Next, you&rsquo;ll pass this combined prompt to your LLM, which then generates a completion that is in the form of a Python script having learned how to format the output based on the example in the prompt.</li><li>You can now hand off the script to a Python interpreter, which you&rsquo;ll use to run the code and generate an answer. For the bakery example script you saw on the previous slide, the answer is 74</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2036.png alt=Untitled></p><ul><li>You&rsquo;ll now append the text containing the answer, which you know is accurate because the calculation was carried out in Python to the PAL formatted prompt you started with.</li><li>By this point you have a prompt that includes the correct answer in context</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2037.png alt=Untitled></p><ul><li><p>Now when you pass the updated prompt to the LLM, it generates a completion that contains the correct answer.</p></li><li><p>Given the relatively simple math in the bakery bread problem, it&rsquo;s likely that the model may have gotten the answer correct just with Chain-of-Thought prompting</p></li><li><p>But for more complex math, including arithmetic with large numbers, trigonometry or calculus, PAL is a powerful technique that allows you to ensure that any calculations done by your application are accurate and reliable</p></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2038.png alt=Untitled></p><h3 id=llm-powered-applications-2>LLM-powered Applications 2 <a href=#llm-powered-applications-2 class=anchor aria-hidden=true>#</a></h3><ul><li>You might be wondering how to automate this process so that you don&rsquo;t have to pass information back and forth between the LLM, and the interpreter by hand.</li><li>This is where the orchestrator that you saw earlier comes in.</li><li>The orchestrator shown here as the yellow box is a technical component that can manage the flow of information and the initiation of calls to external data sources or applications.</li><li>It can also decide what actions to take based on the information contained in the output of the LLM</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2039.png alt=Untitled></p><h3 id=pal-architecture>PAL Architecture <a href=#pal-architecture class=anchor aria-hidden=true>#</a></h3><ul><li>Remember, the LLM is your application&rsquo;s reasoning engine.</li><li>Ultimately, it creates the plan that the orchestrator will interpret and execute.</li><li>In PAL there&rsquo;s only one action to be carried out, the execution of Python code.</li><li>The LLM doesn&rsquo;t really have to decide to run the code, it just has to write the script which the orchestrator then passes to the external interpreter to run</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2040.png alt=Untitled></p><ul><li>However, most real-world applications are likely to be more complicated than the simple PAL architecture.</li><li>Your use case may require interactions with several external data sources.</li><li>As you saw in the shop bought example, you may need to manage multiple decision points, validation actions, and calls to external applications</li></ul><hr><h3 id=react-combining-reasoning-and-action>ReAct: Combining Reasoning and Action <a href=#react-combining-reasoning-and-action class=anchor aria-hidden=true>#</a></h3><ul><li>ReAct that can help LLMs plan out and execute workflows</li></ul><h3 id=react-synergizing-reasoning-and-action-in-llms>ReAct: Synergizing Reasoning and Action in LLMs <a href=#react-synergizing-reasoning-and-action-in-llms class=anchor aria-hidden=true>#</a></h3><ul><li>ReAct is a prompting strategy that combines Chain-of-Thought (CoT) reasoning with action planning.</li><li>The framework was proposed by researchers at Princeton and Google in 2022.</li><li>The paper develops a series of complex prompting examples based on problems from<ul><li>HotPot QA, a multi-step question answering benchmark, that requires reasoning over two or more Wikipedia passages and</li><li>Fever, a benchmark that uses Wikipedia passages to verify facts.</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2041.png alt=Untitled></p><ul><li>ReAct uses structured examples to show a large language model how to reason through a problem and decide on actions to take that move it closer to a solution.</li><li>The example prompts start with a question that will require multiple steps to answer.</li><li>In this example, the goal is to determine which of two magazines was created first.</li><li>The example then includes a related thought, action, observation trio of strings.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2042.png alt=Untitled></p><ul><li>The thought is a reasoning step that demonstrates to the model how to tackle the problem and identify an action to take.</li><li>In the newspaper publishing example, the prompt specifies that the model will search for both magazines and determine which one was published first.</li><li>In order for the model to interact with an external application or data source, it has to identify an action to take from a pre-determined list.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2043.png alt=Untitled></p><ul><li>In the case of the ReAct framework, the authors created a small Python API to interact with Wikipedia.</li><li>The three allowed actions are search, which looks for a Wikipedia entry about a particular topic lookup, which searches for a string on a Wikipedia page.</li><li>And finish, which the model carries out when it decides it has determined the answer.</li><li>As you saw on the previous slide, the thought in the prompt identified two searches to carry out one for each magazine.</li><li>In this example, the first search will be for Arthur&rsquo;s magazine.</li><li>The action is formatted using the specific square bracket notation you see here, so that the model will format its completions in the same way.</li><li>The Python interpreter searches for this code to trigger specific API actions.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2044.png alt=Untitled></p><ul><li>The last part of the prompt template is the observation, this is where the new information provided by the external search is brought into the context of the prompt for the model to interpret.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2045.png alt=Untitled></p><ul><li>The prompt then repeats the cycle as many times as is necessary to obtain the final answer.</li><li>In the second thought, the prompt states the start year of Arthur&rsquo;s magazine and identifies the next step needed to solve the problem.</li><li>The second action is to search for first for women, and the second observation includes text that states the start date of the publication, in this case 1989.</li><li>At this point, all the information required to answer the question is known.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2046.png alt=Untitled></p><ul><li>The third thought states the start year of first for women and then gives the explicit logic used to determine which magazine was published first.</li><li>The final action is to finish the cycle and pass the answer back to the user</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2047.png alt=Untitled></p><p>Note: In the ReAct framework, the LLM can only choose from a limited number of actions that are defined by a set of instructions that is pre-pended to the example prompt text</p><h3 id=react-instructions-define-the-action-space>ReAct Instructions Define the Action Space <a href=#react-instructions-define-the-action-space class=anchor aria-hidden=true>#</a></h3><ul><li>The full text of the instructions is shown here.</li><li>First, the task is defined, telling the model to answer a question using the prompt structure you just explored in detail.</li><li>Next, the instructions give more detail about what is meant by thought and then specifies that the action step can only be one of three types.<ul><li>The first is the search action, which looks for Wikipedia entries related to the specified entity.</li><li>The second is the lookup action, which retrieves the next sentence that contains the specified keyword.</li><li>The last action is finish, which returns the answer and brings the task to an end.</li></ul></li><li>It is critical to define a set of allowed actions when using LLMs to plan tasks that will power applications.</li><li>LLMs are very creative, and they may propose taking steps that don&rsquo;t actually correspond to something that the application can do.</li><li>The final sentence in the instructions lets the LLM know that some examples will come next in the prompt text</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2048.png alt=Untitled></p><h3 id=building-up-the-react-prompt>Building Up the ReAct Prompt <a href=#building-up-the-react-prompt class=anchor aria-hidden=true>#</a></h3><ul><li>Okay, so let&rsquo;s put all the pieces together, for inference.</li><li>You&rsquo;ll start with the ReAct example prompt.</li><li>Note that depending on the LLM you&rsquo;re working with, you may find that you need to include more than one example and carry out future inference.</li><li>Next, you&rsquo;ll prepend the instructions at the beginning of the example and then insert the question you want to answer at the end.</li><li>The full prompt now includes all of these individual pieces, and it can be passed to the LLM for inference</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2049.png alt=Untitled></p><ul><li>The ReAct framework shows one way to use LLMs to power an application through reasoning and action planning.</li><li>This strategy can be extended for your specific use case by creating examples that work through the decisions and actions that will take place in your application</li></ul><h3 id=langchain>LangChain <a href=#langchain class=anchor aria-hidden=true>#</a></h3><ul><li>The LangChain framework provides you with modular pieces that contain the components necessary to work with LLMs.</li><li>These components include prompt templates for many different use cases that you can use to format both input examples and model completions and memory that you can use to store interactions with an LLM.</li><li>The framework also includes pre-built tools that enable you to carry out a wide variety of tasks, including calls to external datasets and various APIs.</li><li>Connecting a selection of these individual components together results in a chain.</li><li>The creators of LangChain have developed a set of predefined chains that have been optimized for different use cases, and you can use these off the shelf to quickly get your app up and running</li><li>Sometimes your application workflow could take multiple paths depending on the information the user provides. In this case, you can&rsquo;t use a pre-determined chain, but instead we&rsquo;ll need the flexibility to decide which actions to take as the user moves through the workflow</li><li>LangChain defines another construct, known as an Agent, that you can use to interpret the input from the user and determine which tool or tools to use to complete the task.<ul><li>LangChain currently includes agents for both PAL and ReAct, among others.</li><li>Agents can be incorporated into chains to take an action or plan and execute a series of actions</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2050.png alt=Untitled></p><h3 id=the-significance-of-scale-application-building>The Significance of Scale: Application Building <a href=#the-significance-of-scale-application-building class=anchor aria-hidden=true>#</a></h3><ul><li>The ability of the model to reason well and plan actions depends on its scale.<ul><li>Larger models are generally your best choice for techniques that use advanced prompting, like PAL or ReAct.</li><li>Smaller models may struggle to understand the tasks in highly structured prompts and may require you to perform additional fine tuning to improve their ability to reason and plan. This could slow down your development process.</li></ul></li><li>If you start with a large, capable model and collect lots of user data in deployment, you may be able to use it to train and fine-tune a smaller model that you can switch to at a later time</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2051.png alt=Untitled></p><hr><h2 id=reading-react-reasoning-and-action>Reading: ReAct: Reasoning and Action <a href=#reading-react-reasoning-and-action class=anchor aria-hidden=true>#</a></h2><p><a href=https://arxiv.org/abs/2210.03629>This paper</a> introduces ReAct, a novel approach that integrates verbal reasoning and interactive decision making in large language models (LLMs). While LLMs have excelled in language understanding and decision making, the combination of reasoning and acting has been neglected. ReAct enables LLMs to generate reasoning traces and task-specific actions, leveraging the synergy between them. The approach demonstrates superior performance over baselines in various tasks, overcoming issues like hallucination and error propagation. ReAct outperforms imitation and reinforcement learning methods in interactive decision making, even with minimal context examples. It not only enhances performance but also improves interpretability, trustworthiness, and diagnosability by allowing humans to distinguish between internal knowledge and external information.</p><p>In summary, ReAct bridges the gap between reasoning and acting in LLMs, yielding remarkable results across language reasoning and decision making tasks. By interleaving reasoning traces and actions, ReAct overcomes limitations and outperforms baselines, not only enhancing model performance but also providing interpretability and trustworthiness, empowering users to understand the model&rsquo;s decision-making process.</p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2052.png alt=Untitled></p><p><strong>Image:</strong> The figure provides a comprehensive visual comparison of different prompting methods in two distinct domains. The first part of the figure (1a) presents a comparison of four prompting methods: Standard, Chain-of-thought (CoT, Reason Only), Act-only, and ReAct (Reason+Act) for solving a HotpotQA question. Each method&rsquo;s approach is demonstrated through task-solving trajectories generated by the model (Act, Thought) and the environment (Obs). The second part of the figure (1b) focuses on a comparison between Act-only and ReAct prompting methods to solve an AlfWorld game. In both domains, in-context examples are omitted from the prompt, highlighting the generated trajectories as a result of the model&rsquo;s actions and thoughts and the observations made in the environment. This visual representation enables a clear understanding of the differences and advantages offered by the ReAct paradigm compared to other prompting methods in diverse task-solving scenarios.</p><hr><h2 id=llm-application-architectures>LLM Application Architectures <a href=#llm-application-architectures class=anchor aria-hidden=true>#</a></h2><h3 id=infrastructure>Infrastructure <a href=#infrastructure class=anchor aria-hidden=true>#</a></h3><ul><li>To begin, let&rsquo;s bring everything you&rsquo;ve seen so far in the lesson together and look at the building blocks for creating LLM powered applications. You&rsquo;ll require several key components to create end-to-end solutions for your applications, starting with the infrastructure layer. This layer provides the compute, storage, and network to serve up your LLMs, as well as to host your application components. You can make use of your on-premises infrastructure for this or have it provided for you via on-demand and pay-as-you-go Cloud services</li></ul><h3 id=llm-models>LLM Models <a href=#llm-models class=anchor aria-hidden=true>#</a></h3><ul><li>Next, you&rsquo;ll include the large language models you want to use in your application. These could include foundation models, as well as the models you have adapted to your specific task. The models are deployed on the appropriate infrastructure for your inference needs. Taking into account whether you need real-time or near-real-time interaction with the model</li></ul><h3 id=information-sources>Information Sources <a href=#information-sources class=anchor aria-hidden=true>#</a></h3><ul><li>You may also have the need to retrieve information from external sources, such as those discussed in the Retrieval Augmented Generation (RAG) section</li></ul><h3 id=gather-outputs--feedback>Gather Outputs & Feedback <a href=#gather-outputs--feedback class=anchor aria-hidden=true>#</a></h3><ul><li>Your application will return the completions from your large language model to the user or consuming application. Depending on your use case, you may need to implement a mechanism to capture and store the outputs. For example, you could build the capacity to store user completions during a session to augment the fixed contexts window size of your LLM. You can also gather feedback from users that may be useful for additional fine-tuning, alignment, or evaluation as your application matures</li></ul><h3 id=llm-tools--frameworks>LLM Tools & Frameworks <a href=#llm-tools--frameworks class=anchor aria-hidden=true>#</a></h3><ul><li>Next, you may need to use additional tools and frameworks for large language models that help you easily implement some of the techniques discussed in this course. As an example, you can use LangChain built-in libraries to implement techniques like PAL, ReAct or Chain-of-Thought (CoT) prompting. You may also utilize model hubs which allow you to centrally manage and share models for use in applications</li></ul><h3 id=application-interfaces>Application Interfaces <a href=#application-interfaces class=anchor aria-hidden=true>#</a></h3><ul><li><p>In the final layer, you typically have some type of user interface that the application will be consumed through, such as a website or a REST API. This layer is where you&rsquo;ll also include the security components required for interacting with your application</p></li><li><p>At a high level, this architecture stack represents the various components to consider as part of your generative AI applications. Your users, whether they are human end-users or other systems that access your application through its APIs, will interact with this entire stack</p></li><li><p>As you can see, the model is typically only one part of the story in building end-to-end generative AI applications</p></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2053.png alt=Untitled></p><h3 id=summary>Summary <a href=#summary class=anchor aria-hidden=true>#</a></h3><ul><li>This week, you saw how to align your models with human preferences, such as helpfulness, harmlessness, and honesty (HHH) by fine-tuning using a technique called Reinforcement Learning with Human Feedback (RLHF).</li><li>Given the popularity of RLHF, there are many existing RL reward models and human alignment datasets available, enabling you to quickly start aligning your models.</li><li>In practice, RLHF is a very effective mechanism that you can use to improve the alignment of your models, reduce the toxicity of their responses, and let you use your models more safely in production.</li><li>You also saw important techniques to optimize your model for inference by reducing the size of the model through distillation, quantization, or pruning.</li><li>This minimizes the amount of hardware resources needed to serve your LLMs in production.</li><li>Lastly, you explored ways that you can help your model perform better in deployment through structured prompts and connections to external data sources and applications.</li><li>LLMs can play an amazing role as the reasoning engine in an application, exploiting their intelligence to power exciting, useful applications.</li><li>Frameworks like LangChain are making it possible to quickly build, deploy, and test LLM-powered applications, and it&rsquo;s a very exciting time for developers.</li></ul><hr><h2 id=optional-aws-sagemaker-jumpstart>Optional: AWS Sagemaker Jumpstart <a href=#optional-aws-sagemaker-jumpstart class=anchor aria-hidden=true>#</a></h2><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2054.png alt=Untitled></p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2055.png alt=Untitled></p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2056.png alt=Untitled></p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2057.png alt=Untitled></p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2058.png alt=Untitled></p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-3-part-2/Untitled%2059.png alt=Untitled></p><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"></div><div class="docs-navigation d-flex justify-content-between"><a href=/notes/nlp/generative-ai-with-llms/week-3-part-1/><div class="card my-1"><div class="card-body py-2">&larr; Week 3 Part 1 - Reinforcement Learning from Human Feedback</div></div></a><a class=ms-auto href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/><div class="card my-1"><div class="card-body py-2">Week 3 - Research Papers &rarr;</div></div></a></div></main></div></div></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-X0X8EQ5BBE"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-X0X8EQ5BBE")</script><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Â© 2023 <a class=text-muted href=/>iliyaML</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=/js/bootstrap.min.650aeec64c81d69d4c0850fc73c93da3f0330cec0a27772feed7f90f60baa5f47f1c45687d71914bdafd1c4e860d40f6dc08ede27a2f08431ff929c9a2d24621.js integrity="sha512-ZQruxkyB1p1MCFD8c8k9o/AzDOwKJ3cv7tf5D2C6pfR/HEVofXGRS9r9HE6GDUD23Ajt4novCEMf+SnJotJGIQ==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.3f0a703c54cbed82ca277187e23cf2d272da28c15ce7e33cde685d40b53d741893d5b74d35bb2d20a81f56c289084f245bdd0c9145d39d7094d3dfbc62d1326a.js integrity="sha512-PwpwPFTL7YLKJ3GH4jzy0nLaKMFc5+M83mhdQLU9dBiT1bdNNbstIKgfVsKJCE8kW90MkUXTnXCU09+8YtEyag==" crossorigin=anonymous defer></script>
<script src=/main.min.cb2e2ebbf2e4002f3117addc33582923b2b3ae5265c22944cd117ebec7abe61c170417c4506d7a0f8f0fc9053dfdf441421d53601ac467042ff3d06ec0ba07fa.js integrity="sha512-yy4uu/LkAC8xF63cM1gpI7KzrlJlwilEzRF+vser5hwXBBfEUG16D48PyQU9/fRBQh1TYBrEZwQv89BuwLoH+g==" crossorigin=anonymous defer></script>
<script src=/index.min.fc2363233f78fedae03a7c057049d7684c87bd2b548453ecd15a0c337fda7e88c087651cf9a80debb9d2c44d66f63df21f2ba3cc1f265028e07fd79038439167.js integrity="sha512-/CNjIz94/trgOnwFcEnXaEyHvStUhFPs0VoMM3/afojAh2Uc+agN67nSxE1m9j3yHyujzB8mUCjgf9eQOEORZw==" crossorigin=anonymous defer></script></body></html>