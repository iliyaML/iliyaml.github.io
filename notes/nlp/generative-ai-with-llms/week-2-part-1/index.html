<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-500.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><link rel=stylesheet href=/main.5a6da61787dd33a1f4266a84356b6bddb654307deead4955e92a35222a9ea082df264aa3d900cbd28a47bf139cbac4d6504566a6d330e46d959e92faa5df6f86.css integrity="sha512-Wm2mF4fdM6H0JmqENWtr3bZUMH3urUlV6So1IiqeoILfJkqj2QDL0opHvxOcusTWUEVmptMw5G2VnpL6pd9vhg==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Week 2 Part 1 - Fine-tuning LLMs with Instruction | iliyaML</title><meta name=description content="Week 2 Part 1 - Fine-tuning LLMs with Instruction"><link rel=canonical href=/notes/nlp/generative-ai-with-llms/week-2-part-1/><meta property="og:locale" content><meta property="og:type" content="article"><meta property="og:title" content="Week 2 Part 1 - Fine-tuning LLMs with Instruction"><meta property="og:description" content="Week 2 Part 1 - Fine-tuning LLMs with Instruction"><meta property="og:url" content="/notes/nlp/generative-ai-with-llms/week-2-part-1/"><meta property="og:site_name" content="iliyaML"><meta property="article:published_time" content="2023-05-23T13:59:39+01:00"><meta property="article:modified_time" content="2023-05-23T13:59:39+01:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content><meta name=twitter:creator content><meta name=twitter:title content="Week 2 Part 1 - Fine-tuning LLMs with Instruction"><meta name=twitter:description content="Week 2 Part 1 - Fine-tuning LLMs with Instruction"><meta name=twitter:card content="summary"><meta name=twitter:image:alt content="Week 2 Part 1 - Fine-tuning LLMs with Instruction"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"/#/schema/person/1","name":"","url":"/","sameAs":[],"image":{"@type":"ImageObject","@id":"/#/schema/image/1","url":"/\u003cnil\u003e","width":null,"height":null,"caption":""}},{"@type":"WebSite","@id":"/#/schema/website/1","url":"/","name":"iliyaML","description":"Hi, Iâ€™m Iliya ðŸ‘‹","publisher":{"@id":"/#/schema/person/1"}},{"@type":"WebPage","@id":"/notes/nlp/generative-ai-with-llms/week-2-part-1/","url":"/notes/nlp/generative-ai-with-llms/week-2-part-1/","name":"Week 2 Part 1 - Fine-tuning LLMs with Instruction","description":"Week 2 Part 1 - Fine-tuning LLMs with Instruction","isPartOf":{"@id":"/#/schema/website/1"},"about":{"@id":"/#/schema/person/1"},"datePublished":"2023-05-23T13:59:39CET","dateModified":"2023-05-23T13:59:39CET","breadcrumb":{"@id":"/notes/nlp/generative-ai-with-llms/week-2-part-1/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"/notes/nlp/generative-ai-with-llms/week-2-part-1/#/schema/image/2"},"inLanguage":"","potentialAction":[{"@type":"ReadAction","target":["/notes/nlp/generative-ai-with-llms/week-2-part-1/"]}]},{"@type":"BreadcrumbList","@id":"/notes/nlp/generative-ai-with-llms/week-2-part-1/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"/","url":"/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@id":"/notesnlpgenerative-ai-with-llmsweek-2-part-1/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"/notes/nlp/generative-ai-with-llms/week-2-part-1/#/schema/image/2","url":null,"contentUrl":null,"caption":"Week 2 Part 1 - Fine-tuning LLMs with Instruction"}]}]}</script><meta name=theme-color content="#fff"><link rel=icon href=/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=/site.webmanifest></head><body class="notes single"><div class=sticky-top><div class=header-bar></div><header class="navbar navbar-expand-lg navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/ aria-label=iliyaML>iliyaML</a>
<button class="btn btn-link order-0 ms-auto d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasExample aria-controls=offcanvasExample><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-more-horizontal"><circle cx="12" cy="12" r="1"/><circle cx="19" cy="12" r="1"/><circle cx="5" cy="12" r="1"/></svg></button><div class="offcanvas offcanvas-start d-lg-none" tabindex=-1 id=offcanvasExample aria-labelledby=offcanvasExampleLabel><div class=header-bar></div><div class=offcanvas-header><h5 class=offcanvas-title id=offcanvasExampleLabel>Browse notes</h5><button type=button class=btn-close data-bs-dismiss=offcanvas aria-label=Close></button></div><div class=offcanvas-body><aside class="doks-sidebar mt-n3"><nav id=doks-docs-nav aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-091fa9121c047db1dd48c3e2ab5f3c91 aria-expanded=false>
Machine Learning</button><div class=collapse id=section-091fa9121c047db1dd48c3e2ab5f3c91><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/machine-learning/k-nearest-neighbors/>K-Nearest Neighbors (KNN)</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/linear-regression/>Linear Regression</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/logistic-regression/>Logistic Regression</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a68b6412b3d8a605c374d3c59e02694 aria-expanded=false>
Deep Learning</button><div class=collapse id=section-6a68b6412b3d8a605c374d3c59e02694><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/deep-learning/neural-networks/>Neural Networks</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-85587c49afa2ea2adacd4bbcdb57d064 aria-expanded=true>
NLP</button><div class="collapse show" id=section-85587c49afa2ea2adacd4bbcdb57d064><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/papers/>Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/attention-is-all-you-need/>Attention is All You Need (2017)</a></li><li><a class="docs-link rounded" href=/notes/nlp/state-of-gpt-2023/>State of GPT (2023)</a></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-d275029ac7b5f661dae7a2b707f60c0e aria-expanded=true>
Generative AI with Large Langauge Models</button><div class="collapse show" id=section-d275029ac7b5f661dae7a2b707f60c0e><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/introduction/>Introduction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-1/>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/transformers-architecture/>Week 1 Part 1 - Transformers Architecture</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-2/>Week 1 Part 2 - Pre-training and Scaling Laws</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-research-papers/>Week 1 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/>Week 2 - Research Papers</a></li><li><a class="docs-link rounded active" href=/notes/nlp/generative-ai-with-llms/week-2-part-1/>Week 2 Part 1 - Fine-tuning LLMs with Instruction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-2/>Week 2 Part 2 - Parameter Efficient Fine-tuning</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-1/>Week 3 Part 1 - Reinforcement Learning from Human Feedback</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-2/>Week 3 Part 2 - LLM-powered Applications</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/>Week 3 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/course-conclusion/>Course Conclusion</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c33e404a441c6ba9648f88af3c68a1ca aria-expanded=false>
Statistics</button><div class=collapse id=section-c33e404a441c6ba9648f88af3c68a1ca><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/statistics/introduction/>Introduction</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-89320dbd4e0a792bc1676fde2d5bccfc aria-expanded=false>
DSA</button><div class=collapse id=section-89320dbd4e0a792bc1676fde2d5bccfc><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/dsa/data-structures-and-algorithms/>Data Structures & Algorithms</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-98bf5c1a28c1981bf8d74d4bc73ceaab aria-expanded=false>
System Design</button><div class=collapse id=section-98bf5c1a28c1981bf8d74d4bc73ceaab><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0460583622f03a52d7693094d6fa2452 aria-expanded=false>
Concepts</button><div class=collapse id=section-0460583622f03a52d7693094d6fa2452><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/concepts/basics/>Basics</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/20-concepts/>20 Concepts</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/25-golden-rules/>25 Golden Rules</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/key-steps/>Key Steps</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-1afa74da05ca145d3418aad9af510109 aria-expanded=false>
Design</button><div class=collapse id=section-1afa74da05ca145d3418aad9af510109><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/design/url-shortener/>URL Shortener</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/short-media-sharing-platform/>Short Media Sharing Platform</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/video-streaming-platform/>Video Streaming Platform</a></li></ul></div></li></ul></div></li></ul></nav></aside></div></div><button class="btn btn-menu order-2 d-block d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-end border-0 py-lg-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-lg-none"></div><div class="offcanvas-header d-lg-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>iliyaML</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body p-4 p-lg-0"><ul class="nav flex-column flex-lg-row align-items-lg-center mt-2 mt-lg-0 ms-lg-2 me-lg-auto"><li class=nav-item><a class="nav-link ps-0 py-1" href=/about/>About</a></li><li class=nav-item><a class="nav-link ps-0 py-1 active" href=/notes/nlp/generative-ai-with-llms/introduction/>Notes</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><form class="doks-search position-relative flex-grow-1 ms-lg-auto me-lg-2"><input id=search class="form-control is-search" type=search placeholder=Search... aria-label=Search... autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><hr class="text-black-50 my-4 d-lg-none"><ul class="nav flex-column flex-lg-row"><li class=nav-item><a class="nav-link social-link" href=https://github.com/iliyaML><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-lg-none">GitHub</small></a></li><li class=nav-item><a class="nav-link social-link" href=https://www.linkedin.com/in/iliya-mohamad-lokman><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-linkedin"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg><small class="ms-2 d-lg-none">LinkedIn</small></a></li></ul><hr class="text-black-50 my-4 d-lg-none"><button id=mode class="btn btn-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></div></div></nav></header></div><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-091fa9121c047db1dd48c3e2ab5f3c91 aria-expanded=false>
Machine Learning</button><div class=collapse id=section-091fa9121c047db1dd48c3e2ab5f3c91><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/machine-learning/k-nearest-neighbors/>K-Nearest Neighbors (KNN)</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/linear-regression/>Linear Regression</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/logistic-regression/>Logistic Regression</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a68b6412b3d8a605c374d3c59e02694 aria-expanded=false>
Deep Learning</button><div class=collapse id=section-6a68b6412b3d8a605c374d3c59e02694><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/deep-learning/neural-networks/>Neural Networks</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-85587c49afa2ea2adacd4bbcdb57d064 aria-expanded=true>
NLP</button><div class="collapse show" id=section-85587c49afa2ea2adacd4bbcdb57d064><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/papers/>Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/attention-is-all-you-need/>Attention is All You Need (2017)</a></li><li><a class="docs-link rounded" href=/notes/nlp/state-of-gpt-2023/>State of GPT (2023)</a></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-d275029ac7b5f661dae7a2b707f60c0e aria-expanded=true>
Generative AI with Large Langauge Models</button><div class="collapse show" id=section-d275029ac7b5f661dae7a2b707f60c0e><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/introduction/>Introduction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-1/>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/transformers-architecture/>Week 1 Part 1 - Transformers Architecture</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-2/>Week 1 Part 2 - Pre-training and Scaling Laws</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-research-papers/>Week 1 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/>Week 2 - Research Papers</a></li><li><a class="docs-link rounded active" href=/notes/nlp/generative-ai-with-llms/week-2-part-1/>Week 2 Part 1 - Fine-tuning LLMs with Instruction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-2/>Week 2 Part 2 - Parameter Efficient Fine-tuning</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-1/>Week 3 Part 1 - Reinforcement Learning from Human Feedback</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-2/>Week 3 Part 2 - LLM-powered Applications</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/>Week 3 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/course-conclusion/>Course Conclusion</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c33e404a441c6ba9648f88af3c68a1ca aria-expanded=false>
Statistics</button><div class=collapse id=section-c33e404a441c6ba9648f88af3c68a1ca><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/statistics/introduction/>Introduction</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-89320dbd4e0a792bc1676fde2d5bccfc aria-expanded=false>
DSA</button><div class=collapse id=section-89320dbd4e0a792bc1676fde2d5bccfc><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/dsa/data-structures-and-algorithms/>Data Structures & Algorithms</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-98bf5c1a28c1981bf8d74d4bc73ceaab aria-expanded=false>
System Design</button><div class=collapse id=section-98bf5c1a28c1981bf8d74d4bc73ceaab><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0460583622f03a52d7693094d6fa2452 aria-expanded=false>
Concepts</button><div class=collapse id=section-0460583622f03a52d7693094d6fa2452><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/concepts/basics/>Basics</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/20-concepts/>20 Concepts</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/25-golden-rules/>25 Golden Rules</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/key-steps/>Key Steps</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-1afa74da05ca145d3418aad9af510109 aria-expanded=false>
Design</button><div class=collapse id=section-1afa74da05ca145d3418aad9af510109><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/design/url-shortener/>URL Shortener</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/short-media-sharing-platform/>Short Media Sharing Platform</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/video-streaming-platform/>Video Streaming Platform</a></li></ul></div></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-091fa9121c047db1dd48c3e2ab5f3c91 aria-expanded=false>
Machine Learning</button><div class=collapse id=section-091fa9121c047db1dd48c3e2ab5f3c91><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/machine-learning/k-nearest-neighbors/>K-Nearest Neighbors (KNN)</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/linear-regression/>Linear Regression</a></li><li><a class="docs-link rounded" href=/notes/machine-learning/logistic-regression/>Logistic Regression</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6a68b6412b3d8a605c374d3c59e02694 aria-expanded=false>
Deep Learning</button><div class=collapse id=section-6a68b6412b3d8a605c374d3c59e02694><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/deep-learning/neural-networks/>Neural Networks</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-85587c49afa2ea2adacd4bbcdb57d064 aria-expanded=true>
NLP</button><div class="collapse show" id=section-85587c49afa2ea2adacd4bbcdb57d064><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/papers/>Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/attention-is-all-you-need/>Attention is All You Need (2017)</a></li><li><a class="docs-link rounded" href=/notes/nlp/state-of-gpt-2023/>State of GPT (2023)</a></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-d275029ac7b5f661dae7a2b707f60c0e aria-expanded=true>
Generative AI with Large Langauge Models</button><div class="collapse show" id=section-d275029ac7b5f661dae7a2b707f60c0e><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/introduction/>Introduction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-1/>Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/transformers-architecture/>Week 1 Part 1 - Transformers Architecture</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-part-2/>Week 1 Part 2 - Pre-training and Scaling Laws</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-1-research-papers/>Week 1 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/>Week 2 - Research Papers</a></li><li><a class="docs-link rounded active" href=/notes/nlp/generative-ai-with-llms/week-2-part-1/>Week 2 Part 1 - Fine-tuning LLMs with Instruction</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-2-part-2/>Week 2 Part 2 - Parameter Efficient Fine-tuning</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-1/>Week 3 Part 1 - Reinforcement Learning from Human Feedback</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-part-2/>Week 3 Part 2 - LLM-powered Applications</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/week-3-research-papers/>Week 3 - Research Papers</a></li><li><a class="docs-link rounded" href=/notes/nlp/generative-ai-with-llms/course-conclusion/>Course Conclusion</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-c33e404a441c6ba9648f88af3c68a1ca aria-expanded=false>
Statistics</button><div class=collapse id=section-c33e404a441c6ba9648f88af3c68a1ca><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/statistics/introduction/>Introduction</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-89320dbd4e0a792bc1676fde2d5bccfc aria-expanded=false>
DSA</button><div class=collapse id=section-89320dbd4e0a792bc1676fde2d5bccfc><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/dsa/data-structures-and-algorithms/>Data Structures & Algorithms</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-98bf5c1a28c1981bf8d74d4bc73ceaab aria-expanded=false>
System Design</button><div class=collapse id=section-98bf5c1a28c1981bf8d74d4bc73ceaab><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-0460583622f03a52d7693094d6fa2452 aria-expanded=false>
Concepts</button><div class=collapse id=section-0460583622f03a52d7693094d6fa2452><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/concepts/basics/>Basics</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/20-concepts/>20 Concepts</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/25-golden-rules/>25 Golden Rules</a></li><li><a class="docs-link rounded" href=/notes/system-design/concepts/key-steps/>Key Steps</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-1afa74da05ca145d3418aad9af510109 aria-expanded=false>
Design</button><div class=collapse id=section-1afa74da05ca145d3418aad9af510109><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/notes/system-design/design/url-shortener/>URL Shortener</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/short-media-sharing-platform/>Short Media Sharing Platform</a></li><li><a class="docs-link rounded" href=/notes/system-design/design/video-streaming-platform/>Video Streaming Platform</a></li></ul></div></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#instruction-fine-tuning>Instruction Fine-Tuning</a><ul><li><a href=#stage-2-of-generative-ai-project-lifecycle---adapt-and-align-model>Stage 2 of Generative AI Project Lifecycle - Adapt and Align Model</a></li><li><a href=#limitations-of-in-context-learning>Limitations of In-Context Learning</a></li><li><a href=#pre-training-recap>Pre-training Recap</a></li><li><a href=#fine-tuning-at-a-high-level>Fine-tuning at a High Level</a></li><li><a href=#instruction-fine-tuning-1>Instruction Fine-tuning</a></li><li><a href=#dataset-preparation-instruction-fine-tuning>Dataset Preparation: Instruction Fine-tuning</a></li><li><a href=#llm-fine-tuning-process>LLM Fine-tuning Process</a></li></ul></li><li><a href=#fine-tuning-on-a-single-task>Fine-Tuning on a Single Task</a><ul><li><a href=#catastrophic-forgetting>Catastrophic Forgetting</a></li><li><a href=#how-to-avoid-catastrophic-forgetting>How to Avoid Catastrophic Forgetting</a></li></ul></li><li><a href=#multi-task-instruction-fine-tuning>Multi-Task Instruction Fine-Tuning</a><ul><li><a href=#instruction-fine-tuning-with-flan>Instruction Fine-tuning with FLAN</a></li><li><a href=#flan-t5>FLAN-T5</a></li><li><a href=#samsum-dialogue-dataset>SAMSum: Dialogue Dataset</a></li><li><a href=#sample-flan-t5-prompt-templates>Sample FLAN-T5 Prompt Templates</a></li><li><a href=#improving-flan-t5s-summarization-capabilities>Improving FLAN-T5â€™s Summarization Capabilities</a></li><li><a href=#dialogsum-dataset>dialogsum Dataset</a></li><li><a href=#summary-before-fine-tuning>Summary Before Fine-tuning</a></li><li><a href=#summary-after-fine-tuning>Summary After Fine-tuning</a></li><li><a href=#fine-tuning-with-your-own-data>Fine-tuning with Your Own Data</a></li></ul></li><li><a href=#reading-scaling-instruct-models>Reading: Scaling Instruct Models</a></li><li><a href=#model-evaluation>Model Evaluation</a><ul><li><a href=#llm-evaluation-challenges>LLM Evaluation Challenges</a></li><li><a href=#llm-evaluation-metrics>LLM Evaluation Metrics</a></li><li><a href=#terminology-review>Terminology Review</a></li><li><a href=#rouge-score>ROUGE Score</a></li><li><a href=#rouge-1>ROUGE-1</a></li><li><a href=#rouge-2>ROUGE-2</a></li><li><a href=#rouge-l>ROUGE-L</a></li><li><a href=#rouge-hacking>ROUGE Hacking</a></li><li><a href=#rouge-clipping>ROUGE Clipping</a></li><li><a href=#bleu-score>BLEU Score</a></li><li><a href=#summary-evaluations-metrics>Summary: Evaluations Metrics</a></li></ul></li><li><a href=#benchmarks>Benchmarks</a><ul><li><a href=#glue>GLUE</a></li><li><a href=#superglue>SuperGLUE</a></li><li><a href=#glue-and-superglue-leaderboards>GLUE and SuperGLUE Leaderboards</a></li><li><a href=#benchmarks-for-massive-models>Benchmarks for Massive Models</a></li><li><a href=#helm>HELM</a></li></ul></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#instruction-fine-tuning>Instruction Fine-Tuning</a><ul><li><a href=#stage-2-of-generative-ai-project-lifecycle---adapt-and-align-model>Stage 2 of Generative AI Project Lifecycle - Adapt and Align Model</a></li><li><a href=#limitations-of-in-context-learning>Limitations of In-Context Learning</a></li><li><a href=#pre-training-recap>Pre-training Recap</a></li><li><a href=#fine-tuning-at-a-high-level>Fine-tuning at a High Level</a></li><li><a href=#instruction-fine-tuning-1>Instruction Fine-tuning</a></li><li><a href=#dataset-preparation-instruction-fine-tuning>Dataset Preparation: Instruction Fine-tuning</a></li><li><a href=#llm-fine-tuning-process>LLM Fine-tuning Process</a></li></ul></li><li><a href=#fine-tuning-on-a-single-task>Fine-Tuning on a Single Task</a><ul><li><a href=#catastrophic-forgetting>Catastrophic Forgetting</a></li><li><a href=#how-to-avoid-catastrophic-forgetting>How to Avoid Catastrophic Forgetting</a></li></ul></li><li><a href=#multi-task-instruction-fine-tuning>Multi-Task Instruction Fine-Tuning</a><ul><li><a href=#instruction-fine-tuning-with-flan>Instruction Fine-tuning with FLAN</a></li><li><a href=#flan-t5>FLAN-T5</a></li><li><a href=#samsum-dialogue-dataset>SAMSum: Dialogue Dataset</a></li><li><a href=#sample-flan-t5-prompt-templates>Sample FLAN-T5 Prompt Templates</a></li><li><a href=#improving-flan-t5s-summarization-capabilities>Improving FLAN-T5â€™s Summarization Capabilities</a></li><li><a href=#dialogsum-dataset>dialogsum Dataset</a></li><li><a href=#summary-before-fine-tuning>Summary Before Fine-tuning</a></li><li><a href=#summary-after-fine-tuning>Summary After Fine-tuning</a></li><li><a href=#fine-tuning-with-your-own-data>Fine-tuning with Your Own Data</a></li></ul></li><li><a href=#reading-scaling-instruct-models>Reading: Scaling Instruct Models</a></li><li><a href=#model-evaluation>Model Evaluation</a><ul><li><a href=#llm-evaluation-challenges>LLM Evaluation Challenges</a></li><li><a href=#llm-evaluation-metrics>LLM Evaluation Metrics</a></li><li><a href=#terminology-review>Terminology Review</a></li><li><a href=#rouge-score>ROUGE Score</a></li><li><a href=#rouge-1>ROUGE-1</a></li><li><a href=#rouge-2>ROUGE-2</a></li><li><a href=#rouge-l>ROUGE-L</a></li><li><a href=#rouge-hacking>ROUGE Hacking</a></li><li><a href=#rouge-clipping>ROUGE Clipping</a></li><li><a href=#bleu-score>BLEU Score</a></li><li><a href=#summary-evaluations-metrics>Summary: Evaluations Metrics</a></li></ul></li><li><a href=#benchmarks>Benchmarks</a><ul><li><a href=#glue>GLUE</a></li><li><a href=#superglue>SuperGLUE</a></li><li><a href=#glue-and-superglue-leaderboards>GLUE and SuperGLUE Leaderboards</a></li><li><a href=#benchmarks-for-massive-models>Benchmarks for Massive Models</a></li><li><a href=#helm>HELM</a></li></ul></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>Week 2 Part 1 - Fine-tuning LLMs with Instruction</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#instruction-fine-tuning>Instruction Fine-Tuning</a><ul><li><a href=#stage-2-of-generative-ai-project-lifecycle---adapt-and-align-model>Stage 2 of Generative AI Project Lifecycle - Adapt and Align Model</a></li><li><a href=#limitations-of-in-context-learning>Limitations of In-Context Learning</a></li><li><a href=#pre-training-recap>Pre-training Recap</a></li><li><a href=#fine-tuning-at-a-high-level>Fine-tuning at a High Level</a></li><li><a href=#instruction-fine-tuning-1>Instruction Fine-tuning</a></li><li><a href=#dataset-preparation-instruction-fine-tuning>Dataset Preparation: Instruction Fine-tuning</a></li><li><a href=#llm-fine-tuning-process>LLM Fine-tuning Process</a></li></ul></li><li><a href=#fine-tuning-on-a-single-task>Fine-Tuning on a Single Task</a><ul><li><a href=#catastrophic-forgetting>Catastrophic Forgetting</a></li><li><a href=#how-to-avoid-catastrophic-forgetting>How to Avoid Catastrophic Forgetting</a></li></ul></li><li><a href=#multi-task-instruction-fine-tuning>Multi-Task Instruction Fine-Tuning</a><ul><li><a href=#instruction-fine-tuning-with-flan>Instruction Fine-tuning with FLAN</a></li><li><a href=#flan-t5>FLAN-T5</a></li><li><a href=#samsum-dialogue-dataset>SAMSum: Dialogue Dataset</a></li><li><a href=#sample-flan-t5-prompt-templates>Sample FLAN-T5 Prompt Templates</a></li><li><a href=#improving-flan-t5s-summarization-capabilities>Improving FLAN-T5â€™s Summarization Capabilities</a></li><li><a href=#dialogsum-dataset>dialogsum Dataset</a></li><li><a href=#summary-before-fine-tuning>Summary Before Fine-tuning</a></li><li><a href=#summary-after-fine-tuning>Summary After Fine-tuning</a></li><li><a href=#fine-tuning-with-your-own-data>Fine-tuning with Your Own Data</a></li></ul></li><li><a href=#reading-scaling-instruct-models>Reading: Scaling Instruct Models</a></li><li><a href=#model-evaluation>Model Evaluation</a><ul><li><a href=#llm-evaluation-challenges>LLM Evaluation Challenges</a></li><li><a href=#llm-evaluation-metrics>LLM Evaluation Metrics</a></li><li><a href=#terminology-review>Terminology Review</a></li><li><a href=#rouge-score>ROUGE Score</a></li><li><a href=#rouge-1>ROUGE-1</a></li><li><a href=#rouge-2>ROUGE-2</a></li><li><a href=#rouge-l>ROUGE-L</a></li><li><a href=#rouge-hacking>ROUGE Hacking</a></li><li><a href=#rouge-clipping>ROUGE Clipping</a></li><li><a href=#bleu-score>BLEU Score</a></li><li><a href=#summary-evaluations-metrics>Summary: Evaluations Metrics</a></li></ul></li><li><a href=#benchmarks>Benchmarks</a><ul><li><a href=#glue>GLUE</a></li><li><a href=#superglue>SuperGLUE</a></li><li><a href=#glue-and-superglue-leaderboards>GLUE and SuperGLUE Leaderboards</a></li><li><a href=#benchmarks-for-massive-models>Benchmarks for Massive Models</a></li><li><a href=#helm>HELM</a></li></ul></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#notes>Notes</a></li><li><a href=#instruction-fine-tuning>Instruction Fine-Tuning</a><ul><li><a href=#stage-2-of-generative-ai-project-lifecycle---adapt-and-align-model>Stage 2 of Generative AI Project Lifecycle - Adapt and Align Model</a></li><li><a href=#limitations-of-in-context-learning>Limitations of In-Context Learning</a></li><li><a href=#pre-training-recap>Pre-training Recap</a></li><li><a href=#fine-tuning-at-a-high-level>Fine-tuning at a High Level</a></li><li><a href=#instruction-fine-tuning-1>Instruction Fine-tuning</a></li><li><a href=#dataset-preparation-instruction-fine-tuning>Dataset Preparation: Instruction Fine-tuning</a></li><li><a href=#llm-fine-tuning-process>LLM Fine-tuning Process</a></li></ul></li><li><a href=#fine-tuning-on-a-single-task>Fine-Tuning on a Single Task</a><ul><li><a href=#catastrophic-forgetting>Catastrophic Forgetting</a></li><li><a href=#how-to-avoid-catastrophic-forgetting>How to Avoid Catastrophic Forgetting</a></li></ul></li><li><a href=#multi-task-instruction-fine-tuning>Multi-Task Instruction Fine-Tuning</a><ul><li><a href=#instruction-fine-tuning-with-flan>Instruction Fine-tuning with FLAN</a></li><li><a href=#flan-t5>FLAN-T5</a></li><li><a href=#samsum-dialogue-dataset>SAMSum: Dialogue Dataset</a></li><li><a href=#sample-flan-t5-prompt-templates>Sample FLAN-T5 Prompt Templates</a></li><li><a href=#improving-flan-t5s-summarization-capabilities>Improving FLAN-T5â€™s Summarization Capabilities</a></li><li><a href=#dialogsum-dataset>dialogsum Dataset</a></li><li><a href=#summary-before-fine-tuning>Summary Before Fine-tuning</a></li><li><a href=#summary-after-fine-tuning>Summary After Fine-tuning</a></li><li><a href=#fine-tuning-with-your-own-data>Fine-tuning with Your Own Data</a></li></ul></li><li><a href=#reading-scaling-instruct-models>Reading: Scaling Instruct Models</a></li><li><a href=#model-evaluation>Model Evaluation</a><ul><li><a href=#llm-evaluation-challenges>LLM Evaluation Challenges</a></li><li><a href=#llm-evaluation-metrics>LLM Evaluation Metrics</a></li><li><a href=#terminology-review>Terminology Review</a></li><li><a href=#rouge-score>ROUGE Score</a></li><li><a href=#rouge-1>ROUGE-1</a></li><li><a href=#rouge-2>ROUGE-2</a></li><li><a href=#rouge-l>ROUGE-L</a></li><li><a href=#rouge-hacking>ROUGE Hacking</a></li><li><a href=#rouge-clipping>ROUGE Clipping</a></li><li><a href=#bleu-score>BLEU Score</a></li><li><a href=#summary-evaluations-metrics>Summary: Evaluations Metrics</a></li></ul></li><li><a href=#benchmarks>Benchmarks</a><ul><li><a href=#glue>GLUE</a></li><li><a href=#superglue>SuperGLUE</a></li><li><a href=#glue-and-superglue-leaderboards>GLUE and SuperGLUE Leaderboards</a></li><li><a href=#benchmarks-for-massive-models>Benchmarks for Massive Models</a></li><li><a href=#helm>HELM</a></li></ul></li></ul></nav></div></nav><p>These notes were developed using lectures/material/transcripts from the <a href=https://www.deeplearning.ai/courses/generative-ai-with-llms/>DeepLearning.AI & AWS - Generative AI with Large Language Models</a> course</p><h2 id=notes>Notes <a href=#notes class=anchor aria-hidden=true>#</a></h2><ul><li><p>Instruction Fine-Tuning</p><ul><li>Stage 2 of Generative AI Project Lifecycle - Adapt and Align Model<ul><li>Prompt Engineering</li><li>Fine-tuning</li><li>Align with Human Feedback</li><li>Evaluate</li></ul></li><li>Limitation of In-Context Learning (ICL)<ul><li>Does not work for smaller models</li><li>Examples take up valuable space in the context window</li></ul></li><li>Pre-training Recap<ul><li>Train LLM using vast amount (GB, TB, PB) on unstructured textual data via self-supervised learning</li></ul></li><li>Fine-tuning - supervised learning process where you use a dataset (GB, TB) of labeled examples (prompt completion pairs)<ul><li>Full Fine-tuning - updates all of the modelâ€™s weights<ul><li>Requires enough memory and compute budget to store and process all the gradients, optimizers and other components that are being updated during training</li></ul></li></ul></li><li>Instruction Fine-tuning - trains the model using examples that demonstrate how it should respond to a specific instruction<ul><li>Data Preparation - prepare instruction dataset<ul><li>Prompt template libraries<ul><li>Classification</li><li>Text Generation</li><li>Text Summarization</li></ul></li></ul></li><li>LLM Fine-tuning Process<ul><li>Divide into training, validation and test splits<ul><li>Validation accuracy - Measure LLM performance using the validation dataset</li><li>Test accuracy - Final performance evaluation using the holdout test dataset</li></ul></li><li>Select prompts from your training data set and pass them to the LLM, which then generates completions</li><li>Compare the distribution of the completion and that of the training label and use the standard <strong>Cross-Entropy</strong> function to calculate loss between the two token distributions</li><li>Use the calculated loss to update the model weights using <strong>Backpropagation</strong></li><li>This is repeated for many batches of prompt completion pairs and over several epochs, update the weights so that the modelâ€™s performance on the task improves</li><li>Results in a new version of the base model called an instruct model</li></ul></li></ul></li></ul></li><li><p>Fine-tuning on a Single Task</p><ul><li>Training on 5-1k labeled examples can result in good performance</li><li>Downside: the process may lead to Catastrophic Forgetting</li><li>How to Avoid Catastrophic Forgetting?<ol><li>Fine-tune on multiple tasks at the same time</li><li>Consider Parameter Efficient Fine-tuning (PEFT)</li></ol></li></ul></li><li><p>Multi-task Instruction Fine-tuning</p><ul><li>An extension of single task fine-tuning, where the training dataset is comprised of example inputs and outputs for multiple tasks: Summarization, Review Rating, Code Translation, Entity Recognition</li><li>Avoids the issue of Catastrophic Forgetting</li><li>Drawback: may need 50-100k examples in your training set</li><li>FLAN (Fine-tuned LAnguage Net) is a specific set of instructions used to fine-tune different models (last step of the training process)<ul><li>FLAN-T5 - Fine-tuned version of pre-trained T5 model</li><li>FLAN-PALM - Fine-tuned version of pre-trained PALM model</li></ul></li><li>Improving Summarization Capabilities</li></ul></li><li><p>Reading: Scaling Instruct Models</p><ul><li>Introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application</li><li>The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model</li></ul></li><li><p>Model Evaluation</p><ul><li>With LLMs, the output is non-deterministic therefore much more challenging to evaluate</li><li>ROUGE (The Recall-Oriented Understudy for Gisting Evaluation) - use for diagnostic evaluation of summarization tasks<ul><li>Compares a summary to one or more reference summaries</li></ul></li><li>BLEU (Bilingual Evaluation Understudy) - use for diagnostic evaluation of translation tasks<ul><li>Compares to human-generated translations</li></ul></li></ul></li><li><p>Benchmarks - evaluating its performance on data that it hasn&rsquo;t seen before</p><ul><li>GLUE - General Language Understanding Evaluation</li><li>SuperGLUE</li></ul><p>Benchmarks for Massive Models</p><ul><li>MMLU - Massive Multi-task Language Understanding</li><li>BIG-bench - Beyond the Imitation Game Benchmark</li><li>HELM - Holistic Evaluation of Language Models<ul><li>Also include metrics for:<ol><li>Accuracy</li><li>Calibration</li><li>Robustness</li><li>Fairness</li><li>Bias</li><li>Toxicity</li><li>Efficiency</li></ol></li></ul></li></ul></li></ul><hr><p><strong>Introduction - Week 2</strong></p><ul><li>Take a look at instruction fine-tuning, so when you have your base model, the thing that&rsquo;s initially pretrained, it&rsquo;s encoded a lot of really good information, usually about the world. So it knows about things, but it doesn&rsquo;t necessarily know how to be able to respond to our prompts, our questions. So when we instruct it to do a certain task, it doesn&rsquo;t necessarily know how to respond. And so instruction fine-tuning helps it to be able to change its behavior to be more helpful for us</li><li>Because by learning off general text off the Internet and other sources, you learn to predict the next word. By predicting what&rsquo;s the next word on the Internet is not the same as following instructions. I thought it&rsquo;s amazing you can take a large language model, train it on hundreds of billions of words off the Internet. And then fine-tune it with a much smaller data set on following instructions and just learn to do that</li><li>have to watch out for, of course, is catastrophic forgetting and this is something that we talk about in the course. So that&rsquo;s where you train the model on some extra data in this insane instruct fine-tuning. And then it forgets all of that stuff that it had before, or a big chunk of that data that it had before. And so there are some techniques that we&rsquo;ll talk about in the course to help combat that. Such as doing instruct fine-tuning across a really broad range of different instruction types. So it&rsquo;s not just a case of just tuning it on just the thing you want it to do. You might have to be a little bit broader than that as well, but we talk about it in the course</li><li>One of the problems with fine-tuning is you take a giant model and you fine-tune every single parameter in that model. You have this big thing to store around and deploy, and it&rsquo;s actually very compute and memory expansive</li><li>talk about parameter efficient fine-tuning or PEFT for short, as a set of methods that can allow you to mitigate some of those concerns, right? So we have a lot of customers that do want to be able to tune for very specific tasks, very specific domains. And parameter efficient fine-tuning is a great way to still achieve similar performance results on a lot of tasks that you can with full fine-tuning. But then actually take advantage of techniques that allow you to freeze those original model weights. Or add adaptive layers on top of that with a much smaller memory footprint, So that you can train for multiple tasks</li><li>one of the techniques that I know you&rsquo;ve used a lot is LoRA</li><li>see a lot of excitement demand around LoRA because of the performance results of using those low rank matrices as opposed to full fine-tuning, right? So you&rsquo;re able to get really good performance results with minimal compute and memory requirements</li><li>many developers will often start off with prompting, and sometimes that gives you good enough performance and that&rsquo;s great. And sometimes prompting hits a ceiling in performance and then this type of fine-tuning with LoRA or other PEFT technique is really critical for unlocking that extra level performance. And then the other thing I&rsquo;m seeing among a lot of OM developers is a discussion debate about the cost of using a giant model, which is a lot of benefits versus for your application fine-tuning a smaller model</li><li>full fine tuning can be cost prohibitive, right? To say the least so the ability to actually be able to use techniques like PEFT to put fine-tuning generative AI models kind of in the hands of everyday users. That do have those cost constraints and they&rsquo;re cost conscious, which is pretty much everyone in the real world</li></ul><hr><h2 id=instruction-fine-tuning>Instruction Fine-Tuning <a href=#instruction-fine-tuning class=anchor aria-hidden=true>#</a></h2><h3 id=stage-2-of-generative-ai-project-lifecycle---adapt-and-align-model>Stage 2 of Generative AI Project Lifecycle - Adapt and Align Model <a href=#stage-2-of-generative-ai-project-lifecycle---adapt-and-align-model class=anchor aria-hidden=true>#</a></h3><ul><li>You&rsquo;ll learn about methods that you can use to improve the performance of an existing model for your specific use case.</li><li>You&rsquo;ll also learn about important metrics that can be used to evaluate the performance of your finetuned LLM and quantify its improvement over the base model you started with</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled.png alt=Untitled></p><h3 id=limitations-of-in-context-learning>Limitations of In-Context Learning <a href=#limitations-of-in-context-learning class=anchor aria-hidden=true>#</a></h3><ul><li>Limitations of in-context learning<ul><li>First, for smaller models, it doesn&rsquo;t always work, even when five or six examples are included.</li><li>Second, any examples you include in your prompt take up valuable space in the context window, reducing the amount of room you have to include other useful information.</li></ul></li><li>Luckily, another solution exists, you can take advantage of a process known as fine-tuning to further train a base model</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%201.png alt=Untitled></p><h3 id=pre-training-recap>Pre-training Recap <a href=#pre-training-recap class=anchor aria-hidden=true>#</a></h3><ul><li>You train the LLM using vast amounts of unstructured textual data via self- supervised learning</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%202.png alt=Untitled></p><h3 id=fine-tuning-at-a-high-level>Fine-tuning at a High Level <a href=#fine-tuning-at-a-high-level class=anchor aria-hidden=true>#</a></h3><ul><li>Fine-tuning is a supervised learning process where you use a data set of labeled examples to update the weights of the LLM.<ul><li>The labeled examples are prompt completion pairs, the fine-tuning process extends the training of the model to improve its ability to generate good completions for a specific task</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%203.png alt=Untitled></p><ul><li>One strategy, known as instruction fine-tuning, is particularly good at improving a model&rsquo;s performance on a variety of tasks</li></ul><h3 id=instruction-fine-tuning-1>Instruction Fine-tuning <a href=#instruction-fine-tuning-1 class=anchor aria-hidden=true>#</a></h3><ul><li>Instruction fine-tuning trains the model using examples that demonstrate how it should respond to a specific instruction</li><li>Here are a couple of example prompts to demonstrate this idea.</li><li>The instruction in both examples is classify this review, and the desired completion is a text string that starts with sentiment followed by either positive or negative.</li><li>The dataset you use for training includes many pairs of prompt completion examples for the task you&rsquo;re interested in, each of which includes an instruction.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%204.png alt=Untitled></p><ul><li>For example, if you want to fine-tune your model to improve its summarization ability, you&rsquo;d build up a data set of examples that begin with the instruction â€œsummarize the following textâ€ or a similar phrase.</li><li>And if you are improving the model&rsquo;s translation skills, your examples would include instructions like â€œtranslate this sentenceâ€.</li><li>These prompt completion examples allow the model to learn to generate responses that follow the given instructions</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%205.png alt=Untitled></p><ul><li>Instruction fine-tuning, where all of the model&rsquo;s weights are updated is known as <strong>Full Fine-tuning</strong>.</li><li>The process results in a new version of the model with updated weights.</li><li>It is important to note that just like pre-training, full fine-tuning requires enough memory and compute budget to store and process all the gradients, optimizers and other components that are being updated during training.</li><li>So you can benefit from the memory optimization and parallel computing strategies that you learned about last week</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%206.png alt=Untitled></p><p>How do you actually go about Instruction Fine-tuning an LLM?</p><h3 id=dataset-preparation-instruction-fine-tuning>Dataset Preparation: Instruction Fine-tuning <a href=#dataset-preparation-instruction-fine-tuning class=anchor aria-hidden=true>#</a></h3><ul><li><p>There are many publicly available datasets that have been used to train earlier generations of language models, although most of them are not formatted as instructions.</p></li><li><p>Luckily, developers have assembled prompt template libraries that can be used to take existing datasets, for example, the large data set of Amazon product reviews and turn them into instruction prompt datasets for fine-tuning.</p></li><li><p>Here are three prompts that are designed to work with the Amazon reviews dataset and that can be used to fine tune models for</p><ol><li>classification,</li><li>text generation and</li><li>text summarization tasks</li></ol></li><li><p>You can see that in each case you pass the original review, here called review_body, to the template, where it gets inserted into the text that starts with an instruction like predict the associated rating, generate a star review, or give a short sentence describing the following product review.</p></li><li><p>The result is a prompt that now contains both an instruction and the example from the data set</p></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%207.png alt=Untitled></p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%208.png alt=Untitled></p><h3 id=llm-fine-tuning-process>LLM Fine-tuning Process <a href=#llm-fine-tuning-process class=anchor aria-hidden=true>#</a></h3><ul><li>Once you have your instruction data set ready, as with standard supervised learning, you divide the data set into<ul><li>training,</li><li>validation and</li><li>test splits</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%209.png alt=Untitled></p><ul><li>During fine-tuning, you select prompts from your training data set and pass them to the LLM, which then generates completions.</li><li>Next, you compare the LLM completion with the response specified in the training data.</li><li>You can see here that the model didn&rsquo;t do a great job, it classified the review as neutral, which is a bit of an understatement. The review is clearly very positive.</li><li>Remember that the output of an LLM is a probability distribution across tokens. So you can compare the distribution of the completion and that of the training label and use the standard <strong>Cross-Entropy</strong> function to calculate loss between the two token distributions.</li><li>And then use the calculated loss to update your model weights in standard <strong>Backpropagation</strong>.</li><li>You&rsquo;ll do this for many batches of prompt completion pairs and over several epochs, update the weights so that the model&rsquo;s performance on the task improves</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2010.png alt=Untitled></p><ul><li>As in standard supervised learning, you can define separate evaluation steps to measure your LLM performance using the holdout validation data set. This will give you the validation accuracy</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2011.png alt=Untitled></p><ul><li>After you&rsquo;ve completed your fine-tuning, you can perform a final performance evaluation using the holdout test data set. This will give you the test accuracy.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2012.png alt=Untitled></p><ul><li>The fine-tuning process results in a new version of the base model, often called an instruct model that is better at the tasks you are interested in</li><li>Fine-tuning with instruction prompts is the most common way to fine-tune LLMs these days.</li><li>From this point on, when you hear or see the term fine-tuning, you can assume that it always means Instruction Fine-tuning</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2013.png alt=Untitled></p><hr><h2 id=fine-tuning-on-a-single-task>Fine-Tuning on a Single Task <a href=#fine-tuning-on-a-single-task class=anchor aria-hidden=true>#</a></h2><ul><li>LLMs have become famous for their ability to perform many different language tasks within a single model, your application may only need to perform a single task</li><li>You can fine-tune a pre-trained model to improve performance on only the task that is of interest to you</li><li>For example, summarization using a dataset of examples for that task. Interestingly, good results can be achieved with relatively few examples.<ul><li>Often just 500-1,000 examples can result in good performance in contrast to the billions of pieces of texts that the model saw during pre-training.</li></ul></li><li>However, there is a potential downside to fine-tuning on a single task. The process may lead to a phenomenon called <strong>Catastrophic Forgetting</strong>.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2014.png alt=Untitled></p><h3 id=catastrophic-forgetting>Catastrophic Forgetting <a href=#catastrophic-forgetting class=anchor aria-hidden=true>#</a></h3><ul><li>Catastrophic forgetting happens because the full fine-tuning process modifies the weights of the original LLM.</li><li>While this leads to great performance on the single fine-tuning task, it can degrade performance on other tasks.</li><li>For example, while fine-tuning can improve the ability of a model to perform sentiment analysis on a review and result in a quality completion, the model may forget how to do other tasks.</li><li>This model knew how to carry out named entity recognition before fine-tuning correctly identifying Charlie as the name of the cat in the sentence.</li><li>But after fine-tuning, the model can no longer carry out this task, confusing both the entity it is supposed to identify and exhibiting behavior related to the new task</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2015.png alt=Untitled></p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2016.png alt=Untitled></p><h3 id=how-to-avoid-catastrophic-forgetting>How to Avoid Catastrophic Forgetting <a href=#how-to-avoid-catastrophic-forgetting class=anchor aria-hidden=true>#</a></h3><ul><li>First of all, it&rsquo;s important to decide whether catastrophic forgetting actually impacts your use case. If all you need is reliable performance on the single task you fine-tuned on, it may not be an issue that the model can&rsquo;t generalize to other tasks. If you do want or need the model to maintain its multitask generalized capabilities, you can perform fine-tuning on multiple tasks at one time. Good multitask fine-tuning may require 50-100,000 examples across many tasks, and so will require more data and compute to train. Will discuss this option in more detail shortly.</li><li>Our second option is to perform parameter efficient fine-tuning, or PEFT for short instead of full fine-tuning. PEFT is a set of techniques that preserves the weights of the original LLM and trains only a small number of task-specific adapter layers and parameters. PEFT shows greater robustness to catastrophic forgetting since most of the pre-trained weights are left unchanged. PEFT is an exciting and active area of research that we will cover later this week</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2017.png alt=Untitled></p><p>Question:</p><p>Which of the following are true in respect to Catastrophic Forgetting? Select all that apply.</p><p><strong>Catastrophic forgetting occurs when a machine learning model forgets previously learned information as it learns new information.</strong></p><p>Correct</p><p>The assertion is true, and this process is especially problematic in sequential learning scenarios where the model is trained on multiple tasks over time.</p><p><strong>One way to mitigate catastrophic forgetting is by using regularization techniques to limit the amount of change that can be made to the weights of the model during training.</strong></p><p>Correct</p><p>One way to mitigate catastrophic forgetting is by using regularization techniques to limit the amount of change that can be made to the weights of the model during training. This can help to preserve the information learned during earlier training phases and prevent overfitting to the new data.</p><hr><p><strong>Catastrophic forgetting is a common problem in machine learning, especially in deep learning models.</strong></p><p>Correct</p><p>This assertion is true because these models typically have many parameters, which can lead to overfitting and make it more difficult to retain previously learned information.</p><hr><h2 id=multi-task-instruction-fine-tuning>Multi-Task Instruction Fine-Tuning <a href=#multi-task-instruction-fine-tuning class=anchor aria-hidden=true>#</a></h2><ul><li>Multitask fine-tuning is an extension of single task fine-tuning, where the training dataset is comprised of example inputs and outputs for multiple tasks.</li><li>Here, the dataset contains examples that instruct the model to carry out a variety of tasks, including summarization, review rating, code translation, and entity recognition.</li><li>You train the model on this mixed dataset so that it can improve the performance of the model on all the tasks simultaneously, thus avoiding the issue of catastrophic forgetting.</li><li>Over many epochs of training, the calculated losses across examples are used to update the weights of the model, resulting in an instruction tuned model that is learned how to be good at many different tasks simultaneously.</li><li>One drawback to multitask fine-tuning is that it requires a lot of data. You may need as many as 50-100,000 examples in your training set.</li><li>However, it can be really worthwhile and worth the effort to assemble this data.</li><li>The resulting models are often very capable and suitable for use in situations where good performance at many tasks is desirable</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2018.png alt=Untitled></p><p>Let&rsquo;s take a look at one family of models that have been trained using multitask instruction fine-tuning</p><h3 id=instruction-fine-tuning-with-flan>Instruction Fine-tuning with FLAN <a href=#instruction-fine-tuning-with-flan class=anchor aria-hidden=true>#</a></h3><ul><li>Instruct model variance differ based on the datasets and tasks used during fine-tuning.</li><li>One example is the FLAN family of models.</li><li>FLAN (Fine-tuned LAnguage Net) is a specific set of instructions used to fine-tune different models.</li><li>Because they&rsquo;re FLAN fine-tuning is the last step of the training process the authors of the original paper called it â€œthe metaphorical dessert to the main course of pre-trainingâ€.</li><li>FLAN-T5, the FLAN instruct version of the T5 foundation model while FLAN-PALM is the FLAN instruct version of the palm foundation model</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2019.png alt=Untitled></p><h3 id=flan-t5>FLAN-T5 <a href=#flan-t5 class=anchor aria-hidden=true>#</a></h3><ul><li>FLAN-T5 is a great general purpose instruct model.</li><li>In total, it&rsquo;s been fine tuned on 473 datasets across 146 task categories.</li><li>Those datasets are chosen from other models and papers as shown here</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2020.png alt=Untitled></p><ul><li>One example of a prompt dataset used for summarization tasks in FLAN-T5 is SAMSum.</li><li>It&rsquo;s part of the muffin collection of tasks and datasets and is used to train language models to summarize dialogue</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2021.png alt=Untitled></p><h3 id=samsum-dialogue-dataset>SAMSum: Dialogue Dataset <a href=#samsum-dialogue-dataset class=anchor aria-hidden=true>#</a></h3><ul><li>SAMSum is a dataset with 16,000 messenger like conversations with summaries. Three examples are shown here with the dialogue on the left and the summaries on the right.</li><li>The dialogues and summaries were crafted by linguists for the express purpose of generating a high-quality training dataset for language models.</li><li>The linguists were asked to create conversations similar to those that they would write on a daily basis, reflecting their proportion of topics of their real life messenger conversations.</li><li>Although language experts then created short summaries of those conversations that included important pieces of information and names of the people in the dialogue</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2022.png alt=Untitled></p><h3 id=sample-flan-t5-prompt-templates>Sample FLAN-T5 Prompt Templates <a href=#sample-flan-t5-prompt-templates class=anchor aria-hidden=true>#</a></h3><ul><li>Here is a prompt template designed to work with this SAMSum dialogue summary dataset.</li><li>The template is actually comprised of several different instructions that all basically ask the model to do this same thing.</li><li>Summarize a dialogue.<ul><li>For example, briefly summarize that dialogue.</li><li>What is a summary of this dialogue?</li><li>What was going on in that conversation?</li></ul></li><li>Including different ways of saying the same instruction helps the model generalize and perform better.</li><li>Just like the prompt templates you saw earlier, you see that in each case, the <strong>dialogue</strong> from the SAMSum dataset is inserted into the template wherever the dialogue field appears.</li><li>The <strong>summary</strong> is used as the label.</li><li>After applying this template to each row in the SAMSum dataset, you can use it to fine tune a dialogue summarization task</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2023.png alt=Untitled></p><p>While FLAN-T5 is a great general use model that shows good capability in many tasks. You may still find that it has room for improvement on tasks for your specific use case</p><h3 id=improving-flan-t5s-summarization-capabilities>Improving FLAN-T5â€™s Summarization Capabilities <a href=#improving-flan-t5s-summarization-capabilities class=anchor aria-hidden=true>#</a></h3><ul><li>For example, imagine you&rsquo;re a data scientist building an app to support your customer service team, process requests received through a chat bot, like the one shown here.</li><li>Your customer service team needs a summary of every dialogue to identify the key actions that the customer is requesting and to determine what actions should be taken in response</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2024.png alt=Untitled></p><ul><li>The SAMSum dataset gives FLAN-T5 some abilities to summarize conversations.</li><li>However, the examples in the dataset are mostly conversations between friends about day-to-day activities and don&rsquo;t overlap much with the language structure observed in customer service chats.</li><li>You can perform additional fine-tuning of the FLAN-T5 model using a dialogue dataset that is much closer to the conversations that happened with your bot.</li></ul><h3 id=dialogsum-dataset>dialogsum Dataset <a href=#dialogsum-dataset class=anchor aria-hidden=true>#</a></h3><ul><li>This is the exact scenario that you&rsquo;ll explore in the lab this week</li><li>You&rsquo;ll make use of an additional domain specific summarization dataset called dialogsum to improve FLAN-T5&rsquo;s is ability to summarize support chat conversations.</li><li>This dataset consists of over 13,000 support chat dialogues and summaries.</li><li>The dialogsum dataset is not part of the FLAN-T5 training data, so the model has not seen these conversations before</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2025.png alt=Untitled></p><ul><li>Let&rsquo;s take a look at example from dialogsum and discuss how a further round of fine-tuning can improve the model.</li><li>This is a support chat that is typical of the examples in the dialogsum dataset.</li><li>The conversation is between a customer and a staff member at a hotel check-in desk.</li><li>The chat has had a template applied so that the instruction to summarize the conversation is included at the start of the text</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2026.png alt=Untitled></p><p>Now, let&rsquo;s take a look at how FLAN-T5 responds to this prompt before doing any additional fine-tuning</p><h3 id=summary-before-fine-tuning>Summary Before Fine-tuning <a href=#summary-before-fine-tuning class=anchor aria-hidden=true>#</a></h3><ul><li>Note that the prompt is now condensed on the left to give you more room to examine the completion of the model.</li><li>Here is the model&rsquo;s response to the instruction.</li><li>You can see that the model does as it&rsquo;s able to identify that the conversation was about a reservation for Tommy.</li><li>However, it does not do as well as the human-generated baseline summary, which includes important information such as Mike asking for information to facilitate check-in and the models completion has also invented information that was not included in the original conversation, specifically the name of the hotel and the city it was located in</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2027.png alt=Untitled></p><h3 id=summary-after-fine-tuning>Summary After Fine-tuning <a href=#summary-after-fine-tuning class=anchor aria-hidden=true>#</a></h3><ul><li>Hopefully, you will agree that this is closer to the human-produced summary.</li><li>There is no fabricated information and the summary includes all of the important details, including the names of both people participating in the conversation</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2028.png alt=Untitled></p><p>This example, use the public dialogsum dataset to demonstrate fine-tuning on custom data</p><h3 id=fine-tuning-with-your-own-data>Fine-tuning with Your Own Data <a href=#fine-tuning-with-your-own-data class=anchor aria-hidden=true>#</a></h3><ul><li>In practice, you&rsquo;ll get the most out of fine-tuning by using your company&rsquo;s own internal data.</li><li>For example, the support chat conversations from your customer support application.<ul><li>This will help the model learn the specifics of how your company likes to summarize conversations and what is most useful to your customer service colleagues</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2029.png alt=Untitled></p><p>Question: What is the purpose of fine-tuning with prompt datasets?</p><p><strong>To improve the performance and adaptability of a pre-trained language model for specific tasks.</strong></p><hr><p>Correct</p><p>This option accurately describes the purpose of fine-tuning with prompt datasets. It aims to improve the performance and adaptability of a pre-trained language model by training it on specific tasks using instruction prompts.</p><hr><h2 id=reading-scaling-instruct-models>Reading: Scaling Instruct Models <a href=#reading-scaling-instruct-models class=anchor aria-hidden=true>#</a></h2><p>FLAN - Fine-tuned LAnguage Net</p><p><a href=https://arxiv.org/abs/2210.11416>This paper</a> introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application. The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model. The paper also provides detailed information on how each of these aspects was evaluated.</p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2030.png alt=Untitled></p><p>Here is the image from the lecture slides that illustrates the fine-tuning tasks and datasets employed in training FLAN. The task selection expands on previous works by incorporating dialogue and program synthesis tasks from Muffin and integrating them with new Chain of Thought Reasoning tasks. It also includes subsets of other task collections, such as T0 and Natural Instructions v2. Some tasks were held-out during training, and they were later used to evaluate the model&rsquo;s performance on unseen tasks.</p><hr><h2 id=model-evaluation>Model Evaluation <a href=#model-evaluation class=anchor aria-hidden=true>#</a></h2><ul><li>How can you formalize the improvement in performance of your fine-tuned model over the pre-trained model you started with?</li><li>Let&rsquo;s explore several metrics that are used by developers of large language models that you can use to assess the performance of your own models and compare to other models out in the world</li></ul><h3 id=llm-evaluation-challenges>LLM Evaluation Challenges <a href=#llm-evaluation-challenges class=anchor aria-hidden=true>#</a></h3><ul><li>In traditional machine learning, you can assess how well a model is doing by looking at its performance on training and validation data sets where the output is already known.</li><li>You&rsquo;re able to calculate simple metrics such as accuracy, which states the fraction of all predictions that are correct because the models are deterministic</li><li>But with large language models where the output is non-deterministic and language-based evaluation is much more challenging</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2031.png alt=Untitled></p><p><strong>First Pair of Sentences</strong></p><ul><li>Take, for example, the sentence, Mike really loves drinking tea.</li><li>This is quite similar to Mike adores sipping tea.</li><li>But how do you measure the similarity?</li></ul><p><strong>Second Pair of Sentences</strong></p><ul><li>Let&rsquo;s look at these other two sentences.<ul><li>Mike does not drink coffee, and Mike does drink coffee.</li></ul></li><li>There is only one word difference between these two sentences.</li><li>However, the meaning is completely different.</li><li>Now, for humans like us with squishy organic brains, we can see the similarities and differences.</li><li>But when you train a model on millions of sentences, you need an automated, structured way to make measurements</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2032.png alt=Untitled></p><h3 id=llm-evaluation-metrics>LLM Evaluation Metrics <a href=#llm-evaluation-metrics class=anchor aria-hidden=true>#</a></h3><ul><li>ROUGE and BLEU, are two widely used evaluation metrics for different tasks.</li><li>ROUGE (The Recall-Oriented Understudy for Gisting Evaluation) is primarily employed to assess the quality of automatically generated summaries by comparing them to human-generated reference summaries.</li><li>BLEU (Bilingual Evaluation Understudy) is an algorithm designed to evaluate the quality of machine-translated text, again, by comparing it to human-generated translations.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2033.png alt=Untitled></p><h3 id=terminology-review>Terminology Review <a href=#terminology-review class=anchor aria-hidden=true>#</a></h3><ul><li>In the anatomy of language, a unigram is equivalent to a single word.</li><li>A bigram is two words and n-gram is a group of n-words</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2034.png alt=Untitled></p><h3 id=rouge-score>ROUGE Score <a href=#rouge-score class=anchor aria-hidden=true>#</a></h3><h3 id=rouge-1>ROUGE-1 <a href=#rouge-1 class=anchor aria-hidden=true>#</a></h3><ul><li>To do so, let&rsquo;s look at a human-generated reference sentence.<ul><li>Reference (human): It is cold outside</li><li>Generated output: It is very cold outside</li></ul></li><li>You can perform simple metric calculations similar to other machine learning tasks using recall, precision, and F1.<ul><li>The recall metric measures the number of words or unigrams that are matched between the reference and the generated output divided by the number of words or unigrams in the reference.<ul><li>Gets a perfect score of one as all the generated words match words in the reference.</li></ul></li><li>Precision measures the unigram matches divided by the output size.</li><li>The F1 score is the harmonic mean of both of these values.</li></ul></li><li>These are very basic metrics that only focused on individual words, hence the one in the name, and don&rsquo;t consider the ordering of the words.<ul><li>It can be deceptive. It&rsquo;s easily possible to generate sentences that score well but would be subjectively poor</li></ul></li><li>Stop for a moment and imagine that the sentence generated by the model was different by just one word.<ul><li>Generated output: It is <strong>not</strong> cold outside.</li><li>The scores would be the same</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2035.png alt=Untitled></p><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2036.png alt=Untitled></p><h3 id=rouge-2>ROUGE-2 <a href=#rouge-2 class=anchor aria-hidden=true>#</a></h3><ul><li>You can get a slightly better score by taking into account bigrams or collections of two words at a time from the reference and generated sentence.</li><li>By working with pairs of words you&rsquo;re acknowledging in a very simple way, the ordering of the words in the sentence.</li><li>By using bigrams, you&rsquo;re able to calculate a ROUGE-2.</li><li>Now, you can calculate the recall, precision, and F1 score using bigram matches instead of individual words.</li><li>You&rsquo;ll notice that the scores are lower than the ROUGE-1 scores.</li><li>With longer sentences, they&rsquo;re a greater chance that bigrams don&rsquo;t match, and the scores may be even lower</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2037.png alt=Untitled></p><h3 id=rouge-l>ROUGE-L <a href=#rouge-l class=anchor aria-hidden=true>#</a></h3><ul><li>Rather than continue on with ROUGE numbers growing bigger to n-grams of three or fours, let&rsquo;s take a different approach.</li><li>Instead, you&rsquo;ll look for the Longest Common Subsequence (LCS) present in both the generated output and the reference output.</li><li>In this case, the longest matching sub-sequences are, it is and cold outside, each with a length of two.</li><li>You can now use the LCS value to calculate the recall precision and F1 score, where the numerator in both the recall and precision calculations is the length of the longest common subsequence, in this case, two.</li><li>Collectively, these three quantities are known as the <strong>ROUGE-L</strong> score.</li><li>As with all of the ROUGE scores, you need to take the values in context.</li><li>You can only use the scores to compare the capabilities of models if the scores were determined for the same task.</li><li>For example, summarization. ROUGE scores for different tasks are not comparable to one another</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2038.png alt=Untitled></p><h3 id=rouge-hacking>ROUGE Hacking <a href=#rouge-hacking class=anchor aria-hidden=true>#</a></h3><ul><li>As you&rsquo;ve seen, a particular problem with simple ROUGE scores is that it&rsquo;s possible for a bad completion to result in a good score.</li><li>Generated output: cold, cold, cold, cold.</li><li>As this generated output contains one of the words from the reference sentence, it will score quite highly, even though the same word is repeated multiple times.</li><li>The ROUGE-1 precision score will be perfect</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2039.png alt=Untitled></p><h3 id=rouge-clipping>ROUGE Clipping <a href=#rouge-clipping class=anchor aria-hidden=true>#</a></h3><ul><li>One way you can counter this issue is by using a clipping function to limit the number of unigram matches to the maximum count for that unigram within the reference.</li><li>In this case, there is one appearance of cold and the reference and so a modified precision with a clip on the unigram matches results in a dramatically reduced score.</li><li>However, you&rsquo;ll still be challenged if their generated words are all present, but just in a different order.</li><li>For example, with this generated sentence, outside cold it is.</li><li>This sentence was called perfectly even on the modified precision with the clipping function as all of the words and the generated output are present in the reference.</li><li>Whilst using a different ROUGE score can help experimenting with a n-gram size that will calculate the most useful score will be dependent on the sentence, the sentence size, and your use case</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2040.png alt=Untitled></p><p>Note: many language model libraries, for example, Hugging Face, include implementations of ROUGE score that you can use to easily evaluate the output of your model.</p><h3 id=bleu-score>BLEU Score <a href=#bleu-score class=anchor aria-hidden=true>#</a></h3><ul><li>BLEU (bilingual evaluation understudy) score is useful for evaluating the quality of machine-translated text</li><li>The score itself is calculated using the average precision over multiple n-gram sizes, just like the ROUGE-1 score that we looked at before, but calculated for a range of n-gram sizes and then averaged.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2041.png alt=Untitled></p><ul><li>The BLEU score quantifies the quality of a translation by checking how many n-grams in the machine-generated translation match those in the reference translation.</li><li>To calculate the score, you average precision across a range of different n-gram sizes.</li><li>If you were to calculate this by hand, you would carry out multiple calculations and then average all of the results to find the BLEU score.</li><li>For this example, let&rsquo;s take a look at a longer sentence so that you can get a better sense of the scores value.<ul><li>The reference human-provided sentence is, I am very happy to say that I am drinking a warm cup of tea.</li></ul></li><li>Now, as you&rsquo;ve seen these individual calculations in depth when you looked at ROUGE, I will show you the results of BLEU using a standard library.</li><li>Calculating the BLEU score is easy with pre-written libraries from providers like Hugging Face and I&rsquo;ve done just that for each of our candidate sentences.</li><li>The first candidate is, I am very happy that I am drinking a cup of tea.<ul><li>The BLEU score is 0.495.</li><li>As we get closer and closer to the original sentence, we get a score that is closer and closer to one</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2042.png alt=Untitled></p><h3 id=summary-evaluations-metrics>Summary: Evaluations Metrics <a href=#summary-evaluations-metrics class=anchor aria-hidden=true>#</a></h3><ul><li>Both ROUGE and BLEU are quite simple metrics and are relatively low-cost to calculate.</li><li>You can use them for simple reference as you iterate over your models, but you shouldn&rsquo;t use them alone to report the final evaluation of a large language model.</li><li>Use ROUGE for diagnostic evaluation of summarization tasks and BLEU for translation tasks</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2043.png alt=Untitled></p><p>For overall evaluation of your model&rsquo;s performance, however, you will need to look at one of the evaluation benchmarks that have been developed by researchers</p><hr><h2 id=benchmarks>Benchmarks <a href=#benchmarks class=anchor aria-hidden=true>#</a></h2><ul><li>LLMs are complex, and simple evaluation metrics like the ROUGE and BLUE scores, can only tell you so much about the capabilities of your model.</li><li>In order to measure and compare LLMs more holistically, you can make use of pre-existing datasets, and associated benchmarks that have been established by LLM researchers specifically for this purpose.</li><li>Selecting the right evaluation dataset is vital, so that you can accurately assess an LLM&rsquo;s performance, and understand its true capabilities.</li><li>You&rsquo;ll find it useful to select datasets that isolate specific model skills, like reasoning or common sense knowledge, and those that focus on potential risks, such as disinformation or copyright infringement.</li><li>An important issue that you should consider is whether the model has seen your evaluation data during training.</li><li>You&rsquo;ll get a more accurate and useful sense of the model&rsquo;s capabilities by evaluating its performance on data that it hasn&rsquo;t seen before</li><li>Benchmarks, such as GLUE, SuperGLUE, or HELM, cover a wide range of tasks and scenarios<ul><li>They do this by designing or collecting datasets that test specific aspects of an LLM</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2044.png alt=Untitled></p><h3 id=glue>GLUE <a href=#glue class=anchor aria-hidden=true>#</a></h3><ul><li>GLUE, or General Language Understanding Evaluation, was introduced in 2018.</li><li>GLUE is a collection of natural language tasks, such as sentiment analysis and question-answering.</li><li>GLUE was created to encourage the development of models that can generalize across multiple tasks, and you can use the benchmark to measure and compare the model performance</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2045.png alt=Untitled></p><h3 id=superglue>SuperGLUE <a href=#superglue class=anchor aria-hidden=true>#</a></h3><ul><li>As a successor to GLUE, SuperGLUE was introduced in 2019, to address limitations in its predecessor.</li><li>It consists of a series of tasks, some of which are not included in GLUE, and some of which are more challenging versions of the same tasks.</li><li>SuperGLUE includes tasks such as multi-sentence reasoning, and reading comprehension</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2046.png alt=Untitled></p><h3 id=glue-and-superglue-leaderboards>GLUE and SuperGLUE Leaderboards <a href=#glue-and-superglue-leaderboards class=anchor aria-hidden=true>#</a></h3><ul><li>Both the GLUE and SuperGLUE benchmarks have leaderboards that can be used to compare and contrast evaluated models.</li><li>The results page is another great resource for tracking the progress of LLMs</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2047.png alt=Untitled></p><h3 id=benchmarks-for-massive-models>Benchmarks for Massive Models <a href=#benchmarks-for-massive-models class=anchor aria-hidden=true>#</a></h3><ul><li>As models get larger, their performance against benchmarks such as SuperGLUE start to match human ability on specific tasks.</li><li>That&rsquo;s to say that models are able to perform as well as humans on the benchmarks tests, but subjectively we can see that they&rsquo;re not performing at human level at tasks in general.</li><li>There is essentially an arms race between the emergent properties of LLMs, and the benchmarks that aim to measure them</li></ul><p><strong>MMLU and BIG-bench</strong></p><ul><li>Here are a couple of recent benchmarks that are pushing LLMs further.</li><li>Massive Multitask Language Understanding, or MMLU, is designed specifically for modern LLMs.<ul><li>To perform well models must possess extensive world knowledge and problem-solving ability.</li><li>Models are tested on elementary mathematics, US history, computer science, law, and more.</li><li>In other words, tasks that extend way beyond basic language understanding.</li></ul></li><li>BIG-bench currently consists of 204 tasks, ranging through linguistics, childhood development, math, common sense reasoning, biology, physics, social bias, software development and more.<ul><li>BIG-bench comes in three different sizes, and part of the reason for this is to keep costs achievable, as running these large benchmarks can incur large inference costs</li></ul></li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2048.png alt=Untitled></p><h3 id=helm>HELM <a href=#helm class=anchor aria-hidden=true>#</a></h3><ul><li>A final benchmark you should know about is the Holistic Evaluation of Language Models, or HELM.</li><li>The HELM framework aims to improve the transparency of models, and to offer guidance on which models perform well for specific tasks.</li><li>HELM takes a multi-metric approach, measuring seven metrics across 16 core scenarios, ensuring that trade-offs between models and metrics are clearly exposed.</li><li>One important feature of HELM is that it assesses on metrics beyond basic accuracy measures, like precision of the F1 score.</li><li>The benchmark also includes metrics for fairness, bias, and toxicity, which are becoming increasingly important to assess as LLMs become more capable of human-like language generation, and in turn of exhibiting potentially harmful behavior.</li></ul><p><img class="img-fluid lazyload blur-up" src=https://raw.githubusercontent.com/iliyaML/hosting/main/generative-ai-with-llms/week-2-part-1/Untitled%2049.png alt=Untitled></p><ul><li>HELM is a living benchmark that aims to continuously evolve with the addition of new scenarios, metrics, and models</li><li>You can take a look at the results page to browse the LLMs that have been evaluated, and review scores that are pertinent to your project&rsquo;s needs.</li></ul><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"></div><div class="docs-navigation d-flex justify-content-between"><a href=/notes/nlp/generative-ai-with-llms/week-2-research-papers/><div class="card my-1"><div class="card-body py-2">&larr; Week 2 - Research Papers</div></div></a><a class=ms-auto href=/notes/nlp/generative-ai-with-llms/week-2-part-2/><div class="card my-1"><div class="card-body py-2">Week 2 Part 2 - Parameter Efficient Fine-tuning &rarr;</div></div></a></div></main></div></div></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-X0X8EQ5BBE"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-X0X8EQ5BBE")</script><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Â© 2023 <a class=text-muted href=/>iliyaML</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=/js/bootstrap.min.650aeec64c81d69d4c0850fc73c93da3f0330cec0a27772feed7f90f60baa5f47f1c45687d71914bdafd1c4e860d40f6dc08ede27a2f08431ff929c9a2d24621.js integrity="sha512-ZQruxkyB1p1MCFD8c8k9o/AzDOwKJ3cv7tf5D2C6pfR/HEVofXGRS9r9HE6GDUD23Ajt4novCEMf+SnJotJGIQ==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.3f0a703c54cbed82ca277187e23cf2d272da28c15ce7e33cde685d40b53d741893d5b74d35bb2d20a81f56c289084f245bdd0c9145d39d7094d3dfbc62d1326a.js integrity="sha512-PwpwPFTL7YLKJ3GH4jzy0nLaKMFc5+M83mhdQLU9dBiT1bdNNbstIKgfVsKJCE8kW90MkUXTnXCU09+8YtEyag==" crossorigin=anonymous defer></script>
<script src=/main.min.cb2e2ebbf2e4002f3117addc33582923b2b3ae5265c22944cd117ebec7abe61c170417c4506d7a0f8f0fc9053dfdf441421d53601ac467042ff3d06ec0ba07fa.js integrity="sha512-yy4uu/LkAC8xF63cM1gpI7KzrlJlwilEzRF+vser5hwXBBfEUG16D48PyQU9/fRBQh1TYBrEZwQv89BuwLoH+g==" crossorigin=anonymous defer></script>
<script src=/index.min.787928e0a959255366c62acb9b16d4f1f63336c0a411a3ec7059e7a5a0b8b9ab24a81b179d154bba8a6722d844ce2a02f730b853a8498e70bdeb258d1cbc7fde.js integrity="sha512-eHko4KlZJVNmxirLmxbU8fYzNsCkEaPscFnnpaC4uaskqBsXnRVLuopnIthEzioC9zC4U6hJjnC96yWNHLx/3g==" crossorigin=anonymous defer></script></body></html>