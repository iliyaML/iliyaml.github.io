<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on </title>
    <link>/notes/nlp/</link>
    <description>Recent content in NLP on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 06 Oct 2020 08:49:15 +0000</lastBuildDate><atom:link href="/notes/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Papers</title>
      <link>/notes/nlp/papers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/notes/nlp/papers/</guid>
      <description>Influential Papers in the field of Natural Language Processing in the last few years.
2017 June - Transformer: Attention is All You Need [arxiv] Notes - Notebooks 2018 June - GPT: Improving Language Understanding by Generative Pre-Training [link] 2018 Oct - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [arxiv] 2019 Feb - GPT-2: Language Models are Unsupervised Multitask Learners [link] 2020 May - GPT-3: Language Models are Few-Shot Learners - [arxiv] 2022 March - InstructGPT: Training Language Models to Follow Instructions with Human Feedback [arxiv] </description>
    </item>
    
    <item>
      <title>Attention is All You Need (2017)</title>
      <link>/notes/nlp/attention-is-all-you-need/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/notes/nlp/attention-is-all-you-need/</guid>
      <description>Paper: Attention is All You Need (2017)
Objective is to improve language translation tasks Introduce the Transformer model, has no recurrence or convolutions, relies solely on Self-Attention Dimensions: (batch_size, seq_length, d_model) d_model = 512, each token is converted into a vector with this dimension Attention Self-Attention or Scaled Dot Product Attention (Query, Key, Value) W_q, W_k, W_v, learned weights of Query, Key and Value Queries and Keys of dimension d_k, they have to be the same dimension Values of dimension d_v Q, K and V obtained by matrix multiplication of input, x and the Weight matrices attention(Q, K, V) = softmax( Q · K^T / sqrt( d_k ) ) · V Multi-Head Attention Transformer Architecture Consists of Encoder-Decoder Preprocessing (same for Encoder and Decoder) Tokenization - splits the sentences into tokens Input Embedding (learned) - converts each token into a vector of dimension, d Positional Encoding - adds positional encoding to the input embedding vectors, d Use sin() when the position is even, cos() when the position is odd Encoder (N = 6 identical layers), auto-encoding, maps an input sequence to a sequence of continuous representations, has two sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Multi-Head Attention (h = 8 parallel attention heads), stacked Self-Attention Queries, Keys and Values are linearly projected h times Each head, d_model / h Results of each head is concatenated and then projected Add &amp;amp; Norm, Add Residual Connection followed by Layer Normalization: LayerNorm(x + Sublayer(x)) (Sublayer) Position-wise Fully Connected Feed Forward Network Linear() → ReLU() → Linear() FFN(x) = max(0, xW1 + b1)W2 + b2 (simplified) Inner layer has dimensionality, d_ff = 2048 Add &amp;amp; Norm Decoder (N = 6 identical layers), auto-regressive (generates an output sequence one element at a time and at each step, feeds back the output to the decoder), has three sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Masked Multi-Head Attention (h = 8 parallel attention heads, nearly identical to the Encoder but includes masking), prevents position from attending to subsequent positions Add &amp;amp; Norm (Sublayer) Encoder-Decoder Attention (or Cross Attention), performs Multi-Head Attention over the output of the encoder stack Receives K and V from Encoder Add &amp;amp; Norm (Sublayer) Position-wise Fully Connected Feed Forward Network (identical to the Encoder) Add &amp;amp; Norm Linear (learned) - target language vocab_size Softmax - converts the output of the decoder to predicted next-token probabilities Advantages Performs better than previous models such as RNN, LSTM, GRU in language translation tasks Can be trained significantly faster than architectures based on recurrence or convolutions </description>
    </item>
    
    <item>
      <title>State of GPT (2023)</title>
      <link>/notes/nlp/state-of-gpt-2023/</link>
      <pubDate>Tue, 23 May 2023 13:59:39 +0100</pubDate>
      
      <guid>/notes/nlp/state-of-gpt-2023/</guid>
      <description>Tuesday, May 23, 2023
References:
20230523 State of GPT by Andrej Karpathy Notes #GitHub
Training GPT Assistant Training Pipeline - consists of 3 main steps Pre-training - require significant compute (~1000 GPUs), end up with a Base model Data Collection Huge corpus of text data from the Internet (Wikipedia, Books, GitHub, etc.) High quantity/Low quality Tokenization Convert text to a list of integers 2 Example Models GPT-3-175B (2020) vs. LLaMA-65B (2023) comparison Size is not everything, LLaMA is a much better model as it has been trained much longer on much bigger dataset (1T tokens) Context length: 1k-100k tokens (working memory) Pre-training Training Process Inputs to the transformer of the shape (B, T) tensor B is the batch size, T is the maximum context length Training sequences are laid out as rows, delimited by special &amp;lt;|endoftext|&amp;gt; tokens Training Curve - loss decreases over time Base Models Learn Powerful, General Representations (GPT-1) Do pre-training then fine-tune on a particular task like sentiment classification Base Models can be Prompted into Completing Tasks (GPT-2) Started the era of prompting as these models can be tricked to do question-answering tasks using clever prompting Base Models in the Wild GPT-4 (base model not released), GPT-3 (available via API), GPT-2 (weights are released), LLaMA (not commercially licensed) Base Models are NOT ‘Assistants’ They are basically document completers, though they can be tricked into being AI Assistants using some clever prompting Supervised Fine-tuning (SFT) - require less compute than pre-training (1-100 GPUs) SFT Dataset - low quantity compared to Pre-training, but high quality QA prompt responses Reinforcement Learning from Human Feedback (RLHF) - still research/experimental territory Reward Modeling (RM) - rank outputs of a model RM Dataset RM Training Reinforcement Learning (RL) RL Training Why RLHF?</description>
    </item>
    
  </channel>
</rss>
