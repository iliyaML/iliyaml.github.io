var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/notes/machine-learning/",title:"Machine Learning",description:"Machine Learning",content:""}),e.add({id:1,href:"/notes/machine-learning/k-nearest-neighbors/",title:"K-Nearest Neighbors (KNN)",description:" A non-parametric supervised learning algorithm used for both classification and regression Steps Choose K value - typically an odd number Initialization - given point Calculate distance (Euclidean) between given point and all data points in the training set Sort the results in increasing order Type Classification - Majority Vote Regression - Average Best K value Use Cross Validation and Learning Curve K-Value overfitting/underfitting Small K - Low Bias, High Variance (Overfitting) High K - High Bias, Low Variance (Underfitting) Pros Cons Easy to understand and simple to implement Suffers from Curse of Dimensionality Suitable for small datasets Requires high memory storage ",content:" A non-parametric supervised learning algorithm used for both classification and regression Steps Choose K value - typically an odd number Initialization - given point Calculate distance (Euclidean) between given point and all data points in the training set Sort the results in increasing order Type Classification - Majority Vote Regression - Average Best K value Use Cross Validation and Learning Curve K-Value overfitting/underfitting Small K - Low Bias, High Variance (Overfitting) High K - High Bias, Low Variance (Underfitting) Pros Cons Easy to understand and simple to implement Suffers from Curse of Dimensionality Suitable for small datasets Requires high memory storage "}),e.add({id:2,href:"/notes/machine-learning/linear-regression/",title:"Linear Regression",description:"A supervised learning algorithm used for regression problems Model an output variable as a linear combination of input features, finds a line (or surface) that best fits the data Formula: y_hat = W^T·X y_hat, dependent/response variable, target W^T, weights or coefficients X, independent/predictor variable(s), features Polynomial Regression: add polynomial features Assumptions Linear Relationship - a linear relationship between each predictor variable and the response variable No Multicollinearity - none of the predictor variables are highly correlated with each other Independence - each observation in the dataset is independent Homoscedasticity - residuals have constant variance at every point in the linear model Multivariate Normality - residuals of the model are normally distributed Pros Cons Highly interpretable Sensitive to outliers Fast to train Can underfit with small, high-dimensional data Approaches",content:` A supervised learning algorithm used for regression problems Model an output variable as a linear combination of input features, finds a line (or surface) that best fits the data Formula: y_hat = W^T·X y_hat, dependent/response variable, target W^T, weights or coefficients X, independent/predictor variable(s), features Polynomial Regression: add polynomial features Assumptions Linear Relationship - a linear relationship between each predictor variable and the response variable No Multicollinearity - none of the predictor variables are highly correlated with each other Independence - each observation in the dataset is independent Homoscedasticity - residuals have constant variance at every point in the linear model Multivariate Normality - residuals of the model are normally distributed Pros Cons Highly interpretable Sensitive to outliers Fast to train Can underfit with small, high-dimensional data Approaches
Ordinary Least Squares, Normal Equation (instant approach)
Gradient Descent (iterative approach)
Feature Scaling Squared Error Cost Function Model Performance Evaluation
R^2 MAE, RMSE, MAPE `}),e.add({id:3,href:"/notes/machine-learning/logistic-regression/",title:"Logistic Regression",description:" A supervised learning algorithm used for classification problems Sigmoid Function: values range from 0 - 1 Assumptions Response Variable is Binary Independence - observations in the dataset are independent of each other No Multicollinearity - none of the predictor variables are highly correlated with each other No Extreme Outliers Linear Relationship between Explanatory Variables and Logit of Response Variable Sample Size is Sufficiently Large Pros Cons Highly interpretable Can overfit with small, high-dimensional data Applicable for multi-class predictions Multiclass Classification: One-vs-Rest (OvR) Approaches Gradient Descent Binary Cross Entropy / Log Loss Cost(hθ(x),y) = −y log(hθ(x))−(1−y) log(1−hθ(x)) Model Performance Evaluation Confusion Matrix, Accuracy, Precision, Recall, F1-Score, ROC Curve ",content:" A supervised learning algorithm used for classification problems Sigmoid Function: values range from 0 - 1 Assumptions Response Variable is Binary Independence - observations in the dataset are independent of each other No Multicollinearity - none of the predictor variables are highly correlated with each other No Extreme Outliers Linear Relationship between Explanatory Variables and Logit of Response Variable Sample Size is Sufficiently Large Pros Cons Highly interpretable Can overfit with small, high-dimensional data Applicable for multi-class predictions Multiclass Classification: One-vs-Rest (OvR) Approaches Gradient Descent Binary Cross Entropy / Log Loss Cost(hθ(x),y) = −y log(hθ(x))−(1−y) log(1−hθ(x)) Model Performance Evaluation Confusion Matrix, Accuracy, Precision, Recall, F1-Score, ROC Curve "}),e.add({id:4,href:"/notes/deep-learning/",title:"Deep Learning",description:"Deep Learning",content:""}),e.add({id:5,href:"/notes/deep-learning/neural-networks/",title:"Neural Networks",description:" A supervised learning algorithm that can be used for regression and classification problems Non-linear model Components Neurons Input Layer Hidden Layer(s) Output Layer Optimizer Function Adam (Adaptive Moment Estimation) Non-linear Activation Functions ReLU, Sigmoid, TanH Softmax - often used in the output layer for multiclass classification Regularization: dropout Loss Function MSE Binary Cross Entropy (Log Loss) Categorical Cross Entropy Forward Propagation: making inference Backward Propagation (Chain Rule): computes derivatives of your cost function with respect to the parameters Pros Cons Can be used for both regression and classification problems Black box Able to solve linearly inseparable problem Require significant amount of data Computationally expensive to train Approaches Gradient Descent Batch Gradient Descent Stochastic Gradient Descent Mini-Batch Gradient Descent ",content:" A supervised learning algorithm that can be used for regression and classification problems Non-linear model Components Neurons Input Layer Hidden Layer(s) Output Layer Optimizer Function Adam (Adaptive Moment Estimation) Non-linear Activation Functions ReLU, Sigmoid, TanH Softmax - often used in the output layer for multiclass classification Regularization: dropout Loss Function MSE Binary Cross Entropy (Log Loss) Categorical Cross Entropy Forward Propagation: making inference Backward Propagation (Chain Rule): computes derivatives of your cost function with respect to the parameters Pros Cons Can be used for both regression and classification problems Black box Able to solve linearly inseparable problem Require significant amount of data Computationally expensive to train Approaches Gradient Descent Batch Gradient Descent Stochastic Gradient Descent Mini-Batch Gradient Descent "}),e.add({id:6,href:"/notes/nlp/",title:"NLP",description:"Natural Language Processing (NLP)",content:""}),e.add({id:7,href:"/notes/nlp/papers/",title:"Papers",description:`Influential Papers in the field of Natural Language Processing in the last few years.
2017 June - Transformer: Attention is All You Need [arxiv] Notes - Notebooks 2018 June - GPT: Improving Language Understanding by Generative Pre-Training [link] 2018 Oct - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [arxiv] 2019 Feb - GPT-2: Language Models are Unsupervised Multitask Learners [link] 2020 May - GPT-3: Language Models are Few-Shot Learners - [arxiv] 2022 March - InstructGPT: Training Language Models to Follow Instructions with Human Feedback [arxiv] `,content:`Influential Papers in the field of Natural Language Processing in the last few years.
2017 June - Transformer: Attention is All You Need [arxiv] Notes - Notebooks 2018 June - GPT: Improving Language Understanding by Generative Pre-Training [link] 2018 Oct - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [arxiv] 2019 Feb - GPT-2: Language Models are Unsupervised Multitask Learners [link] 2020 May - GPT-3: Language Models are Few-Shot Learners - [arxiv] 2022 March - InstructGPT: Training Language Models to Follow Instructions with Human Feedback [arxiv] `}),e.add({id:8,href:"/notes/nlp/attention-is-all-you-need/",title:"Attention is All You Need (2017)",description:`Paper: Attention is All You Need (2017)
Objective is to improve language translation tasks Introduce the Transformer model, has no recurrence or convolutions, relies solely on Self-Attention Dimensions: (batch_size, seq_length, d_model) d_model = 512, each token is converted into a vector with this dimension Attention Self-Attention or Scaled Dot Product Attention (Query, Key, Value) W_q, W_k, W_v, learned weights of Query, Key and Value Queries and Keys of dimension d_k, they have to be the same dimension Values of dimension d_v Q, K and V obtained by matrix multiplication of input, x and the Weight matrices attention(Q, K, V) = softmax( Q · K^T / sqrt( d_k ) ) · V Multi-Head Attention Transformer Architecture Consists of Encoder-Decoder Preprocessing (same for Encoder and Decoder) Tokenization - splits the sentences into tokens Input Embedding (learned) - converts each token into a vector of dimension, d Positional Encoding - adds positional encoding to the input embedding vectors, d Use sin() when the position is even, cos() when the position is odd Encoder (N = 6 identical layers), auto-encoding, maps an input sequence to a sequence of continuous representations, has two sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Multi-Head Attention (h = 8 parallel attention heads), stacked Self-Attention Queries, Keys and Values are linearly projected h times Each head, d_model / h Results of each head is concatenated and then projected Add \u0026amp; Norm, Add Residual Connection followed by Layer Normalization: LayerNorm(x + Sublayer(x)) (Sublayer) Position-wise Fully Connected Feed Forward Network Linear() → ReLU() → Linear() FFN(x) = max(0, xW1 + b1)W2 + b2 (simplified) Inner layer has dimensionality, d_ff = 2048 Add \u0026amp; Norm Decoder (N = 6 identical layers), auto-regressive (generates an output sequence one element at a time and at each step, feeds back the output to the decoder), has three sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Masked Multi-Head Attention (h = 8 parallel attention heads, nearly identical to the Encoder but includes masking), prevents position from attending to subsequent positions Add \u0026amp; Norm (Sublayer) Encoder-Decoder Attention (or Cross Attention), performs Multi-Head Attention over the output of the encoder stack Receives K and V from Encoder Add \u0026amp; Norm (Sublayer) Position-wise Fully Connected Feed Forward Network (identical to the Encoder) Add \u0026amp; Norm Linear (learned) - target language vocab_size Softmax - converts the output of the decoder to predicted next-token probabilities Advantages Performs better than previous models such as RNN, LSTM, GRU in language translation tasks Can be trained significantly faster than architectures based on recurrence or convolutions `,content:`Paper: Attention is All You Need (2017)
Objective is to improve language translation tasks Introduce the Transformer model, has no recurrence or convolutions, relies solely on Self-Attention Dimensions: (batch_size, seq_length, d_model) d_model = 512, each token is converted into a vector with this dimension Attention Self-Attention or Scaled Dot Product Attention (Query, Key, Value) W_q, W_k, W_v, learned weights of Query, Key and Value Queries and Keys of dimension d_k, they have to be the same dimension Values of dimension d_v Q, K and V obtained by matrix multiplication of input, x and the Weight matrices attention(Q, K, V) = softmax( Q · K^T / sqrt( d_k ) ) · V Multi-Head Attention Transformer Architecture Consists of Encoder-Decoder Preprocessing (same for Encoder and Decoder) Tokenization - splits the sentences into tokens Input Embedding (learned) - converts each token into a vector of dimension, d Positional Encoding - adds positional encoding to the input embedding vectors, d Use sin() when the position is even, cos() when the position is odd Encoder (N = 6 identical layers), auto-encoding, maps an input sequence to a sequence of continuous representations, has two sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Multi-Head Attention (h = 8 parallel attention heads), stacked Self-Attention Queries, Keys and Values are linearly projected h times Each head, d_model / h Results of each head is concatenated and then projected Add \u0026amp; Norm, Add Residual Connection followed by Layer Normalization: LayerNorm(x + Sublayer(x)) (Sublayer) Position-wise Fully Connected Feed Forward Network Linear() → ReLU() → Linear() FFN(x) = max(0, xW1 + b1)W2 + b2 (simplified) Inner layer has dimensionality, d_ff = 2048 Add \u0026amp; Norm Decoder (N = 6 identical layers), auto-regressive (generates an output sequence one element at a time and at each step, feeds back the output to the decoder), has three sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Masked Multi-Head Attention (h = 8 parallel attention heads, nearly identical to the Encoder but includes masking), prevents position from attending to subsequent positions Add \u0026amp; Norm (Sublayer) Encoder-Decoder Attention (or Cross Attention), performs Multi-Head Attention over the output of the encoder stack Receives K and V from Encoder Add \u0026amp; Norm (Sublayer) Position-wise Fully Connected Feed Forward Network (identical to the Encoder) Add \u0026amp; Norm Linear (learned) - target language vocab_size Softmax - converts the output of the decoder to predicted next-token probabilities Advantages Performs better than previous models such as RNN, LSTM, GRU in language translation tasks Can be trained significantly faster than architectures based on recurrence or convolutions `}),e.add({id:9,href:"/notes/nlp/state-of-gpt-2023/",title:"State of GPT (2023)",description:"State of GPT by Andrej Karpathy at Microsoft Build 2023",content:`Tuesday, May 23, 2023
References:
20230523 State of GPT by Andrej Karpathy Notes #GitHub
Training GPT Assistant Training Pipeline - consists of 3 main steps Pre-training - require significant compute (~1000 GPUs), end up with a Base model Data Collection Huge corpus of text data from the Internet (Wikipedia, Books, GitHub, etc.) High quantity/Low quality Tokenization Convert text to a list of integers 2 Example Models GPT-3-175B (2020) vs. LLaMA-65B (2023) comparison Size is not everything, LLaMA is a much better model as it has been trained much longer on much bigger dataset (1T tokens) Context length: 1k-100k tokens (working memory) Pre-training Training Process Inputs to the transformer of the shape (B, T) tensor B is the batch size, T is the maximum context length Training sequences are laid out as rows, delimited by special \u0026lt;|endoftext|\u0026gt; tokens Training Curve - loss decreases over time Base Models Learn Powerful, General Representations (GPT-1) Do pre-training then fine-tune on a particular task like sentiment classification Base Models can be Prompted into Completing Tasks (GPT-2) Started the era of prompting as these models can be tricked to do question-answering tasks using clever prompting Base Models in the Wild GPT-4 (base model not released), GPT-3 (available via API), GPT-2 (weights are released), LLaMA (not commercially licensed) Base Models are NOT ‘Assistants’ They are basically document completers, though they can be tricked into being AI Assistants using some clever prompting Supervised Fine-tuning (SFT) - require less compute than pre-training (1-100 GPUs) SFT Dataset - low quantity compared to Pre-training, but high quality QA prompt responses Reinforcement Learning from Human Feedback (RLHF) - still research/experimental territory Reward Modeling (RM) - rank outputs of a model RM Dataset RM Training Reinforcement Learning (RL) RL Training Why RLHF? - it just works better Mode Collapse Not strictly an improvement on the base models, they lose some entropy (outputs of the model lose some diversity) Assistant Models in the Wild Best Models: GPT-4, Claude 1, … Applications Human Text Generation vs. LLM Text Generation For GPT’s, it’s just a sequence of tokens, each chunk is roughly the same amount of computation work for each token Transformers are just like token simulators They do have a lot of storage (10B parameters), and a relatively large working memory (context length or window) Anything that fits into the context window is immediately available (direct access) to the Transformer through the self-attention mechanism Chain of Thought “Transformers need tokens to think” Few shot prompting - give examples that shows the Transformer that it should show its work Can elicit this behavior by adding “Let’s think step by step” in the prompt Conditions the Transformer to show its work Spread out the reasoning over many tokens Ensemble Multiple Attempts Self-Consistency - sample multiple answers as sometimes it can get unlucky with its generation of output Ask for Reflection Ask the model if it achieved the target of your prompt (works for GPT-4) Recreate Our ‘System 2’ - slower, deliberate planning part of our brain Tree of Thoughts paper - maintaining multiple completions for any given prompt, scoring them along the way and keeping the ones that are going well Chains / Agents ReAct paper AutoGPT - allow an LLM to sort of keep a task list and continue to recursively break down tasks Condition on Good Performance “LLMs don’t want to succeed. They want to imitate. You want to succeed, and you should ask for it.” Ask the model to pretend that its a leading expert in something “Let’s work this out in a step by step way to be sure we have the right answer” Conditions it on getting a right answer Tool Use / Plugins - calculator Retrieval-Augmented LLMs Load related information into the model’s working memory (context window) Recipe Take relevant documents, split them up into chunks, embed all of them, and basically get embedding vectors that represent that data Store the embeddings and chunks of text in the vector store At test time, query your vector store, fetch chunks that might be relevant to your task, and stuff them into the prompt, and the model generates the response Constrained Prompting Techniques for enforcing a certain template in the outputs of LLMs Guidance by Microsoft Fine-tuning Parameter Efficient Finetuning Techniques (PEFT) like LORA Training only small, sparse pieces of your model while most of the base model is kept clamped Works pretty well, empirically, and is cheaper RLHF still experimental Default Recommendations Achieve your top possible performance Optimize costs Use Cases - GPTs/LLMs as Copilots Use in low-stakes applications, combine with human oversight Source of inspiration, suggestions Copilots over autonomous agents GPT-4 \u0026amp; Looking Forward OpenAI API State of GPT (2023) #ANDREJ KARPATHY:
Hi, everyone. I’m happy to be here to tell you about the state of GPT. And more generally, about the rapidly growing ecosystem of large language models. So I would like to partition the talk into two parts.
In the first part, I would like to tell you about how we train GPT assistants. And then in the second part, we are going to take a look at how we can use these assistants effectively for your applications.
GPT Assistant Training Pipeline #So first, let’s take a look at the emerging recipe for how to train these assistants. And keep in mind that this is all very new and so rapidly evolving. But so far, the recipe looks something like this.
Now, this is kind of a complicated slide, so I’m going to go through it piece by piece. But roughly speaking, we have four major stages: pre-training, supervised fine tuning, reward modeling, reinforcement learning. They follow each other serially.
Now, in each stage we have a dataset that powers that stage. We have an algorithm that for our purposes will be an objective for training a neural network. And then we have a resulting model. And then there’s some notes on the bottom.
Pre-training #So the first stage we’re going to start with is the pre-training stage. Now, this stage is kind of special in this diagram, and this diagram is not to scale because this stage is where all of the computational work basically happens. This is 99% of the training compute time and also flops.
And so this is where we are dealing with internet-scale datasets with thousands of GPUs in the supercomputer and also months of training, potentially. The other three stages are fine tuning stages that are much more along the lines of some few number of GPUs and hours or days.
So let’s take a look at the pre-training stage to achieve a base model.
Data Collection #First, we’re going to gather a large amount of data. Here’s an example of what we call a data mixture that comes from this paper that was released by Meta, where they released this LLaMA-based model.
Now, you can see roughly the kinds of datasets that enter into these collections. So we have Common Crawl, which is just a web scrape, C4, which is also a Common Crawl, and then some high-quality datasets as well.
So for example, GitHub, Wikipedia, Books, Archive, Stock Exchange and so on. These are all mixed up together and then they are sampled according to some given proportions, and that forms the training sets for the neural net for the GPT.
Tokenization #Now, before we can actually train on this data, we need to go through one more pre-processing step, and that is tokenization. And this is basically a translation of the raw text that we scraped from the internet into sequences of integers, because that’s the native representation over which GPT functions.
Now, this is a lossless kind of translation between pieces of text and tokens and integers, and there are a number of algorithms for the stage. Typically, for example, you could use something like byte pair encoding which iteratively merges little text chunks and groups them into tokens.
And so here I’m showing some example chunks of these tokens, and then this is the raw integer sequence that will actually feed into a transformer.
2 Example Models #Now, here I’m showing two sort of like examples for hyperparameters that govern the stage. GPT-4, we did not release too much information about how it was trained and so on. So I’m using GPT-3’s numbers, but GPT-3 is of course a little bit old by now, about three years ago. But LLaMA is a fairly recent model from Meta.
So these are roughly the orders of magnitude that we’re dealing with when we’re doing pre-training. The vocabulary size is usually 10,000 tokens. The context length is usually something like 2,000, 4,000, or nowadays, even 100,000. And this governs the maximum number of integers that the GPT will look at when it’s trying to predict the next integer in a sequence.
You can see that roughly the number of parameters is, say, 65 billion for LLaMA. Now, even though LLaMA has only 65 parameters compared to GPT-3’s 175 billion parameters, LLaMA is a significantly more powerful model and intuitively that’s because the model is trained for significantly longer, in this case, 1.4 trillion tokens instead of just 300 billion tokens. You shouldn’t judge the power of a model just by the number of parameters that it contains.
Below, I’m showing some tables of a number of hyperparameters that typically go into specifying the transformer neural network. So the number of heads, the dimension size, number of layers and so on.
And on the bottom, I’m showing some training hyperparameters. For example, to train the 65B model, Meta used 2,000 GPUs, roughly 21 days of training, and roughly several million dollars. And so that’s the rough orders of magnitude that you should have in mind for the pre-training stage.
Pre-training #Now, when we’re actually pre-training, what happens? Roughly speaking, we are going to take our tokens and we’re going to lay them out into data batches. We have these arrays that will feed into the transformer, and these arrays are B, the batch size, and these are all independent examples stacked up in rows, and B x T, T being the maximum context length.
So in my picture I only have 10, but this is the context length, and so this could be 2,000, 4,000, et cetera. These are extremely long rows, and what we do is we take these documents, and we pack them into rows, and we delimit them with these special end-of-text tokens, basically telling the transformer where a new document begins. And so here I have a few examples of documents and then I stretched them out into into this input.
Now, we’re going to feed all of these numbers into transformer. And let me let me just focus on a single particular cell, but the same thing will happen at every every cell in this diagram.
So let’s look at the green cell. The green cell is going to take a look at all of the tokens before it, so all of the tokens in yellow. And we’re going to feed that entire context into the transformer neural network. And the transformer is going to try to predict the next token in the sequence, in this case in red.
Now, the transformer, I don’t have too much time to unfortunately, go into the full details of this neural network architecture, but it is just a large blob of neural net stuff for our purposes, and it’s got several – 10 billion parameters typically, or something like that.
And of course, as they tune these parameters, you’re getting slightly different predicted distributions for every single one of these cells. And so, for example, if our vocabulary size is 50,257 tokens, then we’re going to have that many numbers because we need to specify a probability distribution for what comes next, so that we basically have a probability for whatever may follow.
Now, in this specific example for the specific cell, 513 will come next. And so we can use this as a source of supervision to update our transformer weights. And so we’re applying this basically on every single cell in the parallel. And we keep swapping batches and we’re trying to get the transformer to make the correct predictions over what token comes next in a sequence.
Training Process #So let me show you more concretely what this looks like when you train one of these models. This is actually coming from the New York Times, and they trained a small GPT on Shakespeare. And so here’s a small snippet of Shakespeare, and they trained their GPT on it.
Now, in the beginning at initialization, the GPT starts with completely random weights, so you’re just getting completely random outputs as well. But over time, as you train the GPT longer and longer, you are getting more and more coherent and consistent sorts of samples from the model.
And the way you sample from it, of course, is you predict what comes next. You sample from that distribution and you keep feeding that back into the process and you can basically sample large sequences.
And so by the end, you see that the transformer has learned about words and where to put spaces and where to put commas and so on. And so we’re making more and more consistent predictions over time.
Training Curve #These are the kinds of plots that you’re looking at when you’re doing model pre-training. Effectively, we’re looking at the loss function over time as you train. And low loss means that our transformer is predicting the correct – is giving a higher probability to the correct next integer in a sequence.
Base Models Learn Powerful, General Representations #Now, what are we going to do with this model once we’ve trained it after a month? Well, the first thing that we noticed, we in the field, is that these models are basically in the process of language modeling, learn very powerful general representations, and it’s possible to very efficiently fine tune them for any arbitrary downstream task you might be interested in.
So as an example, if you’re interested in sentiment classification, the approach used to be that you collect a bunch of positives and negatives and then you train some kind of an NLP model for for that. But the new approach is to ignore sentiment, classification, go off and do large language model pre-training, train the large transformer and then you can only – you may only have a few examples and you can very efficiently fine tune your model for that task.
And so this works very well in practice. And the reason for this is that basically the transformer is forced to multitask a huge amount of tasks in the language modeling task, because just just in terms of predicting the next token, it’s forced to understand a lot about the structure of the text and all the different concepts therein.
So that was GPT-1.
Base Models can be Prompted into Completing Tasks #Now, around the time of GPT-2, people noticed that actually even better than fine tuning, you can actually prompt these models very effectively. So these are language models and they want to complete documents, and so you can actually trick them into performing tasks just by arranging these fake documents.
So in this example, for example, we have some passage and then we sort of like do, \u0026ldquo;QA, QA, QA,\u0026rdquo; and this is called few-shot prompt, and then we do Q, and then as the transformer is trying complete the document, it’s actually answering our question. And so this is an example of prompt engineering a base model, making it belief that it’s sort of imitating a document and it’s getting it to perform a task.
And so this kicked off, I think, the era of, I would say, prompting over fine tuning and seeing that this actually can work extremely well on a lot of problems, even without training any neural networks, fine tuning or so on.
Base Models in the Wild #Now, since then, we’ve seen an entire evolutionary tree of base models that everyone has trained. Not all of these models are available. For example, the GPT-4 base model was never released. The GPT-4 model that you might be interacting with over API is not a base model. It’s an assistant model and we’re going to cover how to get those in a bit.
The GPT-3 base model is available via the API under the named DaVinci and the GPT-2 base model is available even as weights on our GitHub repo. But currently the best available base model probably is the LLaMA series from Meta, although it is not commercially licensed.
Base Models are NOT ‘Assistants’ #Now, one thing to point out is base models are not assistants. They don’t want to make answers to your questions. They just want to complete documents. So if you tell them, \u0026ldquo;Write a poem about the bread and cheese,\u0026rdquo; it will just – you know, it will answer questions with more questions. It’s just completing what it thinks is a document.
However, you can prompt them in a specific way for base models that is more likely to work. So as an example, here’s a poem about bread and cheese, and in that case it will autocomplete correctly.
You can even trick base models into being assistants. And the way you would do this is you would create like a specific few-shot prompt that makes it look like there’s some kind of a document between a human and assistant, and they’re exchanging sort of information.
And then at the bottom you sort of put your query at the end, and the base model will sort of like condition itself into being like a helpful assistant and kind of answer. But this is not very reliable and doesn’t work super well in practice, although it can be done.
Supervised Fine-tuning (SFT) #So instead we have a different path to make actual GPT assistants, not just base model document completers. And so that takes us into supervised fine tuning. So in the supervised fine-tuning stage, we are going to collect small, but high-quality datasets. And in this case, we’re going to ask human contractors to gather data of the form prompt and ideal response. And we’re going to collect lots of these, typically tens of thousands or something like that.
And then we’re going to still do language modeling on this data. So nothing changed algorithmically. We’re just swapping out a training set. So it used to be internet documents, which is a high-quantity/low-quality, for basically QA prompt response kinds of data, and that is low-quantity/high-quality.
So we would still do language modeling. And then after training we get an SFT model, and you can actually deploy these models, and they are actual assistants, and they work to some extent.
SFT Dataset #Let me show you what an example demonstration might look like. So here’s something that a human contractor might come up with. Here’s some random prompt, \u0026ldquo;Can you write a short introduction about the relevance of the term monopsony,\u0026rdquo; or something like that? And then the contractor also writes out an ideal response.
And when they write out these responses, they are following extensive labeling documentations and they’re being asked to be helpful, truthful and harmless. These are the labeling instructions here. You probably can’t read it, and neither can I, but they’re long and this is just people following instructions and trying to complete these prompts.
So that’s what the dataset looks like. And you can train these models and this works to some extent.
Reinforcement Learning from Human Feedback (RLHF) #Reward Modeling (RM) #Now, you can actually continue the pipeline from here on and go into, RLHF, reinforcement learning from human feedback, which consists of both reward modeling and reinforcement learning.
So let me cover that and then I’ll come back to why you may want to go through the extra steps and how that compares to just SFT models.
RM Dataset #So in the reward modeling step, what we’re going to do is we’re now going to shift our data collection to be of the form of comparisons. So here’s an example of what our dataset will look like. I have the same prompt, identical prompt on the top, which is asking the assistant to write a program or a function that checks if a given string is a palindrome.
And then what we do is we take the SFT model, which we’ve already trained, and we create multiple completions. So in this case we have three completions that the model has created. And then we ask people to rank these completions.
So if you stare at this for a while, and by the way, these are very difficult things to do to compare some of these predictions, and this can take people even hours for single prompt completion pairs. But let’s say we decided that one of these is much better than the others and so on, and so we rank them. We can then follow that with something that looks very much kind of like a binary classification on all the possible pairs between these completions.
RM Training #So what we do now is we lay out our prompt in rows and the prompts is identical across all three rows here. So it’s all the same prompt, but the completion is varied, and so the yellow tokens are coming from the SFT model.
Then what we do is we append another special reward readout token at the end, and we basically only supervise the transformer at this single green token, and the transformer will predict some reward for how good that completion is for that prompt.
And so it basically makes a guess about the quality of each completion. And then once it makes a guess for every one of them, we also have the ground truth, which is telling us the ranking of them. And so we can actually enforce that some of these numbers should be much higher than others and so on. We formulate this into a loss function, and we train our model to make reward predictions that are consistent with the ground truth coming from the comparisons from all these contractors.
So this is how we train our reward model, and that allows us to score how good a completion is for a prompt.
Reinforcement Learning (RL) #Once we have a reward model, we can’t deploy this because this is not very useful as an assistant by itself, but it’s very useful for the reinforcement learning stage that follows now. Because we have a reward model, we can score the quality of any arbitrary completion for any given prompt.
So what we do during reinforcement learning is we basically get, again, a large collection of prompts and now we do reinforcement learning with respect to the reward model.
RL Training #Here’s what that looks like. We take a single prompt, we lay it out in rows, and now we use the SFT model. We use basically the model we’d like to train, which is initialized as SFT model, to create some completions in yellow. And then we append the reward token again, and we read off the reward according to the reward model, which is now kept fixed. It doesn’t change anymore.
And now, the reward model tells us the quality of every single completion for these prompts. And so, what we can do is we can now just basically apply the same language modeling loss function, but we’re currently training on the yellow tokens. And we are weighing the language modeling objective by the rewards indicated by the reward model.
As an example, in the first row, the reward model said that this is a fairly high scoring completion. And so, all the tokens that we happen to sample on the first row are going to get reinforced, and they’re going to get higher probabilities for the future. Conversely, on the second row, the reward model really did not like this completion, -1.2. And so, therefore, every single token that we sampled in that second row is going to get a slightly higher probability for the future. And we do this over and over on many prompts, on many batches. And basically, we get a policy which creates yellow tokens here, and it’s basically all of them, all of the completions here will score high according to the reward model that we trained in the previous stage.
That’s how we train. That’s what the RLHF pipeline is. And then at the end, you get a model that you could deploy. And so, as an example, ChatGPT is an RLHF model, but some other models that you might come across, for example, of the (inaudible) and so on, these are SFT models. We have base models, SFT models, and RLHF models, and that’s kind of like the state of things there.
Why RLHF? #Now why would you want to do RLHF? One answer that is kind of not that exciting is that it just works better. This comes from the instruct GPT paper. According to these experiments a while ago now, these PPO models are RLHF. And we see that they are basically just preferred in a lot of comparisons, when we give them to humans. Humans just prefer basically tokens that come from RLHF models, compared to SFT models, compared to base model that is prompted to be an assistant. And so, it just works better.
But you might ask why? Why does it work better? And I don’t think that there’s a single amazing answer that the community has really agreed on, but I will just offer one reason, potentially, and it has to do with the asymmetry between how easy computationally it is to compare versus generate.
Let’s take an example of generating a haiku. Suppose I ask a model to write a haiku about paperclips. If you’re a contractor trying to give training data, then imagine being a contractor collecting basically data for the SFT. How are you supposed to create a nice haiku for a paperclip? You might just not be very good at that, but if I give you a few examples of haikus, you might be able to appreciate some of these haikus a lot more than others. And so, judging which one of these is good is much easier task. And so, basically this asymmetry makes it so that comparisons are a better way to potentially leverage yourself, as a human and your judgment to create a slightly better model.
Mode Collapse #Now, RLHF models are not strictly an improvement on the base models, in some cases. In particular, we’ve noticed, for example, that they lose some entropy. That means that they give more (PT?) results. They can output lower variations. They can output samples with lower variation than the base model. Base model has lots of entropy and will give lots of diverse outputs.
For example, one kind of place where I still prefer to use a base model is in the setup where you basically have n things and you want to generate more things like it. And so, here is an example that I just cooked up. I want to generate cool Pokémon names. I gave it seven Pokémon names, and I asked the base model to complete the document. And it gave me a lot more Pokémon names. These are fictitious. I tried to look them up. I don’t believe there are actual Pokémons. And this is the kind of task that I think base model would be good at, because it still has lots of entropy and will give you lots of diverse, cool kind of more things that look like whatever you give it before.
Assistant Models in the Wild #Having said all that, these are kind of like the assistant models that are probably available to you at this point. There’s a team at Berkeley that ranked a lot of the available assistant models and gave them basically ELO ratings. Currently, some of the best models, of course, are GPT-4, by far, I would say, followed by Claude GPT 3.5 and then a number of models. Some of these might be available as weights, like the Kuna, Koala, etcetera. And the first three rows here, they are all RLHF models, and all of the other models, to my knowledge, are SFT models, I believe.
Applications #Okay, so that’s how we train these models on the high level. Now, I’m going to switch gears, and let’s look at how we can best apply the GPT assistant model to your problems.
Human Text Generation vs. LLM Text Generation #Now, I would like to work in something of a concrete example. Let’s work with the concrete example here. Let’s say that you are working on an article or a blog post, and you’re going to write this sentence at the end. “California’s population is 53 times that of Alaska.” For some reason, you want to compare the populations of these two states.
Think about the rich internal monologue and tool use, and how much work actually goes computationally in your brain to generate this one final sentence. Here’s maybe what that could look like in your brain.
Okay, for this next step, let me blog. Let me compare these two populations. Okay, first, obviously, I need to get both of these populations. Now, I know that I probably don’t know these populations off the top of my head, so I’m kind of like aware of what I know or don’t know of my self-knowledge, right? I do some tool use, and I go to Wikipedia and I look up California’s population and Alaska’s population.
Now I know that I should divide the two, but again, I know that dividing 39.2 by 0.74 is very unlikely to succeed. That’s not the kind of thing that I can do in my head. And so, therefore I’m going to rely on the calculator. I’m going to use a calculator, punch it in and see that the output is roughly 53. And then maybe I do some reflection and sanity checks in my brain, so that 53 makes sense. Well, that’s quite a large fraction, but then California has the most populous state, so maybe that looks okay.
Then I have all the information I might need, and now I get to the sort of creative portion of writing. I might start to write something like, “California has 53x times greater.” And then I think to myself, that’s actually really awkward phrasing. Let me actually delete that and let me try again. And so, as I’m writing, I have this separate process, almost inspecting what I’m writing and judging whether it looks good or not. And then maybe I delete and maybe I reframe it, and then maybe I’m happy with what comes out.
Basically, long story short, a ton happens under the hood in terms of your internal monologue when you create sentences like this. But what does a sentence like this look like when we are training a GPT on it?
From GPT’s perspective, this is just a sequence of tokens. GPT, when it’s reading or generating these tokens, it just goes chunk, chunk, chunk, chunk, and each chunk is roughly the same amount of computational work for each token. And these transformers are not very shallow networks. They have about 80 layers of reasoning, but 80 is still not too much. And so, this transformer is going to do its best to imitate, but of course, the process here looks very, very different from the process that you took.
In particular, in our final artifacts, in the dataset that we create and then eventually feed to LLMs, all of that internal dialog is completely stripped. And unlike you, the GPT will look at every single token and spend the same amount of compute on every one of them. And so, you can’t expect it to actually like – well, you can’t expect it to do sort of do too much work per token.
And also, in particular, basically these transformers are just like token simulators. They don’t know what they don’t know. They just imitate the next token. They don’t know what they’re good at or not good at. They just try their best to imitate the next token. They don’t reflect in the loop. They don’t sanity check anything. They don’t correct their mistakes along the way by default. They just sample token sequences. They don’t have separate inner monologue streams in their head, right? They are evaluating what’s happening.
Now, they do have some sort of cognitive advantages, I would say, and that is that they do actually have a very large fact-based knowledge across a vast number of areas because they have, say, several 10 billion parameters. It’s a lot of storage for a lot of facts, and they also, I think, have a relatively large and perfect working memory. Whatever fits into the context window is immediately available to the transformer through its internal self-attention mechanism. And so, it’s kind of like perfect memory, but it’s got that finite size. But the transformer has a very direct access to it. And so, it can lossless-ly remember anything that is inside its context window.
That’s kind of how I would compare those two. And the reason I bring all of this up is because I think to a large extent, prompting is just making up for this sort of cognitive difference between these two kind of architectures, like our brains here and LLM brains. You can look at it that way almost.
Chain of Thought #Here’s one thing that people found, for example, works pretty well in practice. Especially if your tasks require reasoning, you can’t expect the transformer to do too much reasoning per token. And so, you have to really spread out the reasoning across more and more tokens. For example, you can’t give a transformer a very complicated question and expect it to get the answer in a single token. There’s just not enough time for it. These transformers need tokens to think, quote/ unquote, I like to say sometimes.
And so, this is some of the things that work well. You may, for example, have a few short prompt that shows the transformer that it should show its work when it’s answering a question. And if you give a few examples, the transformer will imitate that template, and it will just end up working out better in terms of its evaluation.
Additionally, you can elicit this kind of behavior from the transformer by saying, let’s think step by step, because this conditions the transformer into sort of showing its work. And because it kind of snaps into a mode of showing its work, it’s going to do less computational work per token. And so, it’s more likely to succeed as a result, because it’s making slower reasoning over time.
Ensemble Multiple Attempts #Here’s another example. This one is called self-consistency. We saw that we had the ability to start writing, and then it didn’t work out. I can try again, and I can try multiple times and maybe select the one that worked best. In these kinds of approaches, you may sample not just once, but you may sample multiple times, and then have some process for finding the ones that are good, and then keeping just those samples or doing a majority vote, or something like that. Basically, these transformers in the process as they predict the next token, just like you, they can get unlucky. And they could sample not a very good token, and they can go down sort of like a blind alley in terms of reasoning.
And so, unlike you, they cannot recover from that. They are stuck with every single token they sample. And so, they will continue the sequence, even if they even know that this sequence is not going to work out. Give them the ability to look back, inspect or try to find, try to basically sample around it.
Ask for Reflection #Here’s one technique also. It turns out that, actually, LLMs, they know when they’ve screwed up. As an example, say you ask the model to generate a poem that does not rhyme, and it might give you a poem, but it actually rhymes. But it turns out that especially for the bigger models, like GPT-4, you can just ask it, did you meet the assignment? And actually, GPT-4 knows very well that it did not meet the assignment. It just kind of got unlucky in its sampling. And so, it will tell you, no, I didn’t actually meet the assignment. Here, let me try again.
But without you prompting it, it doesn’t even know. It doesn’t know to revisit, and so on. You have to make up for that in your prompts. You have to get it to check. If you don’t ask it to check, it’s not going to check by itself. It’s just a token simulator.
Recreate our ‘System 2’ #I think more generally, a lot of these techniques fall into the bucket of what I would say recreating our System 2. You might be familiar with the System 1, System 2 thinking for humans. System 1 is a fast, automatic process and I think kind of corresponds to an LLM just sampling tokens. And System 2 is the slower, deliberate planning sort of part of your brain.
And so, this is a paper actually from just last week, because the space is pretty quickly evolving. It’s called Tree of Thought, and in Tree of Thought, the authors of this paper proposed maintaining multiple completions for any given prompt. And then they are also scoring them along the way and keeping the ones that are going well, if that makes sense. And so, a lot of people are really playing around with kind of prompt engineering to basically bring back some of these abilities that we sort of have in our brain for LLMs.
Now, one thing I would like to note here is that this is not just a prompt. This is actually prompts that are, together, used with some Python glue code, because you actually have to maintain multiple prompts, and you also have to do some tree search algorithm here to like figure out which prompts to expand, etcetera. It’s a symbiosis of Python glue code and individual prompts that are called in a (wild?) loop or in a bigger algorithm.
I also think there’s a really cool parallel here to AlphaGo. AlphaGo has a policy for placing the next stone when it plays go, and this policy was trained originally by imitating humans. But in addition to this policy, it also does Monte-Carlo tree search. And basically, it will play out a number of possibilities in its head and evaluate all of them, and only keep the ones that work well. And so, I think this is kind of an equivalent of AlphaGo, but for text, if that makes sense.
Chains / Agents #Just like Tree of Thought, I think more generally, people are starting to really explore more general techniques of not just a simple question/answer prompts, but something that looks a lot more like Python glue code, stringing together many prompts.
On the right, I have an example from this paper called React, where they structure the answer to a prompt as a sequence of thought, action, observation, thought, action, observation. And it’s a full rollout, kind of a thinking process to answer the query. And in these actions, the model is also allowed to tool use.
On the left, I have an example of Auto GPT. And now, Auto GPT, by the way, is a project that I think got a lot of hype recently, but I think I still find it kind of inspirationally interesting. It’s a project that allows an LLM to sort of keep a task list and continue to recursively break down tasks. And I don’t think this currently works very well, and I would not advise people to use it in practical applications. I just think it’s something to generally take inspiration from in terms of where this is going, I think, over time.
That’s kind of like giving our model System 2 thinking.
Condition on Good Performance #The next thing that I find kind of interesting is this following sort of, I would say, almost psychological quirk of LLMs, is that LLMs don’t want to succeed. (Laughter.) They want to imitate. You want to succeed, and you should ask for it. (Laughter.) What I mean by that is when transformers are trained, they have training sets. And there can be an entire spectrum of performance qualities in their training data.
For example, there could be some kind of a prompt for some physics question or something like that, and there could be a student solution that is completely wrong, but there can also be an expert answer that is extremely right. And transformers can’t tell the difference between low – I mean, they know about low quality solutions and high quality solutions, but by default, they want to imitate all of it, because they’re just trained on language modeling. And so, at test time, you actually have to ask for a good performance.
In this example, in this paper, they tried various prompts, and let’s think step by step was very powerful, because it sort of like spread out the reasoning over many tokens. But what worked even better is, let’s work this out in a step by step way to be sure we have the right answer. And so, it’s kind of like a conditioning on getting a right answer. And this actually makes the transformer work better, because the transformer doesn’t have to now hedge its probability mass on low quality solutions, as ridiculous as that sounds.
And so, basically, feel free to ask for a strong solution. Say something like, you are a leading expert on this topic, pretend you have IQ 120, etcetera. But don’t try to ask for too much IQ because if you ask for IQ of like 400, you might be out of data distribution, or even worse, you could be in data distribution for some sci-fi stuff, and it will start to take on some sci-fi role playing or something like that. (Laughter.) You have to find like the right amount of IQ, I think. It’s got some U-shaped curve there.
Tool Use / Plugins #Next up, as we saw, when we are trying to solve problems, we know what we are good at and what we’re not good at, and we lean on tools computationally. You want to do the same potentially with your LLMs. In particular, we may want to give them calculators, code interpreters and so on, the ability to do search, and there’s a lot of techniques for doing that.
One thing to keep in mind, again, is that these transformers, by default, may not know what they don’t know. You may even want to tell the transformer in the prompt, you are not very good at mental arithmetic. Whenever you need to do very large number addition, multiplication or whatever, instead, use this calculator. Here’s how you use the calculator. Use this token combination, etcetera, etcetera. You have to actually spell it out because the model, by default, doesn’t know what it’s good at or not good at, necessarily just like you and I might be.
Retrieval-Augmented LLMs #Next up, I think something that is very interesting is we went from a world that was retrieval only. All the way, the pendulum has swung to the other extreme, where it’s memory only in LLMs. But actually, there’s this entire space in between of these retrieval augmented models, and this works extremely well in practice.
As I mentioned, the context window of a transformer is its working memory. If you can load the working memory with any information that is relevant to the task, the model will work extremely well, because it can immediately access all that memory. And so, I think a lot of people are really interested in basically retrieval augmented generation. And on the bottom, I have an example of LAMA index, which has one sort of data connector to lots of different types of data. And you can you can index all of that data, and you can make it accessible to LLMs.
And the emerging recipe there is you take relevant documents, you split them up into chunks, you embed all of them, and you basically get embedding vectors that represent that data. You store that in the vector store, and then at test time, you make some kind of a query to your vector store. And you fetch chunks that might be relevant to your task, and you stuff them into the prompt, and then you generate. This can work quite well in practice.
This is, I think, similar to when you and I solve problems. You can do everything from your memory, and transformers have very large and extensive memory, but also, it really helps to reference some primary documents. Whenever you find yourself going back to a textbook to find something or whenever you find yourself going back to the documentation of a library to look something up, the transformers definitely want to do that, too. You have some memory over how some documentation of a library works, but it’s much better to look it up. Same applies here.
Constrained Prompting #Next, I wanted to briefly talk about constraint prompting. I also find this very interesting. This is basically techniques for enforcing a certain template in the outputs of LLMs. Guidance is one example from Microsoft, actually. And here we are, enforcing that the output from the LLM will be JSON. And this will actually guarantee that the output will take on this form, because they go in and they mess with the probabilities of all the different tokens that come out of the transformer, and they clamp those tokens. And then the transformer is only filling in the blanks here. And then you can enforce additional restrictions on what could go into those blanks.
This might be really helpful, and I think this kind of constraint sampling is also extremely interesting.
Fine-tuning #I also wanted to say a few words about finetuning. It is the case that you can get really far with prompt engineering, but it’s also possible to think about finetuning your models.
Now, finetuning models means that you are actually going to change the weights of the model. It is becoming a lot more accessible to do this in practice, and that’s because of a number of techniques that have been developed and have libraries for very recently.
For example, parameter efficient finetuning techniques like LoRA, make sure that you’re only training small, sparse pieces of your model. Most of the model is kept clamped at the base model and some pieces of it are allowed to change. And it still works pretty well, empirically, and makes it much cheaper to sort of tune only small pieces of your model. It also means that because most of your model is clamped, you can use very low precision inference for computing those parts, because they are not going to be updated by gradient descent. And so, that makes everything a lot more efficient as well.
And in addition, we have a number of open sourced, high quality based models currently, as I mentioned. And I think LAMA’s quite nice, although it is not commercially licensed, I believe, right now.
Something to keep in mind is that basically, finetuning is a lot more technically involved. It requires a lot more, I think, technical expertise to do right. It requires human data contractors for data sets and/or synthetic data pipelines that can be pretty complicated. This will definitely slow down your iteration cycle by a lot.
And I would say on a high level, SFT is achievable because you’re continuing the language modeling task. It’s relatively straightforward. But RLHF, I would say, is very much research territory and is even much harder to get to work. And so, I would probably not advise that someone just tries to roll their own RLHF implementation. These things are pretty unstable, very difficult to train, not something that is, I think, very beginner friendly right now. And it’s also potentially likely also to change pretty rapidly, still.
Default Recommendations* #I think these are my sort of default recommendations right now. I would break up your task into two major parts. Number one, achieve your top performance, and number two, optimize your performance in that order.
Number one, the best performance will currently come from GPT-4 model. It is the most capable by far. Use prompts that are very detailed. They have lots of task context, relevant information and instructions. Think along the lines of what would you tell a task contractor if they can’t e-mail you back? But then also keep in mind that a task contractor is a human, and they have inner monologue, and they’re very clever, etcetera. LLMs do not possess those qualities, so make sure to think through the psychology of the LLM, almost, and cater prompts to that.
Retrieve and add any relevant context and information to these prompts, basically refer to a lot of the prompt engineering techniques. Some of them are highlighted in the slides above, but also this is a very large space, and I would just advise you to look for prompt engineering techniques online. There’s a lot to cover there.
Experiment with a few short examples. What this refers to is you don’t just want to tell, you want to show whenever it’s possible. Give it examples of everything that helps it really understand what you mean, if you can.
Experiment with tools and plugins to offload a task that are difficult for LLMs natively, and then think about not just a single prompt and answer. Think about potential change and reflection, and how you glue them together, and how you could potentially make multiple samples, and so on.
Finally, if you think you’ve squeezed out prompt engineering, which I think you should stick with for a while, look at some potentially finetuning a model to your application, but expect this to be a lot more slower and involved. And then there’s an expert fragile research zone here, and I would say that is RLHF, which currently does work a bit better than SFT, if you can get it to work. But again, this is pretty involved, I would say. And to optimize your costs, try to explore lower capacity models or shorter prompts and so on.
Use Cases #I also wanted to say a few words about the use cases in which I think LLMs are currently well-suited for. In particular, note that there’s a large number of limitations to LLMs today. And so, I would keep that definitely in mind for all your applications. And this is, by the way, could be an entire talk, so I don’t have time to cover it in full detail.
Models may be biased. They may fabricate, hallucinate information. They may have reasoning errors. They may struggle an entire classes of applications. They have knowledge cutoffs, so they might not know any information about, say, September 2021. They are susceptible to a large range of attacks, which are sort of like coming out on Twitter daily, including prompt injection, jailbreak attacks, data poisoning attacks and so on.
My recommendation right now is use LLMs in low stakes applications, combine them always with human oversight, use them as a source of inspiration and suggestions, and think copilots instead of completely autonomous agents that are just performing a task somewhere. It’s just not clear that the models are there right now.
GPT-4 \u0026amp; Looking Forward #I wanted to close by saying that GPT-4 is an amazing artifact. I’m very thankful that it exists, and it’s beautiful. It has a ton of knowledge across so many areas that can do math, code, and so on. And in addition, there’s this thriving ecosystem of everything else that is being built and incorporated into the ecosystem, some of these things I talked about. And all of this power is accessible at your fingertips.
OpenAI API #Here’s everything that’s needed in terms of code to ask GPT-4 a question, to prompt it and get a response. In this case, I said, “Can you say something to inspire the audience of Microsoft Build 2023?” And I just punched this into Python, and verbatim, GPT-4 said the following.
And by the way, I did not know that they used this trick in the keynote, so I thought I was being clever. (Laughter.) But it is really good at this.
It says, “Ladies and gentlemen, innovators and trailblazers of Microsoft Build 2023, welcome to the gathering of brilliant minds like no other. You are architects of the future, the visionaries molding the digital realm in which humanity thrives. Embrace the limitless possibilities of technologies and let your ideas soar as high as your imagination. Together, let’s create a more connected, remarkable and inclusive world for generations to come. Get ready to unleash your creativity, canvass the unknown and turn dreams into reality. Your journey begins today.”
Thank you. (Applause.)
END
`}),e.add({id:10,href:"/notes/nlp/generative-ai-with-llms/",title:"Generative AI with Large Langauge Models",description:"Generative AI with Large Langauge Models",content:""}),e.add({id:11,href:"/notes/nlp/generative-ai-with-llms/introduction/",title:"Introduction",description:"Introduction",content:`Generative AI with Large Language Models course developed by DeepLearning.AI and AWS
Notes #Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle Week 1 Part 1 - Transformers Architecture Week 1 Part 2 - Pre-training and Scaling Laws Week 1 - Research Papers Week 2 Part 1 - Fine-tuning LLMs with Instruction Week 2 Part 2 - Parameter Efficient Fine-tuning Week 2 - Research Papers Week 3 - TODO Credits #The in-line diagrams are taken from Coursera, unless specified otherwise These notes were developed using lectures/material/transcripts from the course `}),e.add({id:12,href:"/notes/nlp/generative-ai-with-llms/week-1-part-1/",title:"Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle",description:"Week 1 Part 1 - Introduction to LLMs and the Generative AI Project Lifecycle",content:`These notes were developed using lectures/material/transcripts from the DeepLearning.AI \u0026amp; AWS - Generative AI with Large Language Models course
Notes #Generative AI \u0026amp; LLMs Gen AI - subset of machine learning Models learn statistical patterns in massive datasets of content LLM Use Cases and Tasks Augmenting LLMs by connecting them to external data sources or using them to invoke external APIs Provide the model with information it doesn\u0026rsquo;t know from its pre-training Text Generation Before Transformers Transformers Architecture Transformers Architecture Generating Text with Transformers How the Overall Prediction Process Works Encoder-Decoder Architecture Encoder - encodes input sequences into a deep representation of the structure and meaning of the input Decoder - working from input token triggers, uses the encoder\u0026rsquo;s contextual understanding to generate new tokens, does this in a loop until a stop condition is reached Variants Encoder-only - BERT Encoder-Decoder - BART Decoder-only - GPT, BLOOM, LLaMA Reading: Transformers: Attention is All You Need (2017) https://arxiv.org/abs/1706.03762 Transformer model - entirely attention-based Self-Attention - compute representations of input sequences, capture long-term dependencies and parallelize computation effectively Encoder-Decoder Layers Two Sublayers Multi-Head Self-Attention - allows the model to attend to different parts of the input sequence Feed-Forward Neural Network - applies a point-wise fully connected layer to each position separately and identically Residual Connections and Layer Normalization - facilitate training and prevent overfitting Positional Encoding - encodes the position of each token in the input sequence, capture the order of the sequence Prompting and Prompt Engineering Prompt - text that you feed into the model Inference - act of generating text Completion - the output text Context Window - full amount of text or the memory that is available to use for the prompt Prompt Engineering - work to develop and improve the prompt In-Context Learning - engineer your prompts to encourage the model to learn by examples Zero Shot Inference One Shot Inference Few Shot Inference Generative Configuration - Inference Parameters max_new_tokens - limit the number of tokens that the model will generate Greedy Decoding - the word/token with the highest probability is selected Random Sampling - select a token using a random-weighted strategy across the probabilities of all tokens do_sample=True - to disable greedy decoding and enable random sampling top_k sampling - choose from only the k tokens with the highest probability specify the number of tokens to randomly choose from top_p sampling - limit the random sampling to the predictions whose combined probabilities do not exceed p specify the total probability that you want the model to choose from temperature - control the randomness of the model output the higher the temperature, the higher the randomness, and the lower the temperature, the lower the randomness How it works? influences the shape of the probability distribution that the model calculates for the next token temperature value is a scaling factor that\u0026rsquo;s applied within the final softmax layer of the model that impacts the shape of the probability distribution of the next token Generative AI Project Lifecycle Scope - define the scope as accurately and narrowly as you can Select - train your own model or work with an existing model Adapt and Align - assess its performance and carry out additional training if needed Prompt Engineering Fine-tuning Full Fine-tuning Parameter Efficient Fine-tuning (PEFT) Reinforcement Learning from Human Feedback (RLHF) Application Integration - optimize your model for deployment and consider any additional infrastructure that your application will require to work well Lab 1 - Prompt Engineering Task: Dialogue Summarization Prompt Template Zero Shot Inference One Shot Inference Few Shot Inference Text Generation Strategies do_sample=True temperature Generative AI \u0026amp; LLMs #Generative AI is a subset of traditional machine learning. And the machine learning models that underpin generative AI have learned these abilities by finding statistical patterns in massive datasets of content that was originally generated by humans Large language models have been trained on trillions of words (tokens) over many weeks and months, and with large amounts of compute power These foundation models (base models), as we call them, with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem solve Here are a collection of foundation models, sometimes called base models, and their relative size in terms of their parameters. You\u0026rsquo;ll cover these parameters in a little more detail later on, but for now, think of them as the model\u0026rsquo;s memory. And the more parameters a model has, the more memory, and as it turns out, the more sophisticated the tasks it can perform LLM Use Cases and Tasks #An area of active development is augmenting LLMs by connecting them to external data sources or using them to invoke external APIs. You can use this ability to provide the model with information it doesn\u0026rsquo;t know from its pre-training and to enable your model to power interactions with the real-world Developers have discovered that as the scale of foundation models grows from hundreds of millions of parameters to billions, even hundreds of billions, the subjective understanding of language that a model possesses also increases. This language understanding stored within the parameters of the model is what processes, reasons, and ultimately solves the tasks you give it, but it\u0026rsquo;s also true that smaller models can be fine tuned to perform well on specific focused tasks Text Generation Before Transformers #RNNs, powerful for their time, were limited by the amount of compute and memory needed to perform well at generative tasks As you scale the RNN implementation to be able to see more of the preceding words in the text, you have to significantly scale the resources that the model uses The problem here is that language is complex. In many languages, one word can have multiple meanings Transformers Architecture #Transformers Architecture
Generating Text with Transformers #How the Overall Prediction Process Works from End to End #Look at a translation task or a sequence-to-sequence task, which incidentally was the original objective of the transformer architecture designers Translate the French phrase [FOREIGN] into English Encoder
First, you\u0026rsquo;ll tokenize the input words using this same tokenizer that was used to train the network. These tokens are then added into the input on the encoder side of the network, passed through the embedding layer, and then fed into the multi-headed attention layers. The outputs of the multi-headed attention layers are fed through a feed-forward network to the output of the encoder. At this point, the data that leaves the encoder is a deep representation of the structure and meaning of the input sequence. Decoder
This representation is inserted into the middle of the decoder to influence the decoder\u0026rsquo;s self-attention mechanisms. Next, a start of sequence token is added to the input of the decoder. This triggers the decoder to predict the next token, which it does based on the contextual understanding that it\u0026rsquo;s being provided from the encoder. The output of the decoder\u0026rsquo;s self-attention layers gets passed through the decoder feed-forward network and through a final softmax output layer. At this point, we have our first token. You\u0026rsquo;ll continue this loop, passing the output token back to the input to trigger the generation of the next token, until the model predicts an end-of-sequence token. At this point, the final sequence of tokens can be detokenized into words, and you have your output. In this case, I love machine learning Text Generation Strategies
There are multiple ways in which you can use the output from the softmax layer to predict the next token. These can influence how creative your generated text is Summary #The complete transformer architecture consists of an encoder and decoder components The encoder encodes input sequences into a deep representation of the structure and meaning of the input The decoder, working from input token triggers, uses the encoder\u0026rsquo;s contextual understanding to generate new tokens. It does this in a loop until some stop condition has been reached Variants #While the translation example you explored here used both the encoder and decoder parts of the transformer, you can split these components apart for variations of the architecture. Encoder-only models also work as sequence-to-sequence models, but without further modification, the input sequence and the output sequence or the same length. Their use is less common these days, but by adding additional layers to the architecture, you can train encoder-only models to perform classification tasks such as sentiment analysis, BERT is an example of an encoder-only model. Encoder-decoder models, as you\u0026rsquo;ve seen, perform well on sequence-to-sequence tasks such as translation, where the input sequence and the output sequence can be different lengths. You can also scale and train this type of model to perform general text generation tasks. Examples of encoder-decoder models include BART as opposed to BERT and T5, the model that you\u0026rsquo;ll use in the labs in this course. Finally, decoder-only models are some of the most commonly used today. Again, as they have scaled, their capabilities have grown. These models can now generalize to most tasks. Popular decoder-only models include the GPT family of models, BLOOM, Jurassic, LLaMA, and many more. You\u0026rsquo;ll be interacting with transformer models through natural language, creating prompts using written words, not code. You don\u0026rsquo;t need to understand all of the details of the underlying architecture to do this. This is called Prompt Engineering Reading: Transformers: Attention is All You Need (2017) #\u0026ldquo;Attention is All You Need\u0026rdquo; is a research paper published in 2017 by Google researchers, which introduced the Transformer model, a novel architecture that revolutionized the field of natural language processing (NLP) and became the basis for the LLMs we now know - such as GPT, PaLM and others. The paper proposes a neural network architecture that replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with an entirely attention-based mechanism.
The Transformer model uses self-attention to compute representations of input sequences, which allows it to capture long-term dependencies and parallelize computation effectively. The authors demonstrate that their model achieves state-of-the-art performance on several machine translation tasks and outperform previous models that rely on RNNs or CNNs.
The Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers. Each layer consists of two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network. The multi-head self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network applies a point-wise fully connected layer to each position separately and identically.
The Transformer model also uses residual connections and layer normalization to facilitate training and prevent overfitting. In addition, the authors introduce a positional encoding scheme that encodes the position of each token in the input sequence, enabling the model to capture the order of the sequence without the need for recurrent or convolutional operations.
You can read the Transformers paper here.
Prompting and Prompt Engineering #Terminology text that you feed into the model is called the prompt the act of generating text is known as inference the output text is known as the completion The full amount of text or the memory that is available to use for the prompt is called the context window This work to develop and improve the prompt is known as prompt engineering Providing examples inside the context window is called in-context learning Zero Shot Inference #Providing examples inside the context window is called in-context learning The largest of the LLMs are surprisingly good at this, grasping the task to be completed and returning a good answer Smaller models, on the other hand, can struggle with this One Shot Inference #Providing an example within the prompt can improve performance The inclusion of a single example is known as one-shot inference, in contrast to the zero-shot prompt you supplied earlier Few Shot Inference #can extend the idea of giving a single example to include multiple examples. This is known as few-shot inference including a mix of examples with different output classes can help the model to understand what it needs to do Summary of In-Context Learning (ICL) #To recap, you can engineer your prompts to encourage the model to learn by examples. While the largest models are good at zero-shot inference with no examples, smaller models can benefit from one-shot or few-shot inference that include examples of the desired behavior. But remember the context window because you have a limit on the amount of in-context learning that you can pass into the model. Generally, if you find that your model isn\u0026rsquo;t performing well when, say, including five or six examples, you should try fine-tuning your model instead. Fine-tuning performs additional training on the model using new data to make it more capable of the task you want it to perform The Significance of Scale: Language Understanding #As larger and larger models have been trained, it\u0026rsquo;s become clear that the ability of models to perform multiple tasks and how well they perform those tasks depends strongly on the scale of the model. As you heard earlier in the lesson, models with more parameters are able to capture more understanding of language. The largest models are surprisingly good at zero-shot inference and are able to infer and successfully complete many tasks that they were not specifically trained to perform. In contrast, smaller models are generally only good at a small number of tasks. Typically, those that are similar to the task that they were trained on. You may have to try out a few models to find the right one for your use case. Once you\u0026rsquo;ve found the model that is working for you, there are a few settings that you can experiment with to influence the structure and style of the completions that the model generates. Question: Which in-context learning method involves creating an initial prompt that states the task to be completed and includes a single example question with answer followed by a second question to be answered by the LLM?
One shot
One shot inference involves providing an example question with answer followed by a second question to be answered by the LLM. Few shot inference provides multiple example prompts and answers while zero shot provides only one prompt to be answered by the LLM.
Generative Configuration #Examine some of the methods and associated configuration parameters that you can use to influence the way that the model makes the final decision about next-word generation Each model exposes a set of configuration parameters that can influence the model\u0026rsquo;s output during inference Note that these are different than the training parameters which are learned during training time. Instead, these configuration parameters are invoked at inference time and give you control over things like the maximum number of tokens in the completion, and how creative the output is max_new_tokens #max_new tokens - use it to limit the number of tokens that the model will generate. You can think of this as putting a cap on the number of times the model will go through the selection process Note how the length of the completion in the example for 200 is shorter. This is because another stop condition was reached, such as the model predicting and end of sequence token. Remember it\u0026rsquo;s max new tokens, not a hard number of new tokens generated The output from the transformer\u0026rsquo;s softmax layer is a probability distribution across the entire dictionary of words that the model uses. Here you can see a selection of words and their probability score next to them. Although we are only showing four words here, imagine that this is a list that carries on to the complete dictionary. Greedy Decoding #Simplest form of next-word prediction, where the model will always choose the word with the highest probability can work very well for short generation but is susceptible to repeated words or repeated sequences of words If you want to generate text that\u0026rsquo;s more natural, more creative and avoids repeating words, you need to use some other controls Random Sampling #Easiest way to introduce some variability, model chooses an output word at random using the probability distribution to weight the selection For example, in the illustration, the word banana has a probability score of 0.02. With random sampling, this equates to a 2% chance that this word will be selected. By using this sampling technique, we reduce the likelihood that words will be repeated. However, depending on the setting, there is a possibility that the output may be too creative, producing words that cause the generation to wander off into topics or words that just don\u0026rsquo;t make sense. Note that in some implementations, you may need to disable greedy and enable random sampling explicitly. For example, the Hugging Face transformers implementation that we use in the lab requires that we set do_sample=True Let\u0026rsquo;s explore top k and top p sampling techniques to help limit the random sampling and increase the chance that the output will be sensible Two Settings, top p and top k are sampling techniques that we can use to help limit the random sampling and increase the chance that the output will be sensible top_k sampling #To limit the options while still allowing some variability, you can specify a top k value which instructs the model to choose from only the k tokens with the highest probability. In this example here, k is set to three, so you\u0026rsquo;re restricting the model to choose from these three options. The model then selects from these options using the probability weighting and in this case, it chooses donut as the next word. This method can help the model have some randomness while preventing the selection of highly improbable completion words. This in turn makes your text generation more likely to sound reasonable and to make sense top_p sampling #Alternatively, you can use the top p setting to limit the random sampling to the predictions whose combined probabilities do not exceed p. For example, if you set p to equal 0.3, the options are cake and donut since their probabilities of 0.2 and 0.1 add up to 0.3. The model then uses the random probability weighting method to choose from these tokens. With top k, you specify the number of tokens to randomly choose from, and
With top p, you specify the total probability that you want the model to choose from.
temperature #One more parameter that you can use to control the randomness of the model output is known as temperature. This parameter influences the shape of the probability distribution that the model calculates for the next token. Broadly speaking, the higher the temperature, the higher the randomness, and the lower the temperature, the lower the randomness. The temperature value is a scaling factor that\u0026rsquo;s applied within the final softmax layer of the model that impacts the shape of the probability distribution of the next token. In contrast to the top k and top p parameters, changing the temperature actually alters the predictions that the model will make. If you choose a low value of temperature, say less than one, the resulting probability distribution from the softmax layer is more strongly peaked with the probability being concentrated in a smaller number of words. You can see this here in the blue bars beside the table, which show a probability bar chart turned on its side. Most of the probability here is concentrated on the word cake. The model will select from this distribution using random sampling and the resulting text will be less random and will more closely follow the most likely word sequences that the model learned during training. If instead you set the temperature to a higher value, say, greater than one, then the model will calculate a broader flatter probability distribution for the next token. Notice that in contrast to the blue bars, the probability is more evenly spread across the tokens. This leads the model to generate text with a higher degree of randomness and more variability in the output compared to a cool temperature setting. This can help you generate text that sounds more creative. If you leave the temperature value equal to one, this will leave the softmax function as default and the unaltered probability distribution will be used. Question: Which configuration parameter for inference can be adjusted to either increase or decrease randomness within the model output layer?
Temperature is used to affect the randomness of the output of the softmax layer. A lower temperature results in reduced variability while a higher temperature results in increased randomness of the output. Summary up till this point
examined the types of tasks that LLMs are capable of performing learned about transformers, the model architecture that powers these amazing tools explored how to get the best possible performance out of these models using prompt engineering experiment with different inference configuration parameters In the next video, you\u0026rsquo;ll start building on this foundational knowledge by thinking through the steps required to develop and launch an LLM-powered application.
Generative AI Project Lifecycle #Scope #The most important step in any project is to define the scope as accurately and narrowly as you can. As you\u0026rsquo;ve seen in this course so far, LLMs are capable of carrying out many tasks, but their abilities depend strongly on the size and architecture of the model. You should think about what function the LLM will have in your specific application. Select #Once you\u0026rsquo;re happy, and you\u0026rsquo;ve scoped your model requirements enough to begin development. Your first decision will be whether to train your own model from scratch or work with an existing base model. In general, you\u0026rsquo;ll start with an existing model, although there are some cases where you may find it necessary to train a model from scratch. You\u0026rsquo;ll learn more about the considerations behind this decision later this week, as well as some rules of thumb to help you estimate the feasibility of training your own model. Adapt and Align #With your model in hand, the next step is to assess its performance and carry out additional training if needed for your application. As you saw earlier this week, prompt engineering can sometimes be enough to get your model to perform well, so you\u0026rsquo;ll likely start by trying in-context learning, using examples suited to your task and use case. There are still cases, however, where the model may not perform as well as you need, even with one or a few short inference In that case, you can try fine-tuning your model. This supervised learning process will be covered in detail in Week 2, and you\u0026rsquo;ll get a chance to try fine tuning a model yourself in the Week 2 lab. As models become more capable, it\u0026rsquo;s becoming increasingly important to ensure that they behave well and in a way that is aligned with human preferences in deployment. In Week 3, you\u0026rsquo;ll learn about an additional fine-tuning technique called Reinforcement Learning from Human Feedback (RLHF), which can help to make sure that your model behaves well. An important aspect of all of these techniques is evaluation. Next week, you will explore some metrics and benchmarks that can be used to determine how well your model is performing or how well aligned it is to your preferences. Note that this adapt and aligned stage of app development can be highly iterative. You may start by trying prompt engineering and evaluating the outputs, then using fine tuning to improve performance and then revisiting and evaluating prompt engineering one more time to get the performance that you need. Application Integration #Finally, when you\u0026rsquo;ve got a model that is meeting your performance needs and is well aligned, you can deploy it into your infrastructure and integrate it with your application. At this stage, an important step is to optimize your model for deployment. This can ensure that you\u0026rsquo;re making the best use of your compute resources and providing the best possible experience for the users of your application. The last but very important step is to consider any additional infrastructure that your application will require to work well. There are some fundamental limitations of LLMs that can be difficult to overcome through training alone like their tendency to invent information when they don\u0026rsquo;t know an answer, or their limited ability to carry out complex reasoning and mathematics. In the last part of this course, you\u0026rsquo;ll learn some powerful techniques that you can use to overcome these limitations. Scope
The most important step in any project is to define the scope as accurately and narrowly as you can. As you\u0026rsquo;ve seen in this course so far, LLMs are capable of carrying out many tasks, but their abilities depend strongly on the size and architecture of the model. You should think about what function the LLM will have in your specific application. Do you need the model to be able to carry out many different tasks, including long-form text generation or with a high degree of capability, or is the task much more specific like named entity recognition so that your model only needs to be good at one thing. As you\u0026rsquo;ll see in the rest of the course, getting really specific about what you need your model to do can save you time and perhaps more importantly, compute cost. Introduction to AWS Labs #SageMaker Studio - a Jupyter based IDE that we\u0026rsquo;ll be doing all of our notebooks in today Lab 1 Walkthrough - Mainly Going through Prompt Engineering #Dataset: Dialogsum Model: Flan-T5 Base AutoModelForSeq2SeqLM Tokenizer - text to tokens The tokenizer\u0026rsquo;s job is to convert raw text into numbers. Those numbers point to a set of vectors or the embeddings as they\u0026rsquo;re often called, that are then used in mathematical operations like our deep learning, back-propagation, our linear algebra, all that fun stuff In-context Learning Zero Shot Inference - somewhat ok One Shot Inference - slightly better Few Shot Inference - in this case 3 full examples Doesn’t do much better than one shot inference Typically, in my experience, above five or six shots, so full prompt and then completions, you really don\u0026rsquo;t gain much after that. Either the model can do it or it can\u0026rsquo;t do it and going about five or six. Generative Configuration Parameters for Inference do_sample=True In some cases, for example, by raising the temperature up above, towards one or even closer to two, you will get very creative type of responses. temperature If you lower it down I believe 0.1 is the minimum for the hugging face implementation anyway, of this generation config class here that\u0026rsquo;s used when you actually generate. If you go down to 0.1, that will actually make the response more conservative and will oftentimes give you the same response over and over. If you go higher, I believe actually 2.0 is the highest. If you try to 2.0, that will start to give you some very wild responses. Lab 1 - Generative AI Use Case: Summarize Dialogue #In this lab, you will do the dialogue summarization task using generative AI. You will explore
how the input text affects the output of the model, and perform prompt engineering to direct it towards the task you need. By comparing zero shot, one shot, and few shot inferences, you will take the first step towards prompt engineering and see how it can enhance the generative output of Large Language Models. `}),e.add({id:13,href:"/notes/nlp/generative-ai-with-llms/transformers-architecture/",title:"Week 1 Part 1 - Transformers Architecture",description:"Week 1 Part 1 - Transformers Architecture",content:`These notes were developed using lectures/material/transcripts from the DeepLearning.AI \u0026amp; AWS - Generative AI with Large Language Models course
Notes #Self-Attention - ability to learn the relevance and context of all the words in a sentence Attention weights are learned during training These layers reflect the importance of each word in that input sequence to all other words in the sequence Transformer Architecture: Encoder-Decoder Translation: sequence-to-sequence task Tokenizer - converts words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with Each token is mapped to a token ID Subword tokenization - gets the best of word-level tokenization and character-level tokenization Note: must use the same tokenizer when generating text Embedding Each token ID is mapped to a vector Original transformer has embedding size of 512 Weights are learned during training - these vectors learn to encode the meaning and context of individual tokens in the input sequence Positional Encoding Model processes each of the input tokens in parallel Adding positional encoding preserves the information about the word order Self-Attention Layer Analyzes the relationships between the tokens Attend to different parts of the input sequence to better capture the contextual dependencies between the words Weights are learned during training - reflect the importance of each word in that input sequence to all other words in the sequence Multi-Headed Attention Layer Multiple sets of self-attention weights or heads are learned in parallel independently of each other Fully Connected Feed-Forward Network Output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary Softmax Layer Logits are normalized into a probability score for each word in the vocabulary The most likely predicted token will have the highest score Text Generation Strategies Transformers Architecture #Self-Attention #The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence Not just as you see here, to each word next to its neighbor, but to every other word in a sentence and to apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input These attention weights are learned during LLM training This is called self-attention and the ability to learn attention in this way across the whole input significantly approves the model\u0026rsquo;s ability to encode language Simplified Diagram of the Transformer Architecture #The transformer architecture is split into two distinct parts the encoder and the decoder These components work in conjunction with each other and they share a number of similarities Tokenizer #Before passing texts into the model to process, you must first tokenize the words This converts the words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with Multiple tokenization methods Word tokenization, Character-level tokenization, Subword tokenization What\u0026rsquo;s important is that once you\u0026rsquo;ve selected a tokenizer to train the model, you must use the same tokenizer when you generate text Embedding #Now that your input is represented as numbers, you can pass it to the embedding layer This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like word2vec use this concept Looking back at the sample sequence, you can see that in this simple case, each word has been matched to a token ID, and each token is mapped into a vector In the original transformer paper, the vector size was actually 512 Visualization: Embedding in 3D #For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words. You can see now how you can relate words that are located close to each other in the embedding space, and how you can calculate the distance between the words as an angle, which gives the model the ability to mathematically understand language Positional Encoding #As you add the token vectors into the base of the encoder or the decoder, you also add positional encoding The model processes each of the input tokens in parallel So by adding the positional encoding, you preserve the information about the word order and don\u0026rsquo;t lose the relevance of the position of the word in the sentence. Self-Attention Layer #Once you\u0026rsquo;ve summed the input tokens and the positional encodings, you pass the resulting vectors to the self-attention layer The model analyzes the relationships between the tokens in your input sequence. As you saw earlier, this allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words. The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence. Multi-Headed Attention Layer #But this does not happen just once, the transformer architecture actually has multi-headed self-attention. This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other. The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common. The intuition here is that each self-attention head will learn a different aspect of language. For example, one head may see the relationship between the people entities in our sentence. Whilst another head may focus on the activity of the sentence. Whilst yet another head may focus on some other properties such as if the words rhyme. Note: you don\u0026rsquo;t dictate ahead of time what aspects of language the attention heads will learn. The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language. While some attention maps are easy to interpret, like the examples discussed here, others may not be. Fully-Connected Feed-Forward Network #Now that all of the attention weights have been applied to your input data, the output is processed through a fully-connected feed-forward network. The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary Softmax Layer #Pass these logits to a final softmax layer, where they are normalized into a probability score for each word. This output includes a probability for every single word in the vocabulary, so there\u0026rsquo;s likely to be thousands of scores here. One single token will have a score higher than the rest. This is the most likely predicted token. There are a number of methods that you can use to vary the final selection from this vector of probabilities (Text Generation Strategies)
`}),e.add({id:14,href:"/notes/nlp/generative-ai-with-llms/week-1-part-2/",title:"Week 1 Part 2 - Pre-training and Scaling Laws",description:"Week 1 Part 2 - Pre-training and Scaling Laws",content:`These notes were developed using lectures/material/transcripts from the DeepLearning.AI \u0026amp; AWS - Generative AI with Large Language Models course
Notes #Pre-training Large Language Models Stage 1 of Generative AI Project Lifecycle - Select Work with an existing Foundation model or train your own model Model cards - model details, uses, bias, risks, limitations, training details and evaluation Model Architectures and Pre-training Objectives LLMs encode a deep statistical representation of language The model’s pre-training phase - the model learns from vast amounts of unstructured textual data (petabytes of text) In this self-supervised learning step, the model internalizes the patterns and structures present in the language Transformer Variants Autoencoding (encoder only) models - trained using Masked Language Modeling (MLM), denoising objective Build bi-directional representations of the input sequence, meaning that the model has an understanding of the full context of a token and not just of the words that come before Tasks: Sentiment Analysis, Named Entity Recognition (NER), Sentence/Word/Token Classification Models: BERT, RoBERTa Autoregressive (decoder only) models - trained using Causal Language Modeling (CLM) Mask the input sequence and can only see the input tokens leading up to the token in question, the model has no knowledge of the end of the sentence. The context is unidirectional By learning to predict the next token from a vast number of examples, the model builds up a statistical representation of language Tasks: Text Generation, Emergent Abilities (Zero Shot Inference, etc.) Models: GPT, BLOOM Sequence-to-Sequence (encoder-decoder) models Exact details of the pre-training objective vary from model to model Tasks: Translation, Text Summarization, and Question Answering Models: T5, BART The Significance of Scale: Task Ability The larger a model, the more likely it is to work as you needed to without additional in-context learning or further training Computational Challenges of Training LLMs Challenges: OutOfMemoryError - CUDA out of memory Require sufficient memory to store all this: Weights: 4 bytes per parameter Two Adam optimizer states: 8 bytes per parameter Gradients: 4 bytes per parameter Activations and Temporary variables needed by your functions: 8 bytes per parameter Total: 4 bytes per parameter + 20 extra bytes per parameter Quantization - reduce the memory requirement by reducing their precision Memory to store a number Bits Exponent Fraction (referred to as Mantissa) Data Types FP32: 4 bytes of memory FP16: 2 bytes of memory BFLOAT16: 2 bytes of memory Hybrid between half precision FP16 and full precision FP32 Downside: not well suited for integer calculations INT8: 1 byte of memory INT4 Quantization statistically projects the original 32-bit floating point numbers into a lower precision space, using scaling factors calculated based on the range of the original 32-bit floating point numbers Quantization-Aware Training (QAT) learns the quantization scaling factors during training Note: Quantization does not reduce the number of model parameters Approximate GPU RAM Needed to Store and Train 1B Parameters Memory needed to store model: 4GB @32-bit full precision 2GB @16-bit half precision 1GB @8-bit precision Memory needed to train model: 80GB @32-bit full precision 40GB @16-bit half precision 20GB @8-bit precision 80GB is the maximum memory for the NVIDIA A100 GPU As the model sizes get larger, you will need to split your model across multiple GPUs for training Optional: Efficient Multi-GPU Compute Strategies When to Use Distributed Compute Model too big for single GPU Model fits on GPU, train data in parallel PyTorch Implementation Distributed Data Parallel (DDP) - Model fits on a single GPU Requires that your model weights and all of the additional parameters, gradients, and optimizer states that are needed for training, fit onto a single GPU Copies your model onto each GPU and sends batches of data to each of the GPUs in parallel. Each data-set is processed in parallel and then a synchronization step combines the results of each GPU, which in turn updates the model on each GPU, which is always identical across chips Fully Sharded Data Parallel (FSDP) - Model no longer fits on a single GPU Model Sharding FSDP is motivated by the “ZERO” paper - zero data overlap between GPUs sharding factor - configure the level of sharding to manage the trade-off between performance and memory utilization 1 = removes the sharding and replicated the full model similar to DDP Maximum number of GPUs = full sharding Anywhere in between = hybrid sharding Can use FSDP for both small and large models and seamlessly scale your model training across multiple GPUs Scaling Laws and Compute-Optimal Models To determine how big models need to be, learn about research that explore the relationship between Model size Training Configuration Performance Scaling Choices for Pre-training Goal: maximize the model\u0026rsquo;s performance of its learning objective, which is minimizing the loss when predicting tokens Options to achieve better performance: Increasing the size of the dataset you train your model on Increasing the number of parameters in your model Constraint: compute budget (number of GPUs, training time, cost) Unit of Compute “petaFLOP/s-day” = a measurement of the number of floating point operations performed at rate of 1 petaFLOP per second for one day 1 petaFLOP corresponds to one quadrillion floating point operations per second In the context of training transformers, 1 petaFLOP per second day is approximately equivalent to eight NVIDIA V100 GPUs, operating at full efficiency for one full day Compute Budget vs Model Performance Increase your compute budget to achieve better model performance In practice however, the compute resources you have available for training will generally be a hard constraint set by factors such as the hardware you have access to, the time available for training and the financial budget of the project Dataset Size and Model Size vs Model Performance As the volume of training data increases, the performance of the model continues to improve As the model size increases in size, the test loss decreases indicating better performance Chinchilla Paper: Training Compute-Optimal Large Language Models Goal: to find the optimal number of parameters and volume of training data for a given compute budget Findings Very large models may be over-parameterized and under-trained Smaller models trained on more data could perform as well as large model Chinchilla Scaling Laws for Model and Dataset Size The optimal training dataset size for a given model is about 20 times larger than the number of parameters in the model The compute optimal Chinchilla model outperforms non-compute optimal models such as GPT-3 on a large range of downstream evaluation tasks Model Size vs Time Expect to see smaller models developed to achieve better results than larger models trained in a non-optimal way Pre-training for Domain Adaptation Highly specialized domains: Law, Medical, Finance, Science BloombergGPT: Domain Adaptation for Finance Scaling Laws Reading: Domain-Specific Training: BloombergGPT A large Decoder-only model Pre-trained on finance data Used Chinchilla Scaling Laws to guide the number of parameters in the model and volume of training data (tokens) Pre-training Large Language Models #Stage 1 of Generative AI Project Lifecycle - Select #Once you have scoped out your use case, and determined how you\u0026rsquo;ll need the LLM to work within your application, your next step is to select a model to work with. Your first choice will be to either work with an existing model, or train your own from scratch. There are specific circumstances where training your own model from scratch might be advantageous, and you\u0026rsquo;ll learn about those later in this lesson. In general, however, you\u0026rsquo;ll begin the process of developing your application using an existing foundation model Model Cards #Describe important details including the best use cases for each model, how it was trained, and known limitations The exact model that you\u0026rsquo;d choose will depend on the details of the task you need to carry out Variance of the transformer model architecture are suited to different language tasks, largely because of differences in how the models are trained Model Architectures and Pre-training Objectives #Take a high-level look at the initial training process for LLMs This phase is often referred to as pre-training. As you saw in Lesson 1, LLMs encode a deep statistical representation of language. This understanding is developed during the model’s pre-training phase when the model learns from vast amounts of unstructured textual data. This can be gigabytes, terabytes, and even petabytes of text. This data is pulled from many sources, including scrapes off the Internet and corpora of texts that have been assembled specifically for training language models. In this self-supervised learning step, the model internalizes the patterns and structures present in the language. These patterns then enable the model to complete its training objective, which depends on the architecture of the model, as you\u0026rsquo;ll see shortly. During pre-training, the model weights get updated to minimize the loss of the training objective. The encoder generates an embedding or vector representation for each token. Pre-training also requires a large amount of compute and the use of GPUs. Note, when you scrape training data from public sites such as the Internet, you often need to process the data to increase quality, address bias, and remove other harmful content. As a result of this data quality curation, often only 1-3% of tokens are used for pre-training. You should consider this when you estimate how much data you need to collect if you decide to pre-train your own model Transformer Variants #Encoder Only Models Encoder Decoder Models Decoder Models Autoencoding Models #Encoder-only models are also known as Autoencoding models, and they are pre-trained using masked language modeling. Here, tokens in the input sequence or randomly mask, and the training objective is to predict the mask tokens in order to reconstruct the original sentence. This is also called a denoising objective. Autoencoding models build bi-directional representations of the input sequence, meaning that the model has an understanding of the full context of a token and not just of the words that come before. Encoder-only models are ideally suited to task that benefit from this bi-directional contexts. You can use them to carry out sentence classification tasks, for example, sentiment analysis or token-level tasks like named entity recognition or word classification. Some well-known examples of an autoencoder model are BERT and RoBERTa Autoregressive Models #let\u0026rsquo;s take a look at decoder-only or autoregressive models, which are pre-trained using causal language modeling. Here, the training objective is to predict the next token based on the previous sequence of tokens. Predicting the next token is sometimes called full language modeling by researchers. Decoder-based autoregressive models, mask the input sequence and can only see the input tokens leading up to the token in question. The model has no knowledge of the end of the sentence. The model then iterates over the input sequence one by one to predict the following token. In contrast to the encoder architecture, this means that the context is unidirectional. By learning to predict the next token from a vast number of examples, the model builds up a statistical representation of language. Models of this type make use of the decoder component off the original architecture without the encoder. Decoder-only models are often used for text generation, although larger decoder-only models show strong zero-shot inference abilities, and can often perform a range of tasks well. Well known examples of decoder-based autoregressive models are GPT and BLOOM Sequence-to-Sequence Models #The final variation of the transformer model is the sequence-to-sequence model that uses both the encoder and decoder parts off the original transformer architecture. The exact details of the pre-training objective vary from model to model. A popular sequence-to-sequence model T5, pre-trains the encoder using span corruption, which masks random sequences of input tokens. Those mass sequences are then replaced with a unique Sentinel token, shown here as x. Sentinel tokens are special tokens added to the vocabulary, but do not correspond to any actual word from the input text. The decoder is then tasked with reconstructing the mask token sequences auto-regressively. The output is the Sentinel token followed by the predicted tokens. You can use sequence-to-sequence models for translation, summarization, and question-answering. They are generally useful in cases where you have a body of texts as both input and output. Besides T5, which you\u0026rsquo;ll use in the labs in this course, another well-known encoder-decoder model is BART Model Summary #To summarize, here\u0026rsquo;s a quick comparison of the different model architectures and the targets off the pre-training objectives. Autoencoding models are pre-trained using masked language modeling. They correspond to the encoder part of the original transformer architecture, and are often used with sentence classification or token classification. Autoregressive models are pre-trained using causal language modeling. Models of this type make use of the decoder component of the original transformer architecture, and often used for text generation. Sequence-to-sequence models use both the encoder and decoder part off the original transformer architecture. The exact details of the pre-training objective vary from model to model. The T5 model is pre-trained using span corruption. Sequence-to-sequence models are often used for translation, summarization, and question-answering. The Significance of Scale: Task Ability #One additional thing to keep in mind is that larger models of any architecture are typically more capable of carrying out their tasks well. Researchers have found that the larger a model, the more likely it is to work as you needed to without additional in-context learning or further training This observed trend of increased model capability with size has driven the development of larger and larger models in recent years. This growth has been fueled by inflection points and research, such as the introduction of the highly scalable transformer architecture, access to massive amounts of data for training, and the development of more powerful compute resources This steady increase in model size actually led some researchers to hypothesize the existence of a new Moore\u0026rsquo;s law for LLMs. Like them, you may be asking, Can we just keep adding parameters to increase performance and make models smarter? Where could this model growth lead? While this may sound great, it turns out that training these enormous models is difficult and very expensive, so much so that it may be infeasible to continuously train larger and larger models. Computational Challenges of Training LLMs #OutOfMemoryError: CUDA out of memory. One of the most common issues you still counter when you try to train large language models is running out of memory. If you\u0026rsquo;ve ever tried training or even just loading your model on NVIDIA GPUs, this error message might look familiar. CUDA, short for Compute Unified Device Architecture, is a collection of libraries and tools developed for Nvidia GPUs. Libraries such as PyTorch and TensorFlow use CUDA to boost performance on matrix multiplication and other operations common to deep learning. You\u0026rsquo;ll encounter these out-of-memory issues because most LLMs are huge, and require a ton of memory to store and train all of their parameters. Approximate GPU RAM Needed to Store 1B Parameters #A single parameter is typically represented by a 32-bit float, which is a way computers represent real numbers. You\u0026rsquo;ll see more details about how numbers gets stored in this format shortly. A 32-bit float takes up four bytes of memory. So to store one billion parameters you\u0026rsquo;ll need four bytes times one billion parameters, or four gigabyte of GPU RAM at 32-bit full precision. This is a lot of memory, and note, if only accounted for the memory to store the model weights so far. Additional GPU RAM Needed to Train 1 B Parameters #If you want to train the model, you\u0026rsquo;ll have to plan for additional components that use GPU memory during training. These include two Adam optimizer states, gradients, activations, and temporary variables needed by your functions. This can easily lead to 20 extra bytes of memory per model parameter. In fact, to account for all of these overhead during training, you\u0026rsquo;ll actually require approximately 20 times the amount of GPU RAM that the model weights alone take up. To train a one billion parameter model at 32-bit full precision, you\u0026rsquo;ll need approximately 80 gigabyte of GPU RAM. This is definitely too large for consumer hardware, and even challenging for hardware used in data centers, if you want to train with a single processor. Eighty gigabyte is the memory capacity of a single NVIDIA A100 GPU, a common processor used for machine learning tasks in the Cloud. What options do you have to reduce the memory required for training?
One technique that you can use to reduce the memory is called Quantization. Quantization #The main idea here is that you reduce the memory required to store the weights of your model by reducing their precision from 32-bit floating point numbers to 16-bit floating point numbers, or eight-bit integer numbers. The corresponding data types used in deep learning frameworks and libraries are FP32 for 32-bit full position, FP16, or BFLOAT16 for 16-bit half precision, and INT8 eight-bit integers. The range of numbers you can represent with FP32 goes from approximately 3x10^-38 to 3x10^38. By default, model weights, activations, and other model parameters are stored in FP32. Quantization statistically projects the original 32-bit floating point numbers into a lower precision space, using scaling factors calculated based on the range of the original 32-bit floating point numbers. Quantization: PI in FP32 #Suppose you want to store a PI to six decimal places in different positions. Floating point numbers are stored as a series of bits zeros and ones. The 32 bits to store numbers in full precision with FP32 consist of one bit for the sign where zero indicates a positive number, and one a negative number. Then eight bits for the exponent of the number, and 23 bits representing the fraction of the number. The fraction is also referred to as the mantissa, or significand. It represents the precision bits off the number. If you convert the 32-bit floating point value back to a decimal value, you notice the slight loss in precision. For reference, here\u0026rsquo;s the real value of PI to 19 decimal places. Quantization: FP16 #Now, let\u0026rsquo;s see what happens if you project this FP32 representation of PI into the FP16, 16-bit lower precision space. The 16 bits consists of one bit for the sign, as you saw for FP32, but now FP16 only assigns five bits to represent the exponent and 10 bits to represent the fraction. Therefore, the range of numbers you can represent with FP16 is vastly smaller from negative 65,504 to positive 65,504. The original FP32 value gets projected to 3.140625 in the 16-bit space. Notice that you lose some precision with this projection. There are only six places after the decimal point now. You\u0026rsquo;ll find that this loss in precision is acceptable in most cases because you\u0026rsquo;re trying to optimize for memory footprint. Storing a value in FP32 requires four bytes of memory. In contrast, storing a value on FP16 requires only two bytes of memory, so with quantization you have reduced the memory requirement by half. Quantization: BFLOAT16 #The AI research community has explored ways to optimize 16-bit quantization. One datatype in particular BFLOAT16, has recently become a popular alternative to FP16. BFLOAT16, short for Brain Floating Point Format developed at Google Brain has become a popular choice in deep learning. Many LLMs, including FLAN-T5, have been pre-trained with BFLOAT16. BFLOAT16 or BF16 is a hybrid between half precision FP16 and full precision FP32. BF16 significantly helps with training stability and is supported by newer GPU\u0026rsquo;s such as NVIDIA\u0026rsquo;s A100. BFLOAT16 is often described as a truncated 32-bit float, as it captures the full dynamic range of the full 32-bit float, that uses only 16-bits. BFLOAT16 uses the full eight bits to represent the exponent, but truncates the fraction to just seven bits. This not only saves memory, but also increases model performance by speeding up calculations. The downside is that BF16 is not well suited for integer calculations, but these are relatively rare in deep learning. Quantization: INT8 #For completeness let\u0026rsquo;s have a look at what happens if you quantize PI from the 32-bit into the even lower precision eight bit space. If you use one bit for the sign INT8 values are represented by the remaining seven bits. This gives you a range to represent numbers from negative 128 to positive 127 and unsurprisingly PI gets projected two or three in the 8-bit lower precision space. This brings new memory requirement down from originally four bytes to just one byte, but obviously results in a pretty dramatic loss of precision. Quantization: Summary #Remember that the goal of quantization is to reduce the memory required to store and train models by reducing the precision off the model weights. Quantization statistically projects the original 32-bit floating point numbers into lower precision spaces using scaling factors calculated based on the range of the original 32-bit floats. Modern deep learning frameworks and libraries support Quantization-Aware Training (QAT), which learns the quantization scaling factors during the training process. The details of this process are beyond the scope of this course. But you\u0026rsquo;ve seen the key point here, that you can use quantization to reduce the memory footprint off the model during training. BFLOAT16 has become a popular choice of precision in deep learning as it maintains the dynamic range of FP32, but reduces the memory footprint by half. Many LLMs, including FLAN-T5, have been pre-trained with BFOLAT16. Visualization: Quantization #Let\u0026rsquo;s return to the challenge of fitting models into GPU memory and take a look at the impact quantization can have By applying quantization, you can reduce your memory consumption required to store the model parameters down to only two gigabyte using 16-bit half precision of 50% saving and you could further reduce the memory footprint by another 50% by representing the model parameters as eight bit integers, which requires only one gigabyte of GPU RAM. Note that in all these cases you still have a model with one billion parameters. As you can see, the circles representing the models are the same size Quantization will give you the same degree of savings when it comes to training. As you heard earlier, you\u0026rsquo;ll quickly hit the limit of a single NVIDIA A100 GPU with 80 gigabytes of memory, when you try to train a one billion parameter model at 32-bit full precision. You\u0026rsquo;ll need to consider using either 16-bit or eight bit quantization if you want to train on a single GPU GPU RAM Needed to Train Larger Models #Remember, many models now have sizes in excess of 50 billion or even 100 billion parameters, meaning you\u0026rsquo;d need up to 500 times more memory capacity to train them, tens of thousands of gigabytes. These enormous models dwarf the one billion parameter model we\u0026rsquo;ve been considering, shown here to scale on the left As models scale beyond a few billion parameters, it becomes impossible to train them on a single GPU. Instead, you\u0026rsquo;ll need to turn to distributed computing techniques while you train your model across multiple GPUs. This could require access to hundreds of GPUs, which is very expensive, another reason why you won\u0026rsquo;t pre-train your own model from scratch most of the time However, an additional training process called Fine-tuning also require storing all training parameters in memory and it\u0026rsquo;s very likely you\u0026rsquo;ll want to fine tune a model at some point. Optional: Efficient Multi-GPU Compute Strategies #Need to scale your model training efforts beyond a single GPU. Need to use multi GPU compute strategies when your model becomes too big to fit in a single GPU But even if your model does fit onto a single GPU, there are benefits to using multiple GPUs to speed up your training It’s useful to know how to distribute compute across GPUs even when you\u0026rsquo;re working with a small model Let\u0026rsquo;s discuss how you can carry out this scaling across multiple GPUs in an efficient way.
Distributed Data Parallel (DDP) #Model still fits on a single GPU
The first step in scaling model training is to distribute large data-sets across multiple GPUs and process these batches of data in parallel. A popular implementation of this model replication technique is PyTorch’s Distributed Data Parallel (DDP) DDP copies your model onto each GPU and sends batches of data to each of the GPUs in parallel. Each data-set is processed in parallel and then a synchronization step combines the results of each GPU, which in turn updates the model on each GPU, which is always identical across chips. This implementation allows parallel computations across all GPUs that results in faster training. Note: DDP requires that your model weights and all of the additional parameters, gradients, and optimizer states that are needed for training, fit onto a single GPU. If your model is too big for this, you should look into another technique called Model Sharding. Fully Sharded Data Parallel (FSDP) #Model no longer fits on a single GPU
A popular implementation of modal sharding is PyTorch’s Fully Sharded Data Parallel (FSDP) FSDP is motivated by a paper published by researchers at Microsoft in 2019 that proposed a technique called ZeRO. ZeRO stands for Zero Redundancy Optimizer and the goal of ZeRO is to optimize memory by distributing or sharding model states across GPUs with ZeRO data overlap. This allows you to scale model training across GPUs when your model doesn\u0026rsquo;t fit in the memory of a single chip. Let\u0026rsquo;s take a quick look at how ZeRO works before coming back to FSDP.
Looked at all of the memory components required for training LLMs, the largest memory requirement was for the optimizer states, which take up twice as much space as the weights, followed by weights themselves and the gradients. Memory Usage in DDP #Let\u0026rsquo;s represent the parameters as this blue box, the gradients and yellow and the optimizer states in green. One limitation off the model replication strategy that I showed before is that you need to keep a full model copy on each GPU, which leads to redundant memory consumption. You are storing the same numbers on every GPU. Zero Redundancy Optimizer #ZeRO, on the other hand, eliminates this redundancy by distributing also referred to as sharding the model parameters, gradients, and optimizer states across GPUs instead of replicating them. At the same time, the communication overhead for a sinking model states stays close to that of the previously discussed DDP. ZeRO offers three optimization stages. ZeRO Stage 1, shots only optimizer states across GPUs, this can reduce your memory footprint by up to a factor of four. ZeRO Stage 2 also shots the gradients across chips. When applied together with Stage 1, this can reduce your memory footprint by up to eight times. Finally, ZeRO Stage 3 shots all components including the model parameters across GPUs. When applied together with Stages 1 and 2, memory reduction is linear with a number of GPUs. For example, sharding across 64 GPUs could reduce your memory by a factor of 64. Let\u0026rsquo;s apply this concept to the visualization of DDP and replace the LLM by the memory representation of model parameters, gradients, and optimizer states.
Visualization: Distributed Data Parallel #Visualization: Fully Sharded Data Parallel #When you use FSDP, you distribute the data across multiple GPUs as you saw happening in DDP. But with FSDP, you also distributed or shard the model parameters, gradients, and optimize the states across the GPU nodes using one of the strategies specified in the ZeRO paper. With this strategy, you can now work with models that are too big to fit on a single chip. In contrast to GDP, where each GPU has all of the model states required for processing each batch of data available locally, FSDP requires you to collect this data from all of the GPUs before the forward and backward pass. Each CPU requests data from the other GPUs on-demand to materialize the sharded data into uncharted data for the duration of the operation. After the operation, you release the uncharted non-local data back to the other GPUs as original sharded data You can also choose to keep it for future operations during backward pass for example. Note, this requires more GPU RAM again, this is a typical performance versus memory trade-off decision. In the final step after the backward pass, FSDP is synchronizes the gradients across the GPUs in the same way they were for DDP. Sharding Factor #Model Sharding as described with FSDP allows you to reduce your overall GPU memory utilization. Optionally, you can specify that FSDP offloads part of the training computation to GPUs to further reduce your GPU memory utilization. To manage the trade-off between performance and memory utilization, you can configure the level of sharding using FSDP is sharding factor. A sharding factor of one basically removes the sharding and replicates the full model similar to DDP. If you set the sharding factor to the maximum number of available GPUs, you turn on full sharding. This has the most memory savings, but increases the communication volume between GPUs. Any sharding factor in-between enables hybrid sharding. Impact of Using FSDP #Let\u0026rsquo;s take a look at how FSDP performs in comparison to DDP measured in teraflops per GPU. These tests were performed using a maximum of 512 NVIDIA V100 GPUs, each with 80 gigabytes of memory. Note, one teraflop corresponds to one trillion floating-point operations per second. The first figure shows FSDP performance for different size T5 models. You can see the different performance numbers for FSDP, full sharding in blue, hybrid shard in orange and full replication in green. For reference, DDP performance is shown in red. For the first 25 models with 611 million parameters and 2.28 billion parameters, the performance of FSDP and DDP is similar. Now, if you choose a model size beyond 2.28 billion, such as T5 with 11.3 billion parameters, DDP runs into the out-of-memory error. FSDP on the other hand can easily handle models this size and achieve much higher teraflops when lowering the model\u0026rsquo;s precision to 16-bit. The second figure shows 7% decrease in per GPU teraflops when increasing the number of GPUs from 8-512 for the 11 billion T5 model, plotted here using a batch size of 16 and orange and a batch size of eight in blue. As the model grows in size and is distributed across more and more GPUs, the increase in communication volume between chips starts to impact the performance, slowing down the computation. In summary, this shows that you can use FSDP for both small and large models and seamlessly scale your model training across multiple GPUs. The most important thing is to have a sense of how the data model parameters and training computations are shared across processes when training LLMs. Given the expense and technical complexity of training models across GPUs, some researchers have been exploring ways to achieve better performance with smaller models. Question: Which of the following best describes the role of data parallelism in the context of training Large Language Models (LLMs) with GPUs?
Data parallelism allows for the use of multiple GPUs to process different parts of the same data simultaneously, speeding up training time.
Correct Data parallelism is a strategy that splits the training data across multiple GPUs. Each GPU processes a different subset of the data simultaneously, which can greatly speed up the overall training time.
Scaling Laws and Compute-Optimal Methods #Learn about research that has explored the relationship between
model size, training, configuration and performance in an effort to determine just how big models need to be
Scaling Choices for Pre-training #Remember, the goal during pre-training is to maximize the model\u0026rsquo;s performance of its learning objective, which is minimizing the loss when predicting tokens. Two options you have to achieve better performance are increasing the size of the dataset you train your model on and increasing the number of parameters in your model. In theory, you could scale either of both of these quantities to improve performance. However, another issue to take into consideration is your compute budget which includes factors like the number of GPUs you have access to and the time you have available for training models Unit of Compute #To help you understand some of the discussion ahead, let\u0026rsquo;s first define a unit of compute that quantifies the required resources. A petaFLOP per second-day is a measurement of the number of floating point operations performed at a rate of one petaFLOP per second, running for an entire day. Note: one petaFLOP corresponds to one quadrillion floating point operations per second. When specifically thinking about training transformers, one petaFLOP per second day is approximately equivalent to eight NVIDIA V100 GPUs, operating at full efficiency for one full day. If you have a more powerful processor that can carry out more operations at once, then a petaFLOP per second day requires fewer chips. For example, two NVIDIA A100 GPUs give equivalent compute to the eight V100 chips. Number of PetaFLOP/s-days to Pre-train Various LLMs #To give you an idea off the scale of these compute budgets, this chart shows a comparison off the petaFLOP per second days required to pre-train different variants of BERT and RoBERTa, which are both encoder only models, T5 and encoder-decoder model and GPT-3, which is a decoder only model. The difference between the models in each family is the number of parameters that were trained, ranging from a few hundred million for BERT base to 175 billion for the largest GPT-3 variant. Note that the y-axis is logarithmic. Each increment vertically is a power of 10. Here we see that T5 XL with three billion parameters required close to 100 petaFLOP per second days while the larger GPT-3 175 billion parameter model required approximately 3,700 petaFLOP per second days. This chart makes it clear that a huge amount of computers required to train the largest models. You can see that bigger models take more compute resources to train and generally also require more data to achieve good performance. It turns out that they are actually well-defined relationships between these three scaling choices. Compute Budget vs Model Performance #Researchers have explored the trade-offs between training dataset size, model size and compute budget. Here\u0026rsquo;s a figure from a paper by researchers at OpenAI that explores the impact of compute budget on model performance. The y-axis is the test loss, which you can consider as a proxy for model performance where smaller values are better. The x-axis is the compute budget in units of petaFLOP per second days. As you just saw, larger numbers can be achieved by either using more compute power or training for longer or both. Each thin blue line here shows the model loss over a single training run. Looking at where the loss starts to decline more slowly for each run, reveals a clear relationship between the compute budget and the model\u0026rsquo;s performance. This can be approximated by a power-law relationship, shown by this pink line. A power law is a mathematical relationship between two variables, where one is proportional to the other raised to some power. When plotted on a graph where both axes are logarithmic, power-law relationships appear as straight lines. The relationship here holds as long as model size and training dataset size don\u0026rsquo;t inhibit the training process. Taken at face value, this would suggest that you can just increase your compute budget to achieve better model performance. Dataset Size and Model Size vs Performance #In practice however, the compute resources you have available for training will generally be a hard constraint set by factors such as the hardware you have access to, the time available for training and the financial budget of the project. If you hold your compute budget fixed, the two levers you have to improve your model\u0026rsquo;s performance are the size of the training dataset and the number of parameters in your model The OpenAI researchers found that these two quantities also show a power-law relationship with a test loss in the case where the other two variables are held fixed This is another figure from the paper exploring the impact of training dataset size on model performance. Here, the compute budget and model size are held fixed and the size of the training dataset is vary. The graph shows that as the volume of training data increases, the performance of the model continues to improve. In the second graph, the compute budget and training dataset size are held constant. Models of varying numbers of parameters are trained. As the model increases in size, the test loss decreases indicating better performance. At this point you might be asking, what\u0026rsquo;s the ideal balance between these three quantities? Well, it turns out a lot of people are interested in this question. Both research and industry communities have published a lot of empirical data for pre-training compute optimal models Chinchilla Paper: Training Compute-Optimal Large Language Models #In a paper published in 2022, a group of researchers led by Jordan Hoffmann, Sebastian Borgeaud and Arthur Mensch carried out a detailed study of the performance of language models of various sizes and quantities of training data. The goal was to find the optimal number of parameters and volume of training data for a given compute budget. The authors name the resulting compute optimal model, Chinchilla. This paper is often referred to as the Chinchilla paper. Compute Optimal Models #The Chinchilla paper hints that many of the 100 billion parameter large language models like GPT-3 may actually be over-parameterized, meaning they have more parameters than they need to achieve a good understanding of language and under-trained so that they would benefit from seeing more training data. The authors hypothesized that smaller models may be able to achieve the same performance as much larger ones if they are trained on larger datasets Chinchilla Scaling Laws for Model and Dataset Size #In this table, you can see a selection of models along with their size and information about the dataset they were trained on. One important takeaway from the Chinchilla paper is that the optimal training dataset size for a given model is about 20 times larger than the number of parameters in the model. Chinchilla was determined to be compute optimal. For a 70 billion parameter model, the ideal training dataset contains 1.4 trillion tokens or 20 times the number of parameters. The last three models in the table were trained on datasets that are smaller than the Chinchilla optimal size. These models may actually be under trained. In contrast, LLaMA was trained on a dataset size of 1.4 trillion tokens, which is close to the Chinchilla recommended number. Another important result from the paper is that the compute optimal Chinchilla model outperforms non-compute optimal models such as GPT-3 on a large range of downstream evaluation tasks Model Size vs Time #With the results of the Chinchilla paper in hand teams have recently started to develop smaller models that achieved similar, if not better results than larger models that were trained in a non-optimal way. Moving forward, you can probably expect to see a deviation from the bigger is always better trends of the last few years as more teams or developers like you start to optimize their model design. The last model shown on this slide, BloombergGPT, is a really interesting model. It was trained in a compute optimal way following the Chinchilla loss and so achieves good performance with the size of 50 billion parameters It\u0026rsquo;s also an interesting example of a situation where pre-training a model from scratch was necessary to achieve good task performance.
Pre-training for Domain Adaptation #If your target domain uses vocabulary and language structures that are not commonly used in day to day language, you may need to perform domain adaptation to achieve good model performance. Legal Language #For example, imagine you\u0026rsquo;re a developer building an app to help lawyers and paralegals summarize legal briefs. Legal writing makes use of very specific terms like mens rea in the first example and res judicata in the second. These words are rarely used outside of the legal world, which means that they are unlikely to have appeared widely in the training text of existing LLMs. As a result, the models may have difficulty understanding these terms or using them correctly. Another issue is that legal language sometimes uses everyday words in a different context, like consideration in the third example. Which has nothing to do with being nice, but instead refers to the main element of a contract that makes the agreement enforceable. Medical Language #For similar reasons, you may face challenges if you try to use an existing LLM in a medical application. Medical language contains many uncommon words to describe medical conditions and procedures. And these may not appear frequently in training datasets consisting of web scrapes and book texts. Some domains also use language in a highly idiosyncratic way. This last example of medical language may just look like a string of random characters, but it\u0026rsquo;s actually a shorthand used by doctors to write prescriptions. This text has a very clear meaning to a pharmacist, take one tablet by mouth four times a day, after meals and at bedtime Because models learn their vocabulary and understanding of language through the original pretraining task. Pretraining your model from scratch will result in better models for highly specialized domains like law, medicine, finance or science BloombergGPT: Domain Adapatation for Finance #First announced in 2023 in a paper by Shijie Wu, Steven Lu, and colleagues at Bloomberg BloombergGPT is an example of a large language model that has been pretrained for a specific domain, in this case, finance The Bloomberg researchers chose to combine both finance data and general purpose tax data to pretrain a model that achieves best in class results on financial benchmarks, while also maintaining competitive performance on general purpose LLM benchmarks. As such, the researchers chose data consisting of 51% financial data and 49% public data. In their paper, the Bloomberg researchers describe the model architecture in more detail. They also discuss how they started with a Chinchilla Scaling Laws for guidance and where they had to make tradeoffs Scaling Laws #These two graphs compare a number of LLMs, including BloombergGPT, to scaling laws that have been discussed by researchers. On the left, the diagonal lines trace the optimal model size in billions of parameters for a range of compute budgets. On the right, the lines trace the compute optimal training data set size measured in number of tokens. The dashed pink line on each graph indicates the compute budget that the Bloomberg team had available for training their new model. The pink shaded regions correspond to the compute optimal scaling loss determined in the Chinchilla paper. In terms of model size, you can see that BloombergGPT roughly follows the Chinchilla approach for the given compute budget of 1.3 million GPU hours, or roughly 230,000,000 petaflops. The model is only a little bit above the pink shaded region, suggesting the number of parameters is fairly close to optimal. However, the actual number of tokens used to pretrain BloombergGPT 569,000,000,000 is below the recommended Chinchilla value for the available compute budget. The smaller than optimal training data set is due to the limited availability of financial domain data showing that real world constraints may force you to make trade offs when pretraining your own models Key Takeways #Walked you through some of the common use cases for LLMs, such as essay writing, dialogue summarization and translation. Gave a detailed presentation of the transformer architecture that powers these models. Discussed some of the parameters you can use at inference time to influence the model\u0026rsquo;s output. Introduced to Generative AI Project Lifecycle that you can use to plan and guide your application development work. Saw how models are trained on vast amounts of text data during an initial training phase called pretraining. This is where models develop their understanding of language. Explored some of the computational challenges of training these models, which are significant. In practice, because of GPU memory limitations, you will almost always use some form of quantization when training your models. Finished with a discussion of scaling laws that have been discovered for LLMs and how they can be used to design compute optimal models. Question: Which of the following statements about pretraining scaling laws are correct? Select all that apply:
Which of the following statements about pretraining scaling laws are correct? Select all that apply:
To scale our model, we need to jointly increase dataset size and model size, or they can become a bottleneck for each other.
Correct
For instance, while increasing dataset size is helpful, if we do not jointly improve the model size, it might not be able to capture value from the larger dataset.
There is a relationship between model size (in number of parameters) and the optimal number of tokens to train the model with.
Correct
This relationship is describe in the Chinchilla paper, that shows that many models might even be overparametrized according to the relationship they found.
When measuring compute budget, we can use \u0026ldquo;PetaFlops per second-Day\u0026rdquo; as a metric.
Correct
Petaflops per second-day is a useful measure for computing budget as it reflects the both hardware and time required to train the model.
Reading: Domain-Specific Training: BloombergGPT #BloombergGPT - https://arxiv.org/abs/2303.17564 a large Decoder-only language model underwent pre-training using an extensive financial dataset to increase its understanding of finance and enabling it to generate finance-related natural language text. Dataset comprises of news articles reports market data During the training of BloombergGPT, the authors used the Chinchilla Scaling Laws to guide the number of parameters in the model and the volume of training data, measured in tokens. The recommendations of Chinchilla are represented by the lines Chinchilla-1, Chinchilla-2 and Chinchilla-3 in the image, and we can see that BloombergGPT is close to it. While the recommended configuration for the team’s available training compute budget was 50 billion parameters and 1.4 trillion tokens, acquiring 1.4 trillion tokens of training data in the finance domain proved challenging. Consequently, they constructed a dataset containing just 700 billion tokens, less than the compute-optimal value. Furthermore, due to early stopping, the training process terminated after processing 569 billion tokens. The BloombergGPT project is a good illustration of pre-training a model for increased domain-specificity, and the challenges that may force trade-offs against compute-optimal model and training configurations. `}),e.add({id:15,href:"/notes/nlp/generative-ai-with-llms/week-1-research-papers/",title:"Week 1 - Research Papers",description:"Week 1 - Research Papers",content:`These notes were developed using lectures/material/transcripts from the DeepLearning.AI \u0026amp; AWS - Generative AI with Large Language Models course
Transformer Architecture #Attention is All You Need - This paper introduced the Transformer architecture, with the core “self-attention” mechanism. This article was the foundation for LLMs. BLOOM: BigScience 176B Model - BLOOM is a open-source LLM with 176B parameters (similar to GPT-3) trained in an open and transparent way. In this paper, the authors present a detailed discussion of the dataset and process used to train the model. You can also see a high-level overview of the model here. Vector Space Models - Series of lessons from DeepLearning.AI\u0026rsquo;s Natural Language Processing specialization discussing the basics of vector space models and their use in language modeling. Pre-training and Scaling Laws #Scaling Laws for Neural Language Models - empirical study by researchers at OpenAI exploring the scaling laws for large language models. Model Architectures and Pre-training Objectives #What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? - The paper examines modeling choices in large pre-trained language models and identifies the optimal approach for zero-shot generalization. HuggingFace Tasks and Model Hub - Collection of resources to tackle varying machine learning tasks using the HuggingFace library. LLaMA: Open and Efficient Foundation Language Models - Article from Meta AI proposing Efficient LLMs (their model with 13B parameters outperform GPT3 with 175B parameters on most benchmarks) Scaling Laws and Compute-Optimal Models #Language Models are Few-Shot Learners - This paper investigates the potential of few-shot learning in Large Language Models. Training Compute-Optimal Large Language Models Study from DeepMind to evaluate the optimal model size and number of tokens for training LLMs. Also known as “Chinchilla Paper”. BloombergGPT: A Large Language Model for Finance - LLM trained specifically for the finance domain, a good example that tried to follow chinchilla laws. `}),e.add({id:16,href:"/notes/nlp/generative-ai-with-llms/week-2-part-1/",title:"Week 2 Part 1 - Fine-tuning LLMs with Instruction",description:"Week 2 Part 1 - Fine-tuning LLMs with Instruction",content:`These notes were developed using lectures/material/transcripts from the DeepLearning.AI \u0026amp; AWS - Generative AI with Large Language Models course
Notes #Instruction Fine-Tuning
Stage 2 of Generative AI Project Lifecycle - Adapt and Align Model Prompt Engineering Fine-tuning Align with Human Feedback Evaluate Limitation of In-Context Learning (ICL) Does not work for smaller models Examples take up valuable space in the context window Pre-training Recap Train LLM using vast amount (GB, TB, PB) on unstructured textual data via self-supervised learning Fine-tuning - supervised learning process where you use a dataset (GB, TB) of labeled examples (prompt completion pairs) Full Fine-tuning - updates all of the model’s weights Requires enough memory and compute budget to store and process all the gradients, optimizers and other components that are being updated during training Instruction Fine-tuning - trains the model using examples that demonstrate how it should respond to a specific instruction Data Preparation - prepare instruction dataset Prompt template libraries Classification Text Generation Text Summarization LLM Fine-tuning Process Divide into training, validation and test splits Validation accuracy - Measure LLM performance using the validation dataset Test accuracy - Final performance evaluation using the holdout test dataset Select prompts from your training data set and pass them to the LLM, which then generates completions Compare the distribution of the completion and that of the training label and use the standard Cross-Entropy function to calculate loss between the two token distributions Use the calculated loss to update the model weights using Backpropagation This is repeated for many batches of prompt completion pairs and over several epochs, update the weights so that the model’s performance on the task improves Results in a new version of the base model called an instruct model Fine-tuning on a Single Task
Training on 5-1k labeled examples can result in good performance Downside: the process may lead to Catastrophic Forgetting How to Avoid Catastrophic Forgetting? Fine-tune on multiple tasks at the same time Consider Parameter Efficient Fine-tuning (PEFT) Multi-task Instruction Fine-tuning
An extension of single task fine-tuning, where the training dataset is comprised of example inputs and outputs for multiple tasks: Summarization, Review Rating, Code Translation, Entity Recognition Avoids the issue of Catastrophic Forgetting Drawback: may need 50-100k examples in your training set FLAN (Fine-tuned LAnguage Net) is a specific set of instructions used to fine-tune different models (last step of the training process) FLAN-T5 - Fine-tuned version of pre-trained T5 model FLAN-PALM - Fine-tuned version of pre-trained PALM model Improving Summarization Capabilities Reading: Scaling Instruct Models
Introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model Model Evaluation
With LLMs, the output is non-deterministic therefore much more challenging to evaluate ROUGE (The Recall-Oriented Understudy for Gisting Evaluation) - use for diagnostic evaluation of summarization tasks Compares a summary to one or more reference summaries BLEU (Bilingual Evaluation Understudy) - use for diagnostic evaluation of translation tasks Compares to human-generated translations Benchmarks - evaluating its performance on data that it hasn\u0026rsquo;t seen before
GLUE - General Language Understanding Evaluation SuperGLUE Benchmarks for Massive Models
MMLU - Massive Multi-task Language Understanding BIG-bench - Beyond the Imitation Game Benchmark HELM - Holistic Evaluation of Language Models Also include metrics for: Accuracy Calibration Robustness Fairness Bias Toxicity Efficiency Introduction - Week 2
Take a look at instruction fine-tuning, so when you have your base model, the thing that\u0026rsquo;s initially pretrained, it\u0026rsquo;s encoded a lot of really good information, usually about the world. So it knows about things, but it doesn\u0026rsquo;t necessarily know how to be able to respond to our prompts, our questions. So when we instruct it to do a certain task, it doesn\u0026rsquo;t necessarily know how to respond. And so instruction fine-tuning helps it to be able to change its behavior to be more helpful for us Because by learning off general text off the Internet and other sources, you learn to predict the next word. By predicting what\u0026rsquo;s the next word on the Internet is not the same as following instructions. I thought it\u0026rsquo;s amazing you can take a large language model, train it on hundreds of billions of words off the Internet. And then fine-tune it with a much smaller data set on following instructions and just learn to do that have to watch out for, of course, is catastrophic forgetting and this is something that we talk about in the course. So that\u0026rsquo;s where you train the model on some extra data in this insane instruct fine-tuning. And then it forgets all of that stuff that it had before, or a big chunk of that data that it had before. And so there are some techniques that we\u0026rsquo;ll talk about in the course to help combat that. Such as doing instruct fine-tuning across a really broad range of different instruction types. So it\u0026rsquo;s not just a case of just tuning it on just the thing you want it to do. You might have to be a little bit broader than that as well, but we talk about it in the course One of the problems with fine-tuning is you take a giant model and you fine-tune every single parameter in that model. You have this big thing to store around and deploy, and it\u0026rsquo;s actually very compute and memory expansive talk about parameter efficient fine-tuning or PEFT for short, as a set of methods that can allow you to mitigate some of those concerns, right? So we have a lot of customers that do want to be able to tune for very specific tasks, very specific domains. And parameter efficient fine-tuning is a great way to still achieve similar performance results on a lot of tasks that you can with full fine-tuning. But then actually take advantage of techniques that allow you to freeze those original model weights. Or add adaptive layers on top of that with a much smaller memory footprint, So that you can train for multiple tasks one of the techniques that I know you\u0026rsquo;ve used a lot is LoRA see a lot of excitement demand around LoRA because of the performance results of using those low rank matrices as opposed to full fine-tuning, right? So you\u0026rsquo;re able to get really good performance results with minimal compute and memory requirements many developers will often start off with prompting, and sometimes that gives you good enough performance and that\u0026rsquo;s great. And sometimes prompting hits a ceiling in performance and then this type of fine-tuning with LoRA or other PEFT technique is really critical for unlocking that extra level performance. And then the other thing I\u0026rsquo;m seeing among a lot of OM developers is a discussion debate about the cost of using a giant model, which is a lot of benefits versus for your application fine-tuning a smaller model full fine tuning can be cost prohibitive, right? To say the least so the ability to actually be able to use techniques like PEFT to put fine-tuning generative AI models kind of in the hands of everyday users. That do have those cost constraints and they\u0026rsquo;re cost conscious, which is pretty much everyone in the real world Instruction Fine-Tuning #Stage 2 of Generative AI Project Lifecycle - Adapt and Align Model #You\u0026rsquo;ll learn about methods that you can use to improve the performance of an existing model for your specific use case. You\u0026rsquo;ll also learn about important metrics that can be used to evaluate the performance of your finetuned LLM and quantify its improvement over the base model you started with Limitations of In-Context Learning #Limitations of in-context learning First, for smaller models, it doesn\u0026rsquo;t always work, even when five or six examples are included. Second, any examples you include in your prompt take up valuable space in the context window, reducing the amount of room you have to include other useful information. Luckily, another solution exists, you can take advantage of a process known as fine-tuning to further train a base model Pre-training Recap #You train the LLM using vast amounts of unstructured textual data via self- supervised learning Fine-tuning at a High Level #Fine-tuning is a supervised learning process where you use a data set of labeled examples to update the weights of the LLM. The labeled examples are prompt completion pairs, the fine-tuning process extends the training of the model to improve its ability to generate good completions for a specific task One strategy, known as instruction fine-tuning, is particularly good at improving a model\u0026rsquo;s performance on a variety of tasks Instruction Fine-tuning #Instruction fine-tuning trains the model using examples that demonstrate how it should respond to a specific instruction Here are a couple of example prompts to demonstrate this idea. The instruction in both examples is classify this review, and the desired completion is a text string that starts with sentiment followed by either positive or negative. The dataset you use for training includes many pairs of prompt completion examples for the task you\u0026rsquo;re interested in, each of which includes an instruction. For example, if you want to fine-tune your model to improve its summarization ability, you\u0026rsquo;d build up a data set of examples that begin with the instruction “summarize the following text” or a similar phrase. And if you are improving the model\u0026rsquo;s translation skills, your examples would include instructions like “translate this sentence”. These prompt completion examples allow the model to learn to generate responses that follow the given instructions Instruction fine-tuning, where all of the model\u0026rsquo;s weights are updated is known as Full Fine-tuning. The process results in a new version of the model with updated weights. It is important to note that just like pre-training, full fine-tuning requires enough memory and compute budget to store and process all the gradients, optimizers and other components that are being updated during training. So you can benefit from the memory optimization and parallel computing strategies that you learned about last week How do you actually go about Instruction Fine-tuning an LLM?
Dataset Preparation: Instruction Fine-tuning #There are many publicly available datasets that have been used to train earlier generations of language models, although most of them are not formatted as instructions.
Luckily, developers have assembled prompt template libraries that can be used to take existing datasets, for example, the large data set of Amazon product reviews and turn them into instruction prompt datasets for fine-tuning.
Here are three prompts that are designed to work with the Amazon reviews dataset and that can be used to fine tune models for
classification, text generation and text summarization tasks You can see that in each case you pass the original review, here called review_body, to the template, where it gets inserted into the text that starts with an instruction like predict the associated rating, generate a star review, or give a short sentence describing the following product review.
The result is a prompt that now contains both an instruction and the example from the data set
LLM Fine-tuning Process #Once you have your instruction data set ready, as with standard supervised learning, you divide the data set into training, validation and test splits During fine-tuning, you select prompts from your training data set and pass them to the LLM, which then generates completions. Next, you compare the LLM completion with the response specified in the training data. You can see here that the model didn\u0026rsquo;t do a great job, it classified the review as neutral, which is a bit of an understatement. The review is clearly very positive. Remember that the output of an LLM is a probability distribution across tokens. So you can compare the distribution of the completion and that of the training label and use the standard Cross-Entropy function to calculate loss between the two token distributions. And then use the calculated loss to update your model weights in standard Backpropagation. You\u0026rsquo;ll do this for many batches of prompt completion pairs and over several epochs, update the weights so that the model\u0026rsquo;s performance on the task improves As in standard supervised learning, you can define separate evaluation steps to measure your LLM performance using the holdout validation data set. This will give you the validation accuracy After you\u0026rsquo;ve completed your fine-tuning, you can perform a final performance evaluation using the holdout test data set. This will give you the test accuracy. The fine-tuning process results in a new version of the base model, often called an instruct model that is better at the tasks you are interested in Fine-tuning with instruction prompts is the most common way to fine-tune LLMs these days. From this point on, when you hear or see the term fine-tuning, you can assume that it always means Instruction Fine-tuning Fine-Tuning on a Single Task #LLMs have become famous for their ability to perform many different language tasks within a single model, your application may only need to perform a single task You can fine-tune a pre-trained model to improve performance on only the task that is of interest to you For example, summarization using a dataset of examples for that task. Interestingly, good results can be achieved with relatively few examples. Often just 500-1,000 examples can result in good performance in contrast to the billions of pieces of texts that the model saw during pre-training. However, there is a potential downside to fine-tuning on a single task. The process may lead to a phenomenon called Catastrophic Forgetting. Catastrophic Forgetting #Catastrophic forgetting happens because the full fine-tuning process modifies the weights of the original LLM. While this leads to great performance on the single fine-tuning task, it can degrade performance on other tasks. For example, while fine-tuning can improve the ability of a model to perform sentiment analysis on a review and result in a quality completion, the model may forget how to do other tasks. This model knew how to carry out named entity recognition before fine-tuning correctly identifying Charlie as the name of the cat in the sentence. But after fine-tuning, the model can no longer carry out this task, confusing both the entity it is supposed to identify and exhibiting behavior related to the new task How to Avoid Catastrophic Forgetting #First of all, it\u0026rsquo;s important to decide whether catastrophic forgetting actually impacts your use case. If all you need is reliable performance on the single task you fine-tuned on, it may not be an issue that the model can\u0026rsquo;t generalize to other tasks. If you do want or need the model to maintain its multitask generalized capabilities, you can perform fine-tuning on multiple tasks at one time. Good multitask fine-tuning may require 50-100,000 examples across many tasks, and so will require more data and compute to train. Will discuss this option in more detail shortly. Our second option is to perform parameter efficient fine-tuning, or PEFT for short instead of full fine-tuning. PEFT is a set of techniques that preserves the weights of the original LLM and trains only a small number of task-specific adapter layers and parameters. PEFT shows greater robustness to catastrophic forgetting since most of the pre-trained weights are left unchanged. PEFT is an exciting and active area of research that we will cover later this week Question:
Which of the following are true in respect to Catastrophic Forgetting? Select all that apply.
Catastrophic forgetting occurs when a machine learning model forgets previously learned information as it learns new information.
Correct
The assertion is true, and this process is especially problematic in sequential learning scenarios where the model is trained on multiple tasks over time.
One way to mitigate catastrophic forgetting is by using regularization techniques to limit the amount of change that can be made to the weights of the model during training.
Correct
One way to mitigate catastrophic forgetting is by using regularization techniques to limit the amount of change that can be made to the weights of the model during training. This can help to preserve the information learned during earlier training phases and prevent overfitting to the new data.
Catastrophic forgetting is a common problem in machine learning, especially in deep learning models.
Correct
This assertion is true because these models typically have many parameters, which can lead to overfitting and make it more difficult to retain previously learned information.
Multi-Task Instruction Fine-Tuning #Multitask fine-tuning is an extension of single task fine-tuning, where the training dataset is comprised of example inputs and outputs for multiple tasks. Here, the dataset contains examples that instruct the model to carry out a variety of tasks, including summarization, review rating, code translation, and entity recognition. You train the model on this mixed dataset so that it can improve the performance of the model on all the tasks simultaneously, thus avoiding the issue of catastrophic forgetting. Over many epochs of training, the calculated losses across examples are used to update the weights of the model, resulting in an instruction tuned model that is learned how to be good at many different tasks simultaneously. One drawback to multitask fine-tuning is that it requires a lot of data. You may need as many as 50-100,000 examples in your training set. However, it can be really worthwhile and worth the effort to assemble this data. The resulting models are often very capable and suitable for use in situations where good performance at many tasks is desirable Let\u0026rsquo;s take a look at one family of models that have been trained using multitask instruction fine-tuning
Instruction Fine-tuning with FLAN #Instruct model variance differ based on the datasets and tasks used during fine-tuning. One example is the FLAN family of models. FLAN (Fine-tuned LAnguage Net) is a specific set of instructions used to fine-tune different models. Because they\u0026rsquo;re FLAN fine-tuning is the last step of the training process the authors of the original paper called it “the metaphorical dessert to the main course of pre-training”. FLAN-T5, the FLAN instruct version of the T5 foundation model while FLAN-PALM is the FLAN instruct version of the palm foundation model FLAN-T5 #FLAN-T5 is a great general purpose instruct model. In total, it\u0026rsquo;s been fine tuned on 473 datasets across 146 task categories. Those datasets are chosen from other models and papers as shown here One example of a prompt dataset used for summarization tasks in FLAN-T5 is SAMSum. It\u0026rsquo;s part of the muffin collection of tasks and datasets and is used to train language models to summarize dialogue SAMSum: Dialogue Dataset #SAMSum is a dataset with 16,000 messenger like conversations with summaries. Three examples are shown here with the dialogue on the left and the summaries on the right. The dialogues and summaries were crafted by linguists for the express purpose of generating a high-quality training dataset for language models. The linguists were asked to create conversations similar to those that they would write on a daily basis, reflecting their proportion of topics of their real life messenger conversations. Although language experts then created short summaries of those conversations that included important pieces of information and names of the people in the dialogue Sample FLAN-T5 Prompt Templates #Here is a prompt template designed to work with this SAMSum dialogue summary dataset. The template is actually comprised of several different instructions that all basically ask the model to do this same thing. Summarize a dialogue. For example, briefly summarize that dialogue. What is a summary of this dialogue? What was going on in that conversation? Including different ways of saying the same instruction helps the model generalize and perform better. Just like the prompt templates you saw earlier, you see that in each case, the dialogue from the SAMSum dataset is inserted into the template wherever the dialogue field appears. The summary is used as the label. After applying this template to each row in the SAMSum dataset, you can use it to fine tune a dialogue summarization task While FLAN-T5 is a great general use model that shows good capability in many tasks. You may still find that it has room for improvement on tasks for your specific use case
Improving FLAN-T5’s Summarization Capabilities #For example, imagine you\u0026rsquo;re a data scientist building an app to support your customer service team, process requests received through a chat bot, like the one shown here. Your customer service team needs a summary of every dialogue to identify the key actions that the customer is requesting and to determine what actions should be taken in response The SAMSum dataset gives FLAN-T5 some abilities to summarize conversations. However, the examples in the dataset are mostly conversations between friends about day-to-day activities and don\u0026rsquo;t overlap much with the language structure observed in customer service chats. You can perform additional fine-tuning of the FLAN-T5 model using a dialogue dataset that is much closer to the conversations that happened with your bot. dialogsum Dataset #This is the exact scenario that you\u0026rsquo;ll explore in the lab this week You\u0026rsquo;ll make use of an additional domain specific summarization dataset called dialogsum to improve FLAN-T5\u0026rsquo;s is ability to summarize support chat conversations. This dataset consists of over 13,000 support chat dialogues and summaries. The dialogsum dataset is not part of the FLAN-T5 training data, so the model has not seen these conversations before Let\u0026rsquo;s take a look at example from dialogsum and discuss how a further round of fine-tuning can improve the model. This is a support chat that is typical of the examples in the dialogsum dataset. The conversation is between a customer and a staff member at a hotel check-in desk. The chat has had a template applied so that the instruction to summarize the conversation is included at the start of the text Now, let\u0026rsquo;s take a look at how FLAN-T5 responds to this prompt before doing any additional fine-tuning
Summary Before Fine-tuning #Note that the prompt is now condensed on the left to give you more room to examine the completion of the model. Here is the model\u0026rsquo;s response to the instruction. You can see that the model does as it\u0026rsquo;s able to identify that the conversation was about a reservation for Tommy. However, it does not do as well as the human-generated baseline summary, which includes important information such as Mike asking for information to facilitate check-in and the models completion has also invented information that was not included in the original conversation, specifically the name of the hotel and the city it was located in Summary After Fine-tuning #Hopefully, you will agree that this is closer to the human-produced summary. There is no fabricated information and the summary includes all of the important details, including the names of both people participating in the conversation This example, use the public dialogsum dataset to demonstrate fine-tuning on custom data
Fine-tuning with Your Own Data #In practice, you\u0026rsquo;ll get the most out of fine-tuning by using your company\u0026rsquo;s own internal data. For example, the support chat conversations from your customer support application. This will help the model learn the specifics of how your company likes to summarize conversations and what is most useful to your customer service colleagues Question: What is the purpose of fine-tuning with prompt datasets?
To improve the performance and adaptability of a pre-trained language model for specific tasks.
Correct
This option accurately describes the purpose of fine-tuning with prompt datasets. It aims to improve the performance and adaptability of a pre-trained language model by training it on specific tasks using instruction prompts.
Reading: Scaling Instruct Models #FLAN - Fine-tuned LAnguage Net
This paper introduces FLAN (Fine-tuned LAnguage Net), an instruction finetuning method, and presents the results of its application. The study demonstrates that by fine-tuning the 540B PaLM model on 1836 tasks while incorporating Chain-of-Thought Reasoning data, FLAN achieves improvements in generalization, human usability, and zero-shot reasoning over the base model. The paper also provides detailed information on how each of these aspects was evaluated.
Here is the image from the lecture slides that illustrates the fine-tuning tasks and datasets employed in training FLAN. The task selection expands on previous works by incorporating dialogue and program synthesis tasks from Muffin and integrating them with new Chain of Thought Reasoning tasks. It also includes subsets of other task collections, such as T0 and Natural Instructions v2. Some tasks were held-out during training, and they were later used to evaluate the model\u0026rsquo;s performance on unseen tasks.
Model Evaluation #How can you formalize the improvement in performance of your fine-tuned model over the pre-trained model you started with? Let\u0026rsquo;s explore several metrics that are used by developers of large language models that you can use to assess the performance of your own models and compare to other models out in the world LLM Evaluation Challenges #In traditional machine learning, you can assess how well a model is doing by looking at its performance on training and validation data sets where the output is already known. You\u0026rsquo;re able to calculate simple metrics such as accuracy, which states the fraction of all predictions that are correct because the models are deterministic But with large language models where the output is non-deterministic and language-based evaluation is much more challenging First Pair of Sentences
Take, for example, the sentence, Mike really loves drinking tea. This is quite similar to Mike adores sipping tea. But how do you measure the similarity? Second Pair of Sentences
Let\u0026rsquo;s look at these other two sentences. Mike does not drink coffee, and Mike does drink coffee. There is only one word difference between these two sentences. However, the meaning is completely different. Now, for humans like us with squishy organic brains, we can see the similarities and differences. But when you train a model on millions of sentences, you need an automated, structured way to make measurements LLM Evaluation Metrics #ROUGE and BLEU, are two widely used evaluation metrics for different tasks. ROUGE (The Recall-Oriented Understudy for Gisting Evaluation) is primarily employed to assess the quality of automatically generated summaries by comparing them to human-generated reference summaries. BLEU (Bilingual Evaluation Understudy) is an algorithm designed to evaluate the quality of machine-translated text, again, by comparing it to human-generated translations. Terminology Review #In the anatomy of language, a unigram is equivalent to a single word. A bigram is two words and n-gram is a group of n-words ROUGE Score #ROUGE-1 #To do so, let\u0026rsquo;s look at a human-generated reference sentence. Reference (human): It is cold outside Generated output: It is very cold outside You can perform simple metric calculations similar to other machine learning tasks using recall, precision, and F1. The recall metric measures the number of words or unigrams that are matched between the reference and the generated output divided by the number of words or unigrams in the reference. Gets a perfect score of one as all the generated words match words in the reference. Precision measures the unigram matches divided by the output size. The F1 score is the harmonic mean of both of these values. These are very basic metrics that only focused on individual words, hence the one in the name, and don\u0026rsquo;t consider the ordering of the words. It can be deceptive. It\u0026rsquo;s easily possible to generate sentences that score well but would be subjectively poor Stop for a moment and imagine that the sentence generated by the model was different by just one word. Generated output: It is not cold outside. The scores would be the same ROUGE-2 #You can get a slightly better score by taking into account bigrams or collections of two words at a time from the reference and generated sentence. By working with pairs of words you\u0026rsquo;re acknowledging in a very simple way, the ordering of the words in the sentence. By using bigrams, you\u0026rsquo;re able to calculate a ROUGE-2. Now, you can calculate the recall, precision, and F1 score using bigram matches instead of individual words. You\u0026rsquo;ll notice that the scores are lower than the ROUGE-1 scores. With longer sentences, they\u0026rsquo;re a greater chance that bigrams don\u0026rsquo;t match, and the scores may be even lower ROUGE-L #Rather than continue on with ROUGE numbers growing bigger to n-grams of three or fours, let\u0026rsquo;s take a different approach. Instead, you\u0026rsquo;ll look for the Longest Common Subsequence (LCS) present in both the generated output and the reference output. In this case, the longest matching sub-sequences are, it is and cold outside, each with a length of two. You can now use the LCS value to calculate the recall precision and F1 score, where the numerator in both the recall and precision calculations is the length of the longest common subsequence, in this case, two. Collectively, these three quantities are known as the ROUGE-L score. As with all of the ROUGE scores, you need to take the values in context. You can only use the scores to compare the capabilities of models if the scores were determined for the same task. For example, summarization. ROUGE scores for different tasks are not comparable to one another ROUGE Hacking #As you\u0026rsquo;ve seen, a particular problem with simple ROUGE scores is that it\u0026rsquo;s possible for a bad completion to result in a good score. Generated output: cold, cold, cold, cold. As this generated output contains one of the words from the reference sentence, it will score quite highly, even though the same word is repeated multiple times. The ROUGE-1 precision score will be perfect ROUGE Clipping #One way you can counter this issue is by using a clipping function to limit the number of unigram matches to the maximum count for that unigram within the reference. In this case, there is one appearance of cold and the reference and so a modified precision with a clip on the unigram matches results in a dramatically reduced score. However, you\u0026rsquo;ll still be challenged if their generated words are all present, but just in a different order. For example, with this generated sentence, outside cold it is. This sentence was called perfectly even on the modified precision with the clipping function as all of the words and the generated output are present in the reference. Whilst using a different ROUGE score can help experimenting with a n-gram size that will calculate the most useful score will be dependent on the sentence, the sentence size, and your use case Note: many language model libraries, for example, Hugging Face, include implementations of ROUGE score that you can use to easily evaluate the output of your model.
BLEU Score #BLEU (bilingual evaluation understudy) score is useful for evaluating the quality of machine-translated text The score itself is calculated using the average precision over multiple n-gram sizes, just like the ROUGE-1 score that we looked at before, but calculated for a range of n-gram sizes and then averaged. The BLEU score quantifies the quality of a translation by checking how many n-grams in the machine-generated translation match those in the reference translation. To calculate the score, you average precision across a range of different n-gram sizes. If you were to calculate this by hand, you would carry out multiple calculations and then average all of the results to find the BLEU score. For this example, let\u0026rsquo;s take a look at a longer sentence so that you can get a better sense of the scores value. The reference human-provided sentence is, I am very happy to say that I am drinking a warm cup of tea. Now, as you\u0026rsquo;ve seen these individual calculations in depth when you looked at ROUGE, I will show you the results of BLEU using a standard library. Calculating the BLEU score is easy with pre-written libraries from providers like Hugging Face and I\u0026rsquo;ve done just that for each of our candidate sentences. The first candidate is, I am very happy that I am drinking a cup of tea. The BLEU score is 0.495. As we get closer and closer to the original sentence, we get a score that is closer and closer to one Summary: Evaluations Metrics #Both ROUGE and BLEU are quite simple metrics and are relatively low-cost to calculate. You can use them for simple reference as you iterate over your models, but you shouldn\u0026rsquo;t use them alone to report the final evaluation of a large language model. Use ROUGE for diagnostic evaluation of summarization tasks and BLEU for translation tasks For overall evaluation of your model\u0026rsquo;s performance, however, you will need to look at one of the evaluation benchmarks that have been developed by researchers
Benchmarks #LLMs are complex, and simple evaluation metrics like the ROUGE and BLUE scores, can only tell you so much about the capabilities of your model. In order to measure and compare LLMs more holistically, you can make use of pre-existing datasets, and associated benchmarks that have been established by LLM researchers specifically for this purpose. Selecting the right evaluation dataset is vital, so that you can accurately assess an LLM\u0026rsquo;s performance, and understand its true capabilities. You\u0026rsquo;ll find it useful to select datasets that isolate specific model skills, like reasoning or common sense knowledge, and those that focus on potential risks, such as disinformation or copyright infringement. An important issue that you should consider is whether the model has seen your evaluation data during training. You\u0026rsquo;ll get a more accurate and useful sense of the model\u0026rsquo;s capabilities by evaluating its performance on data that it hasn\u0026rsquo;t seen before Benchmarks, such as GLUE, SuperGLUE, or HELM, cover a wide range of tasks and scenarios They do this by designing or collecting datasets that test specific aspects of an LLM GLUE #GLUE, or General Language Understanding Evaluation, was introduced in 2018. GLUE is a collection of natural language tasks, such as sentiment analysis and question-answering. GLUE was created to encourage the development of models that can generalize across multiple tasks, and you can use the benchmark to measure and compare the model performance SuperGLUE #As a successor to GLUE, SuperGLUE was introduced in 2019, to address limitations in its predecessor. It consists of a series of tasks, some of which are not included in GLUE, and some of which are more challenging versions of the same tasks. SuperGLUE includes tasks such as multi-sentence reasoning, and reading comprehension GLUE and SuperGLUE Leaderboards #Both the GLUE and SuperGLUE benchmarks have leaderboards that can be used to compare and contrast evaluated models. The results page is another great resource for tracking the progress of LLMs Benchmarks for Massive Models #As models get larger, their performance against benchmarks such as SuperGLUE start to match human ability on specific tasks. That\u0026rsquo;s to say that models are able to perform as well as humans on the benchmarks tests, but subjectively we can see that they\u0026rsquo;re not performing at human level at tasks in general. There is essentially an arms race between the emergent properties of LLMs, and the benchmarks that aim to measure them MMLU and BIG-bench
Here are a couple of recent benchmarks that are pushing LLMs further. Massive Multitask Language Understanding, or MMLU, is designed specifically for modern LLMs. To perform well models must possess extensive world knowledge and problem-solving ability. Models are tested on elementary mathematics, US history, computer science, law, and more. In other words, tasks that extend way beyond basic language understanding. BIG-bench currently consists of 204 tasks, ranging through linguistics, childhood development, math, common sense reasoning, biology, physics, social bias, software development and more. BIG-bench comes in three different sizes, and part of the reason for this is to keep costs achievable, as running these large benchmarks can incur large inference costs HELM #A final benchmark you should know about is the Holistic Evaluation of Language Models, or HELM. The HELM framework aims to improve the transparency of models, and to offer guidance on which models perform well for specific tasks. HELM takes a multi-metric approach, measuring seven metrics across 16 core scenarios, ensuring that trade-offs between models and metrics are clearly exposed. One important feature of HELM is that it assesses on metrics beyond basic accuracy measures, like precision of the F1 score. The benchmark also includes metrics for fairness, bias, and toxicity, which are becoming increasingly important to assess as LLMs become more capable of human-like language generation, and in turn of exhibiting potentially harmful behavior. HELM is a living benchmark that aims to continuously evolve with the addition of new scenarios, metrics, and models You can take a look at the results page to browse the LLMs that have been evaluated, and review scores that are pertinent to your project\u0026rsquo;s needs. `}),e.add({id:17,href:"/notes/nlp/generative-ai-with-llms/week-2-part-2/",title:"Week 2 Part 2 - Parameter Efficient Fine-tuning",description:"Week 2 Part 2 - Parameter Efficient Fine-tuning",content:`These notes were developed using lectures/material/transcripts from the DeepLearning.AI \u0026amp; AWS - Generative AI with Large Language Models course
Notes #Full Fine-tuning - every model weight is updated during supervised learning Requires memory not just to store the model, but various other parameters that are required during the training process Weights Optimizer States Gradients Forward Activations Temporary Memory used Can lead to Catastrophic Forgetting Results in a new version of the model for every task you train on, thus can create an expensive storage problem if you’re fine-tuning for multiple tasks Parameter Efficient Fine-tuning (PEFT) - only update a small subset of the model parameters Freezes most (if not all) of the original model weights and focus on fine-tuning a subset of (existing or additional) model parameters The number of trained parameters is much smaller than the number of parameters in the original LLM In some cases, just 15-20% of the original LLM weights More robust against Catastrophic Forgetting PEFT Saves Space and is Flexible Train only a small number of weights, which results in a much smaller footprint overall (MB, GB) New parameters are combined with the original LLM weights for inference The PEFT weights are trained for each task and can be easily swapped out for inference, allowing efficient adaptation of the original model to multiple tasks PEFT Trade-offs Parameter Efficiency Training Speed Inference Costs Model Performance Memory Efficiency Three Main Classes of PEFT Methods Selective - fine-tune only a subset of the original LLM parameters Reparameterization - work with the original LLM parameters, but reduce the number of parameters to train by creating new low rank transformations of the original network weights LoRA Additive - keep all of the LLM weights frozen and introduce new trainable components Adapters Soft Prompts - Prompt Tuning PEFT Techniques 1: LoRA (Low Rank Adaptation of LLMs) - a strategy that reduces the number of parameters to be trained during fine-tuning by freezing all of the original model parameters and then injecting a pair of rank decomposition matrices alongside the original weights Training Freeze most of the original LLM weights Inject 2 Rank Decomposition Matrices (Rank r is small simension, typically 4, 8, …, 64) Train the weights of the smaller matrices Inference Matrix multiple the low rank matrices Add to original weights Latency - little to no impact as the model has the same number of parameters as the original Apply LoRA to the Self-Attention layers since most of the parameters of LLMs are in these layers Though it can also be applied to other layers as well like FFN layers Can often perform LoRA with a single GPU and avoid the need for a distributed cluster of GPUs Train on Different Tasks Train different rank decomposition matrices for different tasks Update weights before inference The memory required to store these LoRA matrices is very small, therefore can use LoRA to train for many tasks Choosing the LoRA Rank (r) The smaller the rank, the smaller the number of trainable parameters, the bigger the savings on compute Ranks in the range of 4-32 can provide you with a good trade-off between reducing trainable parameters and preserving performance Can combine with Quantization techniques to further reduce memory footprint: QLoRA PEFT Techniques 2: Soft Prompts Trainable tokens are added to the prompt and the model weights are left frozen Lab 2 - Fine-tune a Generative AI Model For Dialogue Summarization Model: flan-t5-base Task: Dialogue Summarization Metrics: ROUGE Comparison of model performance Original model Fully Fine-tuned model PEFT model Parameter Efficient Fine-Tuning #Training LLMs is computationally intensive Full fine-tuning requires memory not just to store the model, but various other parameters that are required during the training process. Even if your computer can hold the model weights, which are now on the order of hundreds of gigabytes for the largest models, you must also be able to allocate memory for optimizer states, gradients, forward activations, and temporary memory throughout the training process. These additional components can be many times larger than the model and can quickly become too large to handle on consumer hardware In contrast to full fine-tuning where every model weight is updated during supervised learning, Parameter Efficient Fine-tuning (PEFT) methods only update a small subset of parameters. Some path techniques freeze most of the model weights and focus on fine-tuning a subset of existing model parameters, for example, particular layers or components Other techniques don\u0026rsquo;t touch the original model weights at all, and instead add a small number of new parameters or layers and fine-tune only the new components. With PEFT, most if not all of the LLM weights are kept frozen. As a result, the number of trained parameters is much smaller than the number of parameters in the original LLM. In some cases, just 15-20% of the original LLM weights. This makes the memory requirements for training much more manageable. In fact, PEFT can often be performed on a single GPU. And because the original LLM is only slightly modified or left unchanged, PEFT is less prone to the Catastrophic Forgetting problems of full fine-tuning Full Fine-tuning Creates Full Copy of Original LLM Per Task #Full fine-tuning results in a new version of the model for every task you train on. Each of these is the same size as the original model, so it can create an expensive storage problem if you\u0026rsquo;re fine-tuning for multiple tasks PEFT Saves Space and is Flexible #With parameter efficient fine-tuning, you train only a small number of weights, which results in a much smaller footprint overall, as small as megabytes depending on the task. The new parameters are combined with the original LLM weights for inference. The PEFT weights are trained for each task and can be easily swapped out for inference, allowing efficient adaptation of the original model to multiple tasks PEFT Trade-offs #There are several methods you can use for parameter efficient fine-tuning, each with trade-offs on parameter efficiency, memory efficiency, training speed, model quality, and inference costs PEFT Methods #Let\u0026rsquo;s take a look at the three main classes of PEFT methods. Selective methods are those that fine-tune only a subset of the original LLM parameters. There are several approaches that you can take to identify which parameters you want to update. You have the option to train only certain components of the model or specific layers, or even individual parameter types. Researchers have found that the performance of these methods is mixed and there are significant trade-offs between parameter efficiency and compute efficiency. We won\u0026rsquo;t focus on them in this course. Reparameterization methods also work with the original LLM parameters, but reduce the number of parameters to train by creating new low rank transformations of the original network weights. A commonly used technique of this type is LoRA, which we\u0026rsquo;ll explore in detail in the next video. Lastly, Additive methods carry out fine-tuning by keeping all of the original LLM weights frozen and introducing new trainable components. Here there are two main approaches. Adapter methods add new trainable layers to the architecture of the model, typically inside the encoder or decoder components after the attention or feed-forward layers. Soft Prompt methods, on the other hand, keep the model architecture fixed and frozen, and focus on manipulating the input to achieve better performance. This can be done by adding trainable parameters to the prompt embeddings or keeping the input fixed and retraining the embedding weights. In this lesson, you\u0026rsquo;ll take a look at a specific soft prompts technique called prompt tuning Question:
\u0026ldquo;Parameter Efficient Fine-Tuning (PEFT) updates only a small subset of parameters. This helps prevent catastrophic forgetting.\u0026rdquo; True or False?
True
Correct Performing full-finetuning can lead to catastrophic forgetting because it changes all parameters on the model. Since PEFT only updates a small subset of parameters, it\u0026rsquo;s more robust against this catastrophic forgetting effect.
PEFT Techniques 1: LoRA #Low-rank Adaptation, or LoRA for short, is a parameter-efficient fine-tuning technique that falls into the Reparameterization category Transformers Recap #The input prompt is turned into tokens, which are then converted to embedding vectors and passed into the encoder and/or decoder parts of the transformer. In both of these components, there are two kinds of neural networks; self-attention and feedforward networks. The weights of these networks are learned during pre-training. After the embedding vectors are created, they\u0026rsquo;re fed into the self-attention layers where a series of weights are applied to calculate the attention scores During full fine-tuning, every parameter in these layers is updated LoRA #LoRA is a strategy that reduces the number of parameters to be trained during fine-tuning by freezing all of the original model parameters and then injecting a pair of rank decomposition matrices alongside the original weights. The dimensions of the smaller matrices are set so that their product is a matrix with the same dimensions as the weights they\u0026rsquo;re modifying. You then keep the original weights of the LLM frozen and train the smaller matrices using the same supervised learning process you saw earlier this week. For inference, The two low-rank matrices are multiplied together to create a matrix with the same dimensions as the frozen weights. You then add this to the original weights and replace them in the model with these updated values You now have a LoRA fine-tuned model that can carry out your specific task. Because this model has the same number of parameters as the original, there is little to no impact on inference latency. Researchers have found that applying LoRA to just the self-attention layers of the model is often enough to fine-tune for a task and achieve performance gains. However, in principle, you can also use LoRA on other components like the feed-forward layers. But since most of the parameters of LLMs are in the attention layers, you get the biggest savings in trainable parameters by applying LoRA to these weights matrices LoRA Example #Let\u0026rsquo;s look at a practical example using the transformer architecture described in the Attention is All You Need paper. The paper specifies that the transformer weights have dimensions of 512 by 64, this means that each weights matrix has 32,768 trainable parameters. If you use LoRA as a fine-tuning method with the rank equal to eight (r=8), you will instead train two small rank decomposition matrices whose small dimension is eight. This means that Matrix A will have dimensions of 8 by 64, resulting in 512 total parameters. A has dimensions r x k = 8 x 64 = 512 parameters Matrix B will have dimensions of 512 by 8, or 4,096 trainable parameters. B has dimensions d x r = 512 x 8 = 4096 parameters By updating the weights of these new low-rank matrices instead of the original weights, you\u0026rsquo;ll be training 4,608 parameters instead of 32,768 and 86% reduction Because LoRA allows you to significantly reduce the number of trainable parameters, you can often perform this method of parameter efficient fine tuning with a single GPU and avoid the need for a distributed cluster of GPUs Train on Different Tasks #Since the rank-decomposition matrices are small, you can fine-tune a different set for each task and then switch them out at inference time by updating the weights. Suppose you train a pair of LoRA matrices for a specific task; let\u0026rsquo;s call it Task A. To carry out inference on this task, you would multiply these matrices together and then add the resulting matrix to the original frozen weights. You then take this new summed weights matrix and replace the original weights where they appear in your model. You can then use this model to carry out inference on Task A. If instead, you want to carry out a different task, say Task B, you simply take the LoRA matrices you trained for this task, calculate their product, and then add this matrix to the original weights and update the model again. The memory required to store these LoRA matrices is very small. So in principle, you can use LoRA to train for many tasks. Switch out the weights when you need to use them, and avoid having to store multiple full-size versions of the LLM Performance of LoRA #Let\u0026rsquo;s use the ROUGE metric you learned about earlier this week to compare the performance of a LoRA fine-tune model to both an original base model and a full fine-tuned version Let\u0026rsquo;s focus on fine-tuning the FLAN-T5 for dialogue summarization, which you explored earlier in the week. Just to remind you, the FLAN-T5-base model has had an initial set of full fine-tuning carried out using a large instruction data set. First, let\u0026rsquo;s set a baseline score for the FLAN-T5 base model and the summarization data set we discussed earlier. Here are the ROUGE scores for the base model where a higher number indicates better performance. You should focus on the ROUGE 1 score for this discussion, but you could use any of these scores for comparison. As you can see, the scores are fairly low. Next, look at the scores for a model that has had additional full fine-tuning on dialogue summarization. Remember, although FLAN-T5 is a capable model, it can still benefit from additional fine-tuning on specific tasks With full fine-tuning, you update every way in the model during supervised learning. You can see that this results in a much higher ROUGE 1 score increasing over the base FLAN-T5 model by 0.19. The additional round of fine-tuning has greatly improved the performance of the model on the summarization task Now let\u0026rsquo;s take a look at the scores for the LoRA fine-tune model. You can see that this process also resulted in a big boost in performance. The ROUGE 1 score has increased from the baseline by 0.17. This is a little lower than full fine-tuning, but not much. However, using LoRA for fine-tuning trained a much smaller number of parameters than full fine-tuning using significantly less compute, so this small trade-off in performance may well be worth it Choosing the LoRA Rank #Still an active area of research. In principle, the smaller the rank, the smaller the number of trainable parameters, and the bigger the savings on compute. In the paper that first proposed LoRA, researchers at Microsoft explored how different choices of rank impacted the model performance on language generation tasks. The table shows the rank of the LoRA matrices in the first column, the final loss value of the model, and the scores for different metrics, including BLEU and ROUGE. The bold values indicate the best scores that were achieved for each metric. The authors found a plateau in the loss value for ranks greater than 16. In other words, using larger LoRA matrices didn\u0026rsquo;t improve performance. The takeaway here is that ranks in the range of 4-32 can provide you with a good trade-off between reducing trainable parameters and preserving performance. Optimizing the choice of rank is an ongoing area of research and best practices may evolve as more practitioners like you make use of LoRA. LoRA is a powerful fine-tuning method that achieves great performance.
The principles behind the method are useful not just for training LLMs, but for models in other domains.
The final path method that you\u0026rsquo;ll explore this week doesn\u0026rsquo;t change the LLM at all and instead focuses on training your input text.
PEFT Techniques 2: Soft Prompts #With LoRA, the goal was to find an efficient way to update the weights of the model without having to train every single parameter again. There are also additive methods within PEFT that aim to improve model performance without changing the weights at all A second parameter efficient fine-tuning method called Prompt Tuning Prompt Tuning is Not Prompt Engineering #With prompt engineering, you work on the language of your prompt to get the completion you want. This could be as simple as trying different words or phrases or more complex, like including examples for one or few-shot Inference. The goal is to help the model understand the nature of the task you\u0026rsquo;re asking it to carry out and to generate a better completion. However, there are some limitations to prompt engineering, as it can require a lot of manual effort to write and try different prompts. You\u0026rsquo;re also limited by the length of the context window, and at the end of the day, you may still not achieve the performance you need for your task Prompt Tuning #With prompt tuning, you add additional trainable tokens to your prompt and leave it up to the supervised learning process to determine their optimal values. The set of trainable tokens is called a soft prompt, and it gets prepended to embedding vectors that represent your input text. The soft prompt vectors have the same length as the embedding vectors of the language tokens. And including somewhere between 20 and 100 virtual tokens can be sufficient for good performance Soft Prompts #The tokens that represent natural language are hard in the sense that they each correspond to a fixed location in the embedding vector space. However, the soft prompts are not fixed discrete words of natural language Instead, you can think of them as virtual tokens that can take on any value within the continuous multidimensional embedding space. And through supervised learning, the model learns the values for these virtual tokens that maximize performance for a given task Full Fine-tuning vs Prompt Tuning #In Full Fine-tuning, the training data set consists of input prompts and output completions or labels. The weights of the large language model are updated during supervised learning In contrast, with Prompt Tuning, the weights of the large language model are frozen and the underlying model does not get updated. Instead, the embedding vectors of the soft prompt gets updated over time to optimize the model\u0026rsquo;s completion of the prompt Prompt tuning is a very parameter efficient strategy because only a few parameters are being trained, in contrast with the millions to billions of parameters in full fine-tuning Prompt Tuning for Multiple Tasks #Similar to what you saw with LoRA, you can train a different set of soft prompts for each task and then easily swap them out at inference time. You can train a set of soft prompts for one task and a different set for another To use them for inference, you prepend your input prompt with the learned tokens to switch to another task, you simply change the soft prompt. Soft prompts are very small on disk, so this kind of fine tuning is extremely efficient and flexible. You\u0026rsquo;ll notice the same LLM is used for all tasks, all you have to do is switch out the soft prompts at inference time Performance of Prompt Tuning #In the original paper exploring the method by Brian Lester and collaborators at Google, the authors compared prompt tuning to several other methods for a range of model sizes. In this figure from the paper, you can see the Model size on the X axis and the SuperGLUE score on the Y axis. This is the evaluation benchmark you learned about earlier this week that grades model performance on a number of different language tasks. The red line shows the scores for models that were created through full fine tuning on a single task. While the orange line shows the score for models created using multitask fine tuning. The green line shows the performance of prompt tuning and finally, the blue line shows scores for prompt engineering only. As you can see, prompt tuning doesn\u0026rsquo;t perform as well as full fine tuning for smaller LLMs. However, as the model size increases, so does the performance of prompt tuning. And once models have around 10 billion parameters, prompt tuning can be as effective as full fine-tuning and offers a significant boost in performance over prompt engineering alone Interpretability of Soft Prompts #One potential issue to consider is the interpretability of learned virtual tokens. Remember, because the soft prompt tokens can take any value within the continuous embedding vector space. The trained tokens don\u0026rsquo;t correspond to any known token, word, or phrase in the vocabulary of the LLM However, an analysis of the nearest neighbor tokens to the soft prompt location shows that they form tight semantic clusters. In other words, the words closest to the soft prompt tokens have similar meanings. The words identified usually have some meaning related to the task, suggesting that the prompts are learning word like representations Summary: PEFT Methods #You explored two PEFT methods in this lesson LoRA, which uses rank decomposition matrices to update the model parameters in an efficient way. And Prompt Tuning, where trainable tokens are added to your prompt and the model weights are left untouched. Both methods enable you to fine tune models with the potential for improved performance on your tasks while using much less compute than full fine-tuning methods LoRA is broadly used in practice because of the comparable performance to full fine tuning for many tasks and data sets Key Takeaways #Walked through how to adapt a foundation model through a process called Instruction Fine-tuning. Along the way, you saw some of the prompt templates and data sets that were used to train the FLAN-T5 model. You also saw how to use evaluation metrics and benchmarks such as ROUGE and HELM to measure success during model finetuning. In practice instruction finetuning has proven very effective and useful across a wide range of natural language use cases and tasks. With just a few hundred examples, you can fine tune a model to your specific task, which is truly amazing. Next, you saw how parameter efficient fine-tuning, or PEFT, can reduce the amount of compute required to finetune a model. You learned about two methods you can use for this LoRA and Prompt Tuning. By the way you can also combine LoRA with the quantization techniques you learned about in week 1 to further reduce your memory footprint. This is known as QLoRA In practice, PEFT is used heavily to minimize compute and memory resources. And ultimately reducing the cost of fine-tuning, allowing you to make the most of your compute budget and speed up your development process
Question: Parameter Efficient Fine-Tuning (PEFT) methods specifically attempt to address some of the challenges of performing full fine-training. Which of the following options describe challenges that PEFT tries to overcome?
Computational constraints
Correct
Because most parameters are frozen, we typically only need to train 15%-20% of the original LLM weights, making the training process less expensive (less memory required)
Storage requirements
Correct
With PEFT, we can change just a small amount of parameters when fine-tuning, so during inference you can combine the original model with the new parameters, instead of duplicating the entire model for each new task you want to perform fine-tuning.
Catastrophic forgetting
Correct
With PEFT, most parameters of the LLM are unchanged, and that helps making it less prone to catastrophic forgetting.
Lab 2 Walkthrough #Try out fine-tuning using PEFT with LoRA for yourself by improving the summarization ability of the Flan-T5 model (flan-t5-base) hands-on with full fine-tuning and Parameter-Efficient Fine-Tuning, also called PEFT with prompt instructions. You will tune the Flan-T5 model further with your own specific prompts for your specific summarization task Lab 1, we were doing the zero-shot inference, the in-context learning. Now we are actually going to modify the weights of our language model, specific to our summarization task and specific to our dataset Compare original model and instruction-fine-tuned model PEFT makes such a big difference, especially when you\u0026rsquo;re constrained by how much compute resources that you have, you can lower the footprint both memory, disk, GPU, CPU, all of the resources can be reduced just by introducing PEFT into your fine-tuning process In the lessons you learned about LoRA, you learned about the rank. Here we\u0026rsquo;re going to choose rank of 32, which is actually relatively high. But we are just starting with that. Here it\u0026rsquo;s the SEQ_2_SEQ_LM, this is FLAN-T5. With just a few extra lines of code here to configure our LoRA fine-tuning. Then here we see we\u0026rsquo;re only going to train 1.4 percent of the trainable model parameters. In a lot of cases you can fine-tune very large models on a single GPU called the PEFT adapters or LoRA adopters, get merged or combined with the original LLM have to take the original LLM and then merge in this LoRA PEFT adapter can actually set the is_trainable flag to false. By setting the is_trainable flag to false, we are telling PyTorch that we\u0026rsquo;re not interested in training this model. All we\u0026rsquo;re interested in doing is the forward pass just to get the summaries. This is significant because we can tell PyTorch to not load any of the update portions of these operators and to basically minimize the footprint needed to just perform the inference with this model. This is a pretty neat flag. This was actually just introduced recently into the PEFT model at the time of this lab. I wanted to show it here because this is a pattern that you want to try to find when you\u0026rsquo;re doing your own modeling. When you know that you\u0026rsquo;re ready to deploy the model for inference, there are usually ways that you can hint to the framework, such as PyTorch that you\u0026rsquo;re not going to be training. This can then further reduce the resources needed to make these predictions PEFT - use much less resources during fine-tuning, than we would have if we did the full instruction. You can imagine this is only just a few thousand samples, but you can imagine at scale how this really can save you tons of compute resources and time by using PEFT by looking at the larger data set Lab 2 - Fine-tune a Generative AI Model For Dialogue Summarization #You will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the FLAN-T5 (flan-t5-base) model, which provides a high quality instruction-tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform PEFT fine-tuning, evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics `}),e.add({id:18,href:"/notes/nlp/generative-ai-with-llms/week-2-research-papers/",title:"Week 2 - Research Papers",description:"Week 2 - Research Papers",content:`These notes were developed using lectures/material/transcripts from the DeepLearning.AI \u0026amp; AWS - Generative AI with Large Language Models course
Multi-task, Instruction Fine-tuning #Scaling Instruction-Finetuned Language Models - Scaling fine-tuning with a focus on task, model size and chain-of-thought data. Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning - This blog (and article) explores instruction fine-tuning, which aims to make language models better at performing NLP tasks with zero-shot inference. Model Evaluation Metrics #HELM - Holistic Evaluation of Language Models - HELM is a living benchmark to evaluate Language Models more transparently. General Language Understanding Evaluation (GLUE) benchmark - This paper introduces GLUE, a benchmark for evaluating models on diverse natural language understanding (NLU) tasks and emphasizing the importance of improved general NLU systems. SuperGLUE - This paper introduces SuperGLUE, a benchmark designed to evaluate the performance of various NLP models on a range of challenging language understanding tasks. ROUGE: A Package for Automatic Evaluation of Summaries - This paper introduces and evaluates four different measures (ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S) in the ROUGE summarization evaluation package, which assess the quality of summaries by comparing them to ideal human-generated summaries. Measuring Massive Multitask Language Understanding (MMLU) - This paper presents a new test to measure multitask accuracy in text models, highlighting the need for substantial improvements in achieving expert-level accuracy and addressing lopsided performance and low accuracy on socially important subjects. BigBench-Hard - Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models - The paper introduces BIG-bench, a benchmark for evaluating language models on challenging tasks, providing insights on scale, calibration, and social bias. Parameter Efficient Fine-tuning (PEFT) #Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning - This paper provides a systematic overview of Parameter-Efficient Fine-tuning (PEFT) Methods in all three categories discussed in the lecture videos. On the Effectiveness of Parameter-Efficient Fine-Tuning - The paper analyzes sparse fine-tuning methods for pre-trained models in NLP. LoRA #LoRA Low-Rank Adaptation of Large Language Models - This paper proposes a parameter-efficient fine-tuning method that makes use of low-rank decomposition matrices to reduce the number of trainable parameters needed for fine-tuning language models. QLoRA: Efficient Finetuning of Quantized LLMs - This paper introduces an efficient method for fine-tuning large language models on a single GPU, based on quantization, achieving impressive results on benchmark tests. Prompt Tuning with Soft Prompts #The Power of Scale for Parameter-Efficient Prompt Tuning - The paper explores \u0026ldquo;prompt tuning,\u0026rdquo; a method for conditioning language models with learned soft prompts, achieving competitive performance compared to full fine-tuning and enabling model reuse for many tasks. `}),e.add({id:19,href:"/notes/statistics/",title:"Statistics",description:"Statistics",content:""}),e.add({id:20,href:"/notes/statistics/introduction/",title:"Introduction",description:`Descriptive Statistics (Exploratory Data Analysis) #Summary Statistics Used to understand and define the sample data Measures of Central Tendency Mean - average, sensitive to outliers Median - middle point of the data, robust to outliers Mode - most frequent value Measures of Dispersion Variance - average of the squared distance of each point to the mean Standard Deviation - how much our data is spread out around the mean, most commonly used Range - difference between largest and smallest observation in the data, sensitive to outliers Interquartile Range - difference between the 25th and 75th percentile of the data Don’t allow us to make conclusions beyond the data we’ve analyzed or reach conclusions regarding any hypothesis we might have made Inferential Statistics #Used to make generalizations about a population Key Make sure samples are representative Selecting a good sample is critical for making inferences about the population Sampling error `,content:`Descriptive Statistics (Exploratory Data Analysis) #Summary Statistics Used to understand and define the sample data Measures of Central Tendency Mean - average, sensitive to outliers Median - middle point of the data, robust to outliers Mode - most frequent value Measures of Dispersion Variance - average of the squared distance of each point to the mean Standard Deviation - how much our data is spread out around the mean, most commonly used Range - difference between largest and smallest observation in the data, sensitive to outliers Interquartile Range - difference between the 25th and 75th percentile of the data Don’t allow us to make conclusions beyond the data we’ve analyzed or reach conclusions regarding any hypothesis we might have made Inferential Statistics #Used to make generalizations about a population Key Make sure samples are representative Selecting a good sample is critical for making inferences about the population Sampling error `}),e.add({id:21,href:"/notes/dsa/",title:"DSA",description:"Data Structures \u0026 Algorithms",content:""}),e.add({id:22,href:"/notes/dsa/data-structures-and-algorithms/",title:"Data Structures \u0026 Algorithms",description:"Data Structures \u0026 Algorithms",content:" Data Structures Algorithms Problems "}),e.add({id:23,href:"/notes/system-design/",title:"System Design",description:"System Design",content:""}),e.add({id:24,href:"/notes/system-design/concepts/",title:"Concepts",description:"System Design",content:""}),e.add({id:25,href:"/notes/system-design/concepts/basics/",title:"Basics",description:`“Everything is a trade-off” #Distributed System → a network of computers that work together to perform task(s) Characteristics of a Distributed System Availability → % uptime (ex: 99.99%) Reliability → remain operational despite component(s) failure, implies Availability Both achieved by Replication of data and Redundancy of services Scalability / Scaling → cope with increased demand without drop in performance Vertical Scaling - get a bigger machine, increase CPU, RAM, Storage, etc.`,content:`“Everything is a trade-off” #Distributed System → a network of computers that work together to perform task(s) Characteristics of a Distributed System Availability → % uptime (ex: 99.99%) Reliability → remain operational despite component(s) failure, implies Availability Both achieved by Replication of data and Redundancy of services Scalability / Scaling → cope with increased demand without drop in performance Vertical Scaling - get a bigger machine, increase CPU, RAM, Storage, etc. Horizontal Scaling - add more machines/nodes, require Load Balancing Efficiency Latency / Response Time Throughput / Bandwidth Manageability / Maintainability / Serviceability → simplicity and speed to repair system, impacts Availability Theorems Definitions Consistency → system maintains a consistent state across all nodes Availability → system should always respond to requests regardless of the current state of the system Partition Tolerance → system continues to function even if there is a network partition Latency → time taken for system to respond to requests CAP → in the presence of a network partition, system must choose between Availability and Consistency PACELC → extension of CAP theorem, in the absence of network partitions, system must choose between Latency (Lower) and Consistency (Strong) Consistency Patterns Weak Consistency Eventual Consistency → data replicated asynchronously, takes milliseconds Strong Consistency → data replicated synchronously Availability Patterns Complementary Patterns: Fail-Over and Replication Fail-Over Active-Passive → heartbeats sent between active and passive servers, if the heartbeat is interrupted, the passive server assumes the active server’s IP address and resumes service Active-Active → both servers are managing traffic, spreading the load between them Public Facing → DNS need to know IPs of servers Internal Facing → Application logic need to know IPs of servers Replication → improve reliability and fault tolerance Single Leader, Multi Leader, Leaderless Single Leader Replication → writes/reads go to leader, reads go to followers (read-replicas), suitable for read-heavy system Multi Leader Replication → both leaders serves reads and writes and coordinate with each other on writes Leaderless Replication Synchronous, Asynchronous, Semi-Synchronous Synchronous → Strong Consistency Asynchronous → Eventual Consistency Datastores Relational / SQL → structured data, stored in tables/rows/columns, has primary key(s) and foreign key(s), able to perform joins between tables, support transactions, ACID-compliant, require atomic reads/writes, normalization Non-relational / NoSQL → semi-structured and unstructured data, BASE-compliant, denormalization Key-value → like a HashMap In-Memory Key-value Document → similar to JSON Wide Column → use tables, rows and columns but names and format of the columns can vary from row to row in the same table Graph → model relationships, use nodes (entities) and edges (relationships), think of Adjacency List, Neo4j Data Consistency Models Trade-off between Consistency and Availability ACID (Atomicity, Consistency, Isolation, Durability) → associated with relational DB, support transactions and data integrity features, need strict consistency and isolation Offer stronger consistency guarantees but may be less available BASE (Basically Available, Soft state, Eventually consistent) → associated with non-relational DB, easily horizontally scalable, need high availability and scalability Offer weaker consistency guarantees but is more available Data Replication → process of making multiple copies of data and storing them on different servers, improves Availability and Durability of data Sharding or Data Partitioning → process of distributing data across a set of servers, improves the scalability and performance of the system Horizontal Partitioning / Sharding→ store rows in different nodes, use some key to partition data, key is an attribute of the data Range-based → ranges of a key Hash-based → pass key to hash function Directory-based → Lookup table Custom-based Vertical Partitioning → store columns in different nodes Hashing → hashing function that maps key to value Consistent Hashing → Distributed Hash Table (DHT) algorithm, only n / m keys need to be remapped when a node gets added/removed (n = no. of keys, m = no. of slots) Database Indexes Proxies Reverse Proxy → intermediary between Web Servers and the Internet Forward Proxy → intermediary between clients and the Internet Load Balancing/Balancer → distribute traffic across cluster of servers (traffic police) Redundant LB (active/passive) → eliminate single Point of Failure (PoF) Traffic Distribution Perform Health Checks Naive: Round Robin, Weighted Round Robin Better: Least Response Time Location: Between User and Web Server Web Server and Internal Platform Layer (Application Servers or Cache Servers) Internal Platform Layer and Database Cache → store data temporarily in-memory Time to Live (TTL) Cache Invalidation Write-through Cache Write-around Cache Write-back Cache Cache Eviction Policies Least Recently Used (LRU) Content Delivery Network (CDN) → geographically distributed cache servers, push content closer to the users, improve performance by reducing latency (shortened physical distance) Push CDN → push content to CDN whenever changes occur on your server Pull CDN → grab new content from your server Network Protocols → help transmit data over the internet Transmission (TCP) → transport-layer protocol, ensure that data is delivered to its destination without errors and in the correct order User Datagram Protocol (UDP) → transport-layer protocol, does not guarantee the delivery of data or the order in which it is delivered, used for real-time applications (online gaming or voice over IP), low latency is more important than reliability Hypertext Transfer Protocol (HTTP) → application-layer protocol, used to transfer data over the web HTTP1 → stateless protocol, request-response model, text-based encoding, built on top of TCP HTTP2 → similar to previous version but improved: supports multiplexing (able to send multiple requests over a single connection), binary encoding, support server push, header compression HTTP3 → stateful protocol, based on UDP, new binary encoding (QUIC), server push Communication Protocols Polling → AJAX/Fetch requests at regular intervals Long Polling → like Polling, but server holds on to the connection longer until new data is available (timeout if too long without response) WebSockets → full duplex (bidirectional) connection over TCP, establish “Websocket handshake” between client and server Server Sent Events → client establishes a persistent and long-term connection with server, server sends real-time data to client (unidirectional) Architecture Patterns Monolith → deploy as a single, large, stand-alone application (single codebase) Microservices → deploy application as a collection of small, independent services that communicate with each other over the network Event-Driven Architecture (EDA) → events to trigger and communicate between decoupled services, consist of Event Producers, Event Routers (Message Brokers), and Event Consumers (Workers) Serverless → Functions that respond to requests, event-driven Components of Web-based Distributed System Clients → Mobile, Desktop, etc. Domain Name System (DNS) → map domain names to IP addresses Content Delivery Network (CDN) → geographically distributed, store static content API Gateway → authentication/authorization, route to proper services Load Balancer → traffic police Web Server(s) → handle HTTP requests and responses only Message Queue → store and transmit requests for further processing by Application Server(s) Application Server(s) → execute business logic to application programs through any number of protocols File System → store and manage files Cache Server(s) → temporary, store hot DB rows in memory Database Server(s) → read and write data to disk Popular Services Relational Database → Amazon RDS/Aurora, Azure SQL Database, Google Spanner Non-relational Database → Amazon DynamoDB, Azure CosmosDB, Google Datastore Object Storage → AWS S3, Azure Blob Storage, Google Cloud Storage Cache (in-memory datastore) → Amazon ElastiCache, Azure Cache, Google Memorystore Redis, Memcached CDN → Amazon CloudFront, Azure CDN, Google CDN Serverless → AWS Lambda, Azure Functions, Google Cloud Functions `}),e.add({id:26,href:"/notes/system-design/concepts/20-concepts/",title:"20 Concepts",description:`Notes from 20 System Design Concepts Explained in 10 Minutes
Vertical Scaling → Add more resources like RAM or CPU Horizontal Scaling → Add replicas; each server can handle a subset of requests; can almost scale infinitely; don’t need beefy machines; adds redundancy and fault tolerance; eliminates single POF Load Balancer → a server known as Reverse Proxy; redirects incoming requests to the appropriate server; Round Robin (cycle through our pool of servers); Hashing incoming request ID; goal is to even the amount of traffic each server is getting; if servers are located all around the world, can route a request to the nearest location Content Delivery Networks → serving static files (images/videos/HTML/CSS/JS); network of servers located all around the world; does not run any application logic; work by taking files from the origin server and copying them into CDN servers; an either be pull or push basis; a technique for caching Caching → creating copies of data so that it can be refetched faster in the future; making network requests can be expensive, so our browsers sometimes cache data onto our disk, reading from disk can be expensive, so computer copies it into memory, but reading from memory can be expensive, so operating system will copy a subset of it into L1, L2 or L3 CPU cache IP Address → every computer assigned an IP address which uniquely identifies a device on a network TCP / IP → the Internet Protocol Suite, includes IP, TCP and UDP, TCP: files are broken down into individual packets and sent over the Internet, arrive at the destination, packets are numbered so they can be reassembled in the right order; if packets are missing, TCP ensures they will be resent as TCP is a reliable protocol; HTTP and WebSockets are built on top of TCP Domain Name System (DNS) → largely decentralized service that works to translate a domain to its IP address; when you buy a domain from a DNS registrar, you can create a DNA A record (stands for address), and then you can the enter the IP address of your server, so when you search and your computer makes a DNS query to get the IP address, it’ll use the A record mapping to get the address and then your operating system will cache it so that it does not need to make a DNS query every single time HTTP → use this protocol to view websites as TCP is too low level; it is an application layer protocol; client-server model, client initiates a request (includes 1.`,content:`Notes from 20 System Design Concepts Explained in 10 Minutes
Vertical Scaling → Add more resources like RAM or CPU Horizontal Scaling → Add replicas; each server can handle a subset of requests; can almost scale infinitely; don’t need beefy machines; adds redundancy and fault tolerance; eliminates single POF Load Balancer → a server known as Reverse Proxy; redirects incoming requests to the appropriate server; Round Robin (cycle through our pool of servers); Hashing incoming request ID; goal is to even the amount of traffic each server is getting; if servers are located all around the world, can route a request to the nearest location Content Delivery Networks → serving static files (images/videos/HTML/CSS/JS); network of servers located all around the world; does not run any application logic; work by taking files from the origin server and copying them into CDN servers; an either be pull or push basis; a technique for caching Caching → creating copies of data so that it can be refetched faster in the future; making network requests can be expensive, so our browsers sometimes cache data onto our disk, reading from disk can be expensive, so computer copies it into memory, but reading from memory can be expensive, so operating system will copy a subset of it into L1, L2 or L3 CPU cache IP Address → every computer assigned an IP address which uniquely identifies a device on a network TCP / IP → the Internet Protocol Suite, includes IP, TCP and UDP, TCP: files are broken down into individual packets and sent over the Internet, arrive at the destination, packets are numbered so they can be reassembled in the right order; if packets are missing, TCP ensures they will be resent as TCP is a reliable protocol; HTTP and WebSockets are built on top of TCP Domain Name System (DNS) → largely decentralized service that works to translate a domain to its IP address; when you buy a domain from a DNS registrar, you can create a DNA A record (stands for address), and then you can the enter the IP address of your server, so when you search and your computer makes a DNS query to get the IP address, it’ll use the A record mapping to get the address and then your operating system will cache it so that it does not need to make a DNS query every single time HTTP → use this protocol to view websites as TCP is too low level; it is an application layer protocol; client-server model, client initiates a request (includes 1. request header - metadata about the package, 2. request body: package contents) REST → API pattern; most popular; a standardization around HTTP APIs; making them stateless and following consistent guidelines GraphQL → API pattern; introduced by Facebook in 2015; idea is to only make a single request aka a Query, and you get to choose exactly which requests you want to fetch, means you can fetch multiple resources with a single request, and don’t end up overfetching data that is not needed gRPC → API pattern, though it’s really considered a framework; released by Google in 2016; meant as an improvement over REST APIs; RPC framework mainly used for server-to-server communication, but there is also gRPC Web which allows using gRPC from the browser; performance boost comes from protocol buffers (protobuf), comparing to JSON which REST APIs mainly use, upside: data is serialized into a binary format which is usually more storage efficient and sends less data over a network, downside: JSON is much more human readable since it’s just plain text WebSockets → application layer protocol; popular for real-time chat messaging’ avoids overhead with something like Polling; supports bi-directional communication, when you get a new message it’s immediately pushed to the receiver’s device and vice-versa SQL → relational database management systems (RDBMS) like MySQL or Postgres; more efficiently store data using data structures like B trees; fast retrieval of data using SQL queries; data is stored into rows and tables; RDBMS are ACID-compliant ACID → Durability: data is stored on disk; Isolation: different concurrent transactions will not interfere with each other; Atomicity: every transaction is “All or Nothing”; Consistency: foreign key and other constraints are other key constraints are enforced NoSQL → issue with consistency led to the creation of NoSQL; consistency makes databases harder to scale, so NoSQL databases drop this constraint and the idea of relations all together; there are many types of NoSQL databases: key-value stores, document stores, graph databases; if we don’t have to enforce any foreign key constraints, that means we can break up our database and scale it horizontally with different machines Sharding → technique to break up our database and scale it horizontally with different machines; have a shard key, decide which portion of the data to put on which machine Replication → to scale database reads, make read-only copies, called Leader-Follower replication, where every write gets sent to the Leader while every read can go either to a Leader or Follower; there’s also a Leader-Leader replication, where every replica can be used for read or write, but can result in inconsistent data, best to use it where you can have a replica for every region in the world for example, it can be complex to keep replicas in sync CAP Theorem → weigh trade-offs with replicated design; given a network partition in a database, we can only choose to favor either data consistency or data availability, “pick two of three”; note that consistency means something different than the one in ACID; more complete is the PACELC Theorem Message Queues → kind of like databases, have durable storage, can be replicated for redundancy, can be sharded for scalability; have many use cases; system receiving more data than it can process, good to introduce a message queue, so our data can be persisted before we are able to process it; obtain the added benefit that different parts of our application can become decoupled For more System Design concepts, click here.
`}),e.add({id:27,href:"/notes/system-design/concepts/25-golden-rules/",title:"25 Golden Rules",description:`Notes from 25 Golden Rules for System Design Interview
If we are dealing with a read-heavy system, it\u0026rsquo;s good to consider using a Cache
If we need low latency in the system, it\u0026rsquo;s good to consider using a Cache \u0026amp; CDN
If we are dealing with a write-heavy system, it\u0026rsquo;s good to use a Message Queue for Async processing
If we need a system to be ACID complaint, we should go for RDBMS or SQL Database`,content:`Notes from 25 Golden Rules for System Design Interview
If we are dealing with a read-heavy system, it\u0026rsquo;s good to consider using a Cache
If we need low latency in the system, it\u0026rsquo;s good to consider using a Cache \u0026amp; CDN
If we are dealing with a write-heavy system, it\u0026rsquo;s good to use a Message Queue for Async processing
If we need a system to be ACID complaint, we should go for RDBMS or SQL Database
If data is unstructured \u0026amp; doesn\u0026rsquo;t require ACID properties, we should go for NO-SQL Database
If the system has complex data in the form of videos, images, files etc, we should go for Blob/Object storage
If the system requires complex pre-computation like a news feed, we should use a Message Queue \u0026amp; Cache
If the system requires searching data in high volume, we should consider using a search index, tries or a search engine like Elasticsearch
If the system requires to Scale SQL Database, we should consider using Database Sharding
If the system requires High Availability, Performance, \u0026amp; Throughput, we should consider using a Load Balancer
If the system requires faster data delivery globally, reliability, high availability, \u0026amp; performance, we should consider using a CDN
If the system has data with nodes, edges, and relationships like friend lists, \u0026amp; road connections, we should consider using a Graph Database
If the system needs scaling of various components like servers, databases, etc, we should consider using Horizontal Scaling
If the system requires high-performing database queries, we should use Database Indexes
If the system requires bulk job processing, we should consider using Batch Processing \u0026amp; Message Queues
If the system requires reducing server load and preventing DOS attacks, we should use a Rate Limiter
If the system has Microservices, we should consider using an API Gateway (Authentication, SSL Termination, Routing etc)
If the system has a single point of failure, we should implement Redundancy in that component
If the system needs to be fault-tolerant, \u0026amp; durable, we should implement Data Replication (creating multiple copies of data on different servers)
If the system needs user-to-user communication (bi-directional) in a fast way, we should use WebSockets
If the system needs the ability to detect failures in a distributed system, we should implement a Heartbeat
If the system needs to ensure data integrity, we should use Checksum Algorithm
If the system needs to transfer data between various servers in a decentralized way, we should go for the Gossip Protocol
If the system needs to scale servers with add/removal of nodes efficiently, with no hotspots, we should implement Consistent Hashing
If the system needs anything to deal with a location like maps, nearby resources, we should consider using Quadtree, Geohash, etc
`}),e.add({id:28,href:"/notes/system-design/concepts/key-steps/",title:"Key Steps",description:`Steps #Requirements / Clarifications / Minimum Viable Product (MVP) / Goals Functional Requirements Basic functionalities of the app Non-functional Requirements Availability, Consistency, Latency, Scalability… High Availability, High Reliability, Low Latency, Highly Scalable Can consistency take a hit in favor of availability/lower latency? Low latency in … Extended Requirements (recommended if you have more time) Estimation and Constraints / Back of the Envelope Calculation / Rough Estimates Read heavy? Write Heavy?`,content:`Steps #Requirements / Clarifications / Minimum Viable Product (MVP) / Goals Functional Requirements Basic functionalities of the app Non-functional Requirements Availability, Consistency, Latency, Scalability… High Availability, High Reliability, Low Latency, Highly Scalable Can consistency take a hit in favor of availability/lower latency? Low latency in … Extended Requirements (recommended if you have more time) Estimation and Constraints / Back of the Envelope Calculation / Rough Estimates Read heavy? Write Heavy? Read to Write Ratio System will focus on the more common Traffic Total Users Daily Active Users (DAU) Queries Per Second (QPS) / Requests Per Second (RPS) Bandwidth (manage traffic and balance load between servers) Storage Memory / Cache Data Model Design / Database Design Relations Type of Database Is the data relational? Require joins? Schema / Table API Design Function Signatures High Level Component Design Identify components that are needed API Gateway, Load Balancers, Multiple Application Servers Separate read and write servers Datastores Database Distributed File Storage System (photos and videos) / CDN Detailed Component Design No right answer, consider trade-offs How will we partition our data to distribute it to multiple databases? How do we handle hot active users? Storing most recent data, should we store our data in such a way that is optimized for scanning latest data How much and at which layer should we introduce caching? What components need better load balancing? Identify and Resolve Bottlenecks Is there a Single Point of Failure Enough Data Replication? Enough Copies of Services? Performance Monitoring Universal Tricks #Introduce Cache For read-heavy system Reduce load on database Multiple instances and replicas of our globally distributed cache Redis / Memcached Cache Eviction: LRU Pareto Principle: 80-20 rule CDN for static assets Geographically distributed Address latency Cache Eviction: LRU Pareto Principle: 80-20 rule Redundancy and Replication Introduce Load Balancers For horizontal scaling Consistent Hashing - useful strategy for distributed caching system and distributed hash tables Initially, RR then something Dynamic Consistent Hashing Uniformly distribute requests among different nodes such that we should be able to add or remove nodes with minimal effort High Reliability Multiple copies Multiple Datastores Relational Database Sharding / Horizontal Partitioning Multiple Read Replicas (part of handling heavy reads) Non-relational Database Easily scalable (horizontal scaling) Relational Database Read-heavy system Indexing for faster search makes columns faster to query by creating pointers to where data is stored within a database Popular Services
SQL Azure SQL Database (MySQL, PostgreSQL) NoSQL Apache Cassandra Amazon RDS Google Cloud Datastore Key-Value Store Amazon DynamoDB Key Object Store Amazon S3 (Simple Storage Service) Azure Blob Storage Google Cloud Storage Graph Database Neo4j CDN Amazon CloudFront Azure CDN Google CDN Cache Redis Memcached Search ElasticSearch Not commonly talked about
Security Other Outlines: https://github.com/jguamie/system-design/blob/master/notes/system-design-outline.md
Miscellaneous #1 byte = 8 bits
1 KB = 10^3 byte
1 MB = 10^6 byte
1 GB = 10^9 byte
1 TB = 10^12 byte
1 PB = 10^15 byte
1 EB = 10^18 byte
In UTF-8, 1 char can range from 1-4 bytes
`}),e.add({id:29,href:"/notes/system-design/design/",title:"Design",description:"System Design",content:""}),e.add({id:30,href:"/notes/system-design/design/url-shortener/",title:"URL Shortener",description:`Notes from grokking system design
Similar: TinyURL, bitly
Requirements / Goals #Functional Generate shorter and unique alias of URL, redirect users to the original URL Option to choose custom short link Short links will expire after a default/certain timespan Non-functional High availability Minimal latency (URL redirection) Alias should not be predictable Extended Analytics - total number of redirects occurred Capacity Estimation / Constraints #100:1 read to write ratio (Read-heavy system) Reads = redirection requests Writes = new URL shortenings Scale?`,content:`Notes from grokking system design
Similar: TinyURL, bitly
Requirements / Goals #Functional Generate shorter and unique alias of URL, redirect users to the original URL Option to choose custom short link Short links will expire after a default/certain timespan Non-functional High availability Minimal latency (URL redirection) Alias should not be predictable Extended Analytics - total number of redirects occurred Capacity Estimation / Constraints #100:1 read to write ratio (Read-heavy system) Reads = redirection requests Writes = new URL shortenings Scale? Thousands, millions, billions? Traffic Estimates Assume 500 M new URL shortenings / month (write) Therefore, 100 * 500 M = 50 B redirections (read) QPS New URL shortenings 500 M / (30 days * 24 hrs * 3600 s) = ~ 200 URL / s (write) URL redirections 100 * 200 URL / s = 20 K / s (read) Storage Estimates Assume storing for 5 years 500 M * 5 yrs * 12 months = 30 B objects 30 B objects * 500 bytes = 15 TB Bandwidth Estimates Write 200 new URL shortenings / s * 500 bytes = 100 KB/s Read 20 K * 500 bytes = ~10 MB/s Cache Memory Estimates 80-20 rule, 20% of URLs will generate 80% of traffic Cache 20% of requests 0.2 * 1.7 Billion * 500 bytes = ~170 GB Summary
Value New URLs (write) 200 /s URL redirections (read) 20 K/s Incoming Data (write) 100 KB/s Outgoing Data (read) 10 MB/s Storage for 5 Years 15 TB Memory for Cache 170 GB System APIs #REST service
create_url(api_dev_key: string, original_url: string, custom_alias: string, user_name: string, expire_date: string) returns shortened URL or error view_url() delete_url(api_dev_key: string, url_key: string) API key to control usage and prevent abuse Database Design #Observations Need to store billions of records Read-heavy Each record is small (\u0026lt;1000) No relationships between each record, except for storing which user created the short link Schema URL Hash varchar PK OriginalURL varchar CreationDate date ExpirationDate date User UserID int PK Name varchar Email varchar CreationDate date LastLogin date Best choice: NoSQL key-value store (DynamoDB / Cassandra / Riak), easier to scale Store billions of rows with no relationships between objects Basic System Design and Algorithm #Base62 Encode (a-zA-Z0-9), Base64 includes ‘+’ and ‘/’ Base62 Base62 62^6 = ~56 Billion Possible Strings 64^6 = ~68.7 Billion Possible Strings 62^7 = 3.5 Trillion Possible Strings 64^7 = ~281 Trillion Possible Strings MD5 / base62 encode MD5(long URL) → base62(128-bit hash value) → 20+ characters Key Generation Service (KGS) Generate unique random keys of length 6 offline and store them in a database (key-DB) Retrieve hash from key-DB No need to worry about collisions No need to perform encoding Concurrency Issues Multiple servers reading keys concurrently KGS can use two tables (key-DB): unused keys, used keys As soon as KGS hands a key to an application server, it moves it to the used keys table KGS can also have keys in memory, it can quickly provide them whenever a servers needs them As we load them in memory, move it to the used table Ensures each server gets unique keys If KGS dies before assigning all the loaded keys to some server, we will be wasting those keys - fine since we have a lot of keys Make sure not to give same key to multiple servers Must synchronize (or get a lock on) the data structure holding the keys before removing keys from it and giving them to a server Size of key-DB Base64 encoding, 68.7 N unique six letter keys Assume 1 byte to store 1 alpha-numeric character 6 characters * 68.7 B = 412 GB KGS Single PoF? Standby replica of KGS, primary server dies, standby take over Data Partitioning and Replication #Store billions of URLs Two ways to partition the DB, store data in different servers Range-based Store in partitions based on first letter of hash key Drawback: unbalanced DB servers, unequal load Hash-based Take a hash of the object we are storing, then calculate which partition to use based on the hash Can take the hash of the ‘key’ or the actual URL to determine the partition Hashing function randomly distribute URLs into different partitions (map any key to a number between 1-256), number would represent the partition in which we store our object Drawback: overloaded partitions Solution: Consistent Hashing Cache #Memcached/Redis to store full URLs with respective hashes 80-20% rule Memory Requirements 170 GB to cache 20% of daily traffic Can easily fit into one machine Alternatively, can use a couple of small servers to store all these hot URLs Eviction Policy: Least Recently Used (LRU) Use a Linked Hash Map or similar data structure Replicate cache servers to distribute load between them Whenever there is a cache miss, servers would hit the database Update the cache and pass new entry to all cache replicas Load Balancing #In three places: Between clients, application servers, database servers and cache servers Start: Round Robin (RR) Drawback: overloaded servers More intelligent load balancer solution required to avoid server overload Least Connection / Least Response / Least Bandwidth Purging or DB Cleanup #Reached expiration time Lazy cleanup Whenever a user tries to access an expired link, we delete the link and return an error Separate Cleanup Service Rune periodically removing expired links from storage and cache Considered lightweight, can run when traffic is low After removing expired link, put the key back in key-DB to be reused Telemetry #Statistics to track Country of visitor Date and time of access Web page that refers the click / Platform where page was accessed Security and Permissions #Create Private URLs or allow a particular set of users to access a URL? Store permission level (public/private) with each URL in the DB Create a separate table to store UserIDs that have permission to see a specific URL Send 401 error if no access `}),e.add({id:31,href:"/notes/system-design/design/short-media-sharing-platform/",title:"Short Media Sharing Platform",description:`Notes from grokking system design
Services: Instagram, TikTok
System Requirements / Goals #Functional Requirements Upload, download, view photos and short videos Video or photo metadata search Able to follow other users Non-functional Requirements High availability Consistency can take a hit (in the interest of availability), if a user doesn’t see a photo/video for a while, still fine High reliability (media is never lost) Acceptable latency (loading media) Capacity Estimation / Constraints #Read-heavy system Users Total Users: 500 M Daily Active Users (DAU): 1 M Storage For 1 Day: 2 M Uploaded Media * 200 KB average file size = 400 GB / day For 10 years: 400 GB / day * 365 days * 10 years = ~1425 TB High Level System Design #(Write) Upload media service (Read) View/Search media service Object storage servers - store media Database servers - store media metadata and users metadata Database Schema #Media MediaID int PK MediaPath varchar CreationDate datetime User UserID int PK Name varchar Email varchar DateOfBirth datetime CreationDate datetime LastLogin datetime UserFollow UserID1 int PK UserID2 int PK Relational Database Require joins Challenge to scale the application NoSQL Database Distributed key-value store Schema Media User UserFollow Apache Cassandra Maintain replicas for high reliability Media Data Store Distributed File Storage (HDFS) Distributed Object Storage (Amazon S3) Component Design #Split reads and writes to separate servers, avoid overload, scale independently (Microservices) Web servers have a connection limit (Assume 500, therefore can’t have more concurrent reads/writes more than this) (Write) Upload media service Slow, reads from disk (Read) View media service Faster, especially served from cache Reliability and Redundancy #High Availability and Reliability - Store multiple copies of media Multiple copies of the services, system will run even if one instance of a service dies Eliminate single PoF Data Sharding #Metadata Sharding (Options) Based on UserID (Not recommended) Based on MediaID Generate unique MediaIDs first and then find a shard number through MediaID % 10 servers Generate MediaIDs Dedicate a separate database instance to generate auto-incrementing IDs Define a table containing only an ID field As new media added into the system, insert a new row in this table and take that ID to be our MediaID of the new media Single PoF?`,content:`Notes from grokking system design
Services: Instagram, TikTok
System Requirements / Goals #Functional Requirements Upload, download, view photos and short videos Video or photo metadata search Able to follow other users Non-functional Requirements High availability Consistency can take a hit (in the interest of availability), if a user doesn’t see a photo/video for a while, still fine High reliability (media is never lost) Acceptable latency (loading media) Capacity Estimation / Constraints #Read-heavy system Users Total Users: 500 M Daily Active Users (DAU): 1 M Storage For 1 Day: 2 M Uploaded Media * 200 KB average file size = 400 GB / day For 10 years: 400 GB / day * 365 days * 10 years = ~1425 TB High Level System Design #(Write) Upload media service (Read) View/Search media service Object storage servers - store media Database servers - store media metadata and users metadata Database Schema #Media MediaID int PK MediaPath varchar CreationDate datetime User UserID int PK Name varchar Email varchar DateOfBirth datetime CreationDate datetime LastLogin datetime UserFollow UserID1 int PK UserID2 int PK Relational Database Require joins Challenge to scale the application NoSQL Database Distributed key-value store Schema Media User UserFollow Apache Cassandra Maintain replicas for high reliability Media Data Store Distributed File Storage (HDFS) Distributed Object Storage (Amazon S3) Component Design #Split reads and writes to separate servers, avoid overload, scale independently (Microservices) Web servers have a connection limit (Assume 500, therefore can’t have more concurrent reads/writes more than this) (Write) Upload media service Slow, reads from disk (Read) View media service Faster, especially served from cache Reliability and Redundancy #High Availability and Reliability - Store multiple copies of media Multiple copies of the services, system will run even if one instance of a service dies Eliminate single PoF Data Sharding #Metadata Sharding (Options) Based on UserID (Not recommended) Based on MediaID Generate unique MediaIDs first and then find a shard number through MediaID % 10 servers Generate MediaIDs Dedicate a separate database instance to generate auto-incrementing IDs Define a table containing only an ID field As new media added into the system, insert a new row in this table and take that ID to be our MediaID of the new media Single PoF? Two DB instance, one generating even numbers, one generating odd numbers Design can be extended for Users, Media-Comments Cache and Load Balancing #Need large-scale media delivery system (Global) Pareto Principle: 80-20 rule 20% of daily read volume for media is generating 80% of traffic Cache 20% of media files and metadata CDN for static assets Geographically distributed cache servers Push content closer to the user Cache for Metadata Cache hot database rows Memcached Application servers check cache first before hitting DB Cache Eviction Policy: LRU `}),e.add({id:32,href:"/notes/system-design/design/video-streaming-platform/",title:"Video Streaming Platform",description:`Notes from grokking system design
Services: Youtube, Netflix
Requirements #Functional Upload, view and share videos Search videos from titles/description Record stats (likes, number of views) Comment on videos Non-functional High Availability Consistency can take a hit in the interest of availability/lower latency, fine if an uploaded video takes a while to be available High Reliability (uploaded videos are never lost) Minimal Latency (while watching videos) Capacity Estimation / Constraints #Read-heavy system Read (View) to Write (Upload) Ratio 200:1 Traffic Total Users: 1.`,content:`Notes from grokking system design
Services: Youtube, Netflix
Requirements #Functional Upload, view and share videos Search videos from titles/description Record stats (likes, number of views) Comment on videos Non-functional High Availability Consistency can take a hit in the interest of availability/lower latency, fine if an uploaded video takes a while to be available High Reliability (uploaded videos are never lost) Minimal Latency (while watching videos) Capacity Estimation / Constraints #Read-heavy system Read (View) to Write (Upload) Ratio 200:1 Traffic Total Users: 1.5 B Daily Active Users (DAU): 800 M Assume: A user watches 5 videos / day (Read) 800 M DAU * 5 videos / day * 1 day / 86400 s = 46 K videos / s (Write) 46 K videos / s * 1/200 = 230 videos / s Storage Assume 500 hr worth of videos are uploaded / min On average, 1 min video needs 50 MB storage (videos need to be stored in multiple formats) 500 hr of videos uploaded / min * 60 min / hr * 50 MB = 1500 GB / min = 25 GB / s Ignored video compression and replication Bandwidth - 10 MB / min (Write / Ingoing) 500 hr of videos uploaded / min * 60 min / hr * 10 MB / min = 300 GB / min = 5 GB / s (Read / Outgoing) 200 * 5 GB / s = 1 TB / s System APIs #upload_video(api_dev_key, video_contents, video_title, video_description, tags[], category_id, default_language, recording_details) Parameters: api_dev_key (string): limit users video_contents (stream): video to be uploaded video_title (string) video_description (string) tags (string[]) category_id (string) default_language (string) recording_details (string): Location info Returns: result (JSON): status: successful upload returns HTTP 202 (request accepted), if successful, can send email notification as well with link link: access to the video search_video(api_dev_key, search_query, user_location, maximum_videos_to_return, page_token) Parameters: api_dev_key (string) search_query (string): search term user_location (string) maximum_videos_to_return (number) page_token (string): specify a page in the result set that should be returned Returns: result (JSON): contains information about the list of video resources matching the search query videos (array): each has video title, a thumbnail, a video creation date, and a view count stream_video(api_dev_key, video_id, offset, codec, resolution) Parameters: api_dev_key (string) video_id (string) offset (number): time offset in seconds from the beginning of video codec (string) \u0026amp; resolution (string): to support play/pause from multiple devices Returns: stream Media stream (video chunk) from given offset Database Schema #Video Metadata - SQL Video VideoID (PK), Title, Description, Size, Thumbnail, Uploader/UserID, TotalNumberOfLikes, TotalNumberOfViews Comment CommentID (PK), VideoID (FK), UserID (FK), Comment, CreationDate User Metadata - SQL UserID (PK), Name, Email, Address, Age, RegistrationDetails High Level Design #Processing Queue - videos to be dequeued for encoding, thumbnail generation and storage Encoder - encode uploaded videos into multiple formats Thumbnails Generator Video and Thumbnail Storage - distributed file storage / object storage User Datastore Video Metadata Datastore Detailed Component Design #Read-heavy system - more views than uploads Video stored using a Distributed File Storage System (HDFS) or Distributed Object Storage (S3) Maintain multiple copies of videos Microservices Architecture - separate the services, scale them independently Read Service LB - distribute traffic to different servers since we have multiple copies of videos Write Service Single Leader Replication Can cause some staleness in data, but still acceptable (takes some milliseconds to update) Video Uploads - Support resuming from the same point if connection fails, store uploaded videos on server Video Encoding - New task added to processing queue to encode video into multiple formats Thumbnails Read-heavy - Higher thumbnail views than video views Watching one video, there can be multiple other thumbnails of other videos Each thumbnail is small in size (5-10 KB) Store in Bigtable Combines multiple files into one block to store on the disk Efficient in reading a small amount of data Cache hot thumbnails to improve latencies Metadata Sharding #Based on UserID - store all data for a specific user on one server Pass UserID to hash function which will map the user to a database server Query videos for a particular user will be straightforward, pass UserID to the hash function To search videos by titles, query all servers and each server will return a set of videos, a centralized server will then aggregate and rank these results before returning them to the user Issues Performance bottleneck if a user becomes popular (a lot of queries on the server holding the user) Nonuniform distribution of data as some users may be storing a lot more that others Solution - use Consistent Hashing to balance the load between servers or repartition/redistributed the data Based on VideoID - hash function map each VideoID to a random server where we will store that video’s metadata To find videos of a user, query all servers and each server will return a set of videos, a centralized server will then aggregate and rank these results before returning them to the user This approach solves our problem of popular users but shifts it to popular videos Improve performance by caching hot videos Video Deduplication #Perform depuplication early in the process Run matching algorithms (Block Matching, Phase Correlation, etc.) to find video duplications Replace existing video if If new video higher quality If new video is longer, or existing video is subpart of new video Intelligently divide the video into smaller chunks, only upload the new parts Load Balancing #Use Consistent Hashing among cache servers Cache \u0026amp; Content Delivery Network #Cache hot database rows for metadata servers Eviction Policy: LRU, discard least recently viewed row first Pareto Principle: 80-20 rule 20% is generating 80% of traffic Cache the 20% CDN - Geographically distributed cache servers Move popular videos to CDNs CDNs replicate content in multiple places - videos will stream from a friendlier network CDN machines make heavy use of caching and can mostly serve videos out of memory Fault Tolerance #Use Consistent Hashing for distribution among DB servers Help replace dead servers Help distribute load among servers Other Details #Youtube was originally built using a relational database (MySQL) Discovered new ways to scale it, resulted in Vitess `}),e.add({id:33,href:"/notes/",title:"Notes",description:"Personal Notes",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()