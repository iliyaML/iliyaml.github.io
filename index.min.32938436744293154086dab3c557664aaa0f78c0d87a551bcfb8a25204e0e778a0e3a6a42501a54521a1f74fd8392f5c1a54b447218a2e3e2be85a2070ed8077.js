var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/notes/machine-learning/",title:"Machine Learning",description:"Machine Learning",content:""}),e.add({id:1,href:"/notes/machine-learning/k-nearest-neighbors/",title:"K-Nearest Neighbors (KNN)",description:" A non-parametric supervised learning algorithm used for both classification and regression Steps Choose K value - typically an odd number Initialization - given point Calculate distance (Euclidean) between given point and all data points in the training set Sort the results in increasing order Type Classification - Majority Vote Regression - Average Best K value Use Cross Validation and Learning Curve K-Value overfitting/underfitting Small K - Low Bias, High Variance (Overfitting) High K - High Bias, Low Variance (Underfitting) Pros Cons Easy to understand and simple to implement Suffers from Curse of Dimensionality Suitable for small datasets Requires high memory storage ",content:" A non-parametric supervised learning algorithm used for both classification and regression Steps Choose K value - typically an odd number Initialization - given point Calculate distance (Euclidean) between given point and all data points in the training set Sort the results in increasing order Type Classification - Majority Vote Regression - Average Best K value Use Cross Validation and Learning Curve K-Value overfitting/underfitting Small K - Low Bias, High Variance (Overfitting) High K - High Bias, Low Variance (Underfitting) Pros Cons Easy to understand and simple to implement Suffers from Curse of Dimensionality Suitable for small datasets Requires high memory storage "}),e.add({id:2,href:"/notes/machine-learning/linear-regression/",title:"Linear Regression",description:"A supervised learning algorithm used for regression problems Model an output variable as a linear combination of input features, finds a line (or surface) that best fits the data Formula: y_hat = W^T·X y_hat, dependent/response variable, target W^T, weights or coefficients X, independent/predictor variable(s), features Polynomial Regression: add polynomial features Assumptions Linear Relationship - a linear relationship between each predictor variable and the response variable No Multicollinearity - none of the predictor variables are highly correlated with each other Independence - each observation in the dataset is independent Homoscedasticity - residuals have constant variance at every point in the linear model Multivariate Normality - residuals of the model are normally distributed Pros Cons Highly interpretable Sensitive to outliers Fast to train Can underfit with small, high-dimensional data Approaches",content:` A supervised learning algorithm used for regression problems Model an output variable as a linear combination of input features, finds a line (or surface) that best fits the data Formula: y_hat = W^T·X y_hat, dependent/response variable, target W^T, weights or coefficients X, independent/predictor variable(s), features Polynomial Regression: add polynomial features Assumptions Linear Relationship - a linear relationship between each predictor variable and the response variable No Multicollinearity - none of the predictor variables are highly correlated with each other Independence - each observation in the dataset is independent Homoscedasticity - residuals have constant variance at every point in the linear model Multivariate Normality - residuals of the model are normally distributed Pros Cons Highly interpretable Sensitive to outliers Fast to train Can underfit with small, high-dimensional data Approaches
Ordinary Least Squares, Normal Equation (instant approach)
Gradient Descent (iterative approach)
Feature Scaling Squared Error Cost Function Model Performance Evaluation
R^2 MAE, RMSE, MAPE `}),e.add({id:3,href:"/notes/machine-learning/logistic-regression/",title:"Logistic Regression",description:" A supervised learning algorithm used for classification problems Sigmoid Function: values range from 0 - 1 Assumptions Response Variable is Binary Independence - observations in the dataset are independent of each other No Multicollinearity - none of the predictor variables are highly correlated with each other No Extreme Outliers Linear Relationship between Explanatory Variables and Logit of Response Variable Sample Size is Sufficiently Large Pros Cons Highly interpretable Can overfit with small, high-dimensional data Applicable for multi-class predictions Multiclass Classification: One-vs-Rest (OvR) Approaches Gradient Descent Binary Cross Entropy / Log Loss Cost(hθ(x),y) = −y log(hθ(x))−(1−y) log(1−hθ(x)) Model Performance Evaluation Confusion Matrix, Accuracy, Precision, Recall, F1-Score, ROC Curve ",content:" A supervised learning algorithm used for classification problems Sigmoid Function: values range from 0 - 1 Assumptions Response Variable is Binary Independence - observations in the dataset are independent of each other No Multicollinearity - none of the predictor variables are highly correlated with each other No Extreme Outliers Linear Relationship between Explanatory Variables and Logit of Response Variable Sample Size is Sufficiently Large Pros Cons Highly interpretable Can overfit with small, high-dimensional data Applicable for multi-class predictions Multiclass Classification: One-vs-Rest (OvR) Approaches Gradient Descent Binary Cross Entropy / Log Loss Cost(hθ(x),y) = −y log(hθ(x))−(1−y) log(1−hθ(x)) Model Performance Evaluation Confusion Matrix, Accuracy, Precision, Recall, F1-Score, ROC Curve "}),e.add({id:4,href:"/notes/deep-learning/",title:"Deep Learning",description:"Deep Learning",content:""}),e.add({id:5,href:"/notes/deep-learning/neural-networks/",title:"Neural Networks",description:" A supervised learning algorithm that can be used for regression and classification problems Non-linear model Components Neurons Input Layer Hidden Layer(s) Output Layer Optimizer Function Adam (Adaptive Moment Estimation) Non-linear Activation Functions ReLU, Sigmoid, TanH Softmax - often used in the output layer for multiclass classification Regularization: dropout Loss Function MSE Binary Cross Entropy (Log Loss) Categorical Cross Entropy Forward Propagation: making inference Backward Propagation (Chain Rule): computes derivatives of your cost function with respect to the parameters Pros Cons Can be used for both regression and classification problems Black box Able to solve linearly inseparable problem Require significant amount of data Computationally expensive to train Approaches Gradient Descent Batch Gradient Descent Stochastic Gradient Descent Mini-Batch Gradient Descent ",content:" A supervised learning algorithm that can be used for regression and classification problems Non-linear model Components Neurons Input Layer Hidden Layer(s) Output Layer Optimizer Function Adam (Adaptive Moment Estimation) Non-linear Activation Functions ReLU, Sigmoid, TanH Softmax - often used in the output layer for multiclass classification Regularization: dropout Loss Function MSE Binary Cross Entropy (Log Loss) Categorical Cross Entropy Forward Propagation: making inference Backward Propagation (Chain Rule): computes derivatives of your cost function with respect to the parameters Pros Cons Can be used for both regression and classification problems Black box Able to solve linearly inseparable problem Require significant amount of data Computationally expensive to train Approaches Gradient Descent Batch Gradient Descent Stochastic Gradient Descent Mini-Batch Gradient Descent "}),e.add({id:6,href:"/notes/nlp/",title:"NLP",description:"Natural Language Processing (NLP)",content:""}),e.add({id:7,href:"/notes/nlp/papers/",title:"Papers",description:`Influential Papers in the field of Natural Language Processing in the last few years.
2017 June - Transformer: Attention is All You Need [arxiv] Notes - Notebooks 2018 June - GPT: Improving Language Understanding by Generative Pre-Training [link] 2018 Oct - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [arxiv] 2019 Feb - GPT-2: Language Models are Unsupervised Multitask Learners [link] 2020 May - GPT-3: Language Models are Few-Shot Learners - [arxiv] 2022 March - InstructGPT: Training Language Models to Follow Instructions with Human Feedback [arxiv] `,content:`Influential Papers in the field of Natural Language Processing in the last few years.
2017 June - Transformer: Attention is All You Need [arxiv] Notes - Notebooks 2018 June - GPT: Improving Language Understanding by Generative Pre-Training [link] 2018 Oct - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [arxiv] 2019 Feb - GPT-2: Language Models are Unsupervised Multitask Learners [link] 2020 May - GPT-3: Language Models are Few-Shot Learners - [arxiv] 2022 March - InstructGPT: Training Language Models to Follow Instructions with Human Feedback [arxiv] `}),e.add({id:8,href:"/notes/nlp/attention-is-all-you-need/",title:"Attention is All You Need (2017)",description:`Paper: Attention is All You Need (2017)
Objective is to improve language translation tasks Introduce the Transformer model, has no recurrence or convolutions, relies solely on Self-Attention Dimensions: (batch_size, seq_length, d_model) d_model = 512, each token is converted into a vector with this dimension Attention Self-Attention or Scaled Dot Product Attention (Query, Key, Value) W_q, W_k, W_v, learned weights of Query, Key and Value Queries and Keys of dimension d_k, they have to be the same dimension Values of dimension d_v Q, K and V obtained by matrix multiplication of input, x and the Weight matrices attention(Q, K, V) = softmax( Q · K^T / sqrt( d_k ) ) · V Multi-Head Attention Transformer Architecture Consists of Encoder-Decoder Preprocessing (same for Encoder and Decoder) Tokenization - splits the sentences into tokens Input Embedding (learned) - converts each token into a vector of dimension, d Positional Encoding - adds positional encoding to the input embedding vectors, d Use sin() when the position is even, cos() when the position is odd Encoder (N = 6 identical layers), auto-encoding, maps an input sequence to a sequence of continuous representations, has two sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Multi-Head Attention (h = 8 parallel attention heads), stacked Self-Attention Queries, Keys and Values are linearly projected h times Each head, d_model / h Results of each head is concatenated and then projected Add \u0026amp; Norm, Add Residual Connection followed by Layer Normalization: LayerNorm(x + Sublayer(x)) (Sublayer) Position-wise Fully Connected Feed Forward Network Linear() → ReLU() → Linear() FFN(x) = max(0, xW1 + b1)W2 + b2 (simplified) Inner layer has dimensionality, d_ff = 2048 Add \u0026amp; Norm Decoder (N = 6 identical layers), auto-regressive (generates an output sequence one element at a time and at each step, feeds back the output to the decoder), has three sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Masked Multi-Head Attention (h = 8 parallel attention heads, nearly identical to the Encoder but includes masking), prevents position from attending to subsequent positions Add \u0026amp; Norm (Sublayer) Encoder-Decoder Attention (or Cross Attention), performs Multi-Head Attention over the output of the encoder stack Receives K and V from Encoder Add \u0026amp; Norm (Sublayer) Position-wise Fully Connected Feed Forward Network (identical to the Encoder) Add \u0026amp; Norm Linear (learned) - target language vocab_size Softmax - converts the output of the decoder to predicted next-token probabilities Advantages Performs better than previous models such as RNN, LSTM, GRU in language translation tasks Can be trained significantly faster than architectures based on recurrence or convolutions `,content:`Paper: Attention is All You Need (2017)
Objective is to improve language translation tasks Introduce the Transformer model, has no recurrence or convolutions, relies solely on Self-Attention Dimensions: (batch_size, seq_length, d_model) d_model = 512, each token is converted into a vector with this dimension Attention Self-Attention or Scaled Dot Product Attention (Query, Key, Value) W_q, W_k, W_v, learned weights of Query, Key and Value Queries and Keys of dimension d_k, they have to be the same dimension Values of dimension d_v Q, K and V obtained by matrix multiplication of input, x and the Weight matrices attention(Q, K, V) = softmax( Q · K^T / sqrt( d_k ) ) · V Multi-Head Attention Transformer Architecture Consists of Encoder-Decoder Preprocessing (same for Encoder and Decoder) Tokenization - splits the sentences into tokens Input Embedding (learned) - converts each token into a vector of dimension, d Positional Encoding - adds positional encoding to the input embedding vectors, d Use sin() when the position is even, cos() when the position is odd Encoder (N = 6 identical layers), auto-encoding, maps an input sequence to a sequence of continuous representations, has two sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Multi-Head Attention (h = 8 parallel attention heads), stacked Self-Attention Queries, Keys and Values are linearly projected h times Each head, d_model / h Results of each head is concatenated and then projected Add \u0026amp; Norm, Add Residual Connection followed by Layer Normalization: LayerNorm(x + Sublayer(x)) (Sublayer) Position-wise Fully Connected Feed Forward Network Linear() → ReLU() → Linear() FFN(x) = max(0, xW1 + b1)W2 + b2 (simplified) Inner layer has dimensionality, d_ff = 2048 Add \u0026amp; Norm Decoder (N = 6 identical layers), auto-regressive (generates an output sequence one element at a time and at each step, feeds back the output to the decoder), has three sublayers (all sublayers produce outputs of dimension d_model): Residual Connection: x (Sublayer) Masked Multi-Head Attention (h = 8 parallel attention heads, nearly identical to the Encoder but includes masking), prevents position from attending to subsequent positions Add \u0026amp; Norm (Sublayer) Encoder-Decoder Attention (or Cross Attention), performs Multi-Head Attention over the output of the encoder stack Receives K and V from Encoder Add \u0026amp; Norm (Sublayer) Position-wise Fully Connected Feed Forward Network (identical to the Encoder) Add \u0026amp; Norm Linear (learned) - target language vocab_size Softmax - converts the output of the decoder to predicted next-token probabilities Advantages Performs better than previous models such as RNN, LSTM, GRU in language translation tasks Can be trained significantly faster than architectures based on recurrence or convolutions `}),e.add({id:9,href:"/notes/nlp/state-of-gpt-2023/",title:"State of GPT (2023)",description:"State of GPT by Andrej Karpathy at Microsoft Build 2023",content:`Tuesday, May 23, 2023
References:
20230523 State of GPT by Andrej Karpathy Notes #GitHub
Training GPT Assistant Training Pipeline - consists of 3 main steps Pre-training - require significant compute (~1000 GPUs), end up with a Base model Data Collection Huge corpus of text data from the Internet (Wikipedia, Books, GitHub, etc.) High quantity/Low quality Tokenization Convert text to a list of integers 2 Example Models GPT-3-175B (2020) vs. LLaMA-65B (2023) comparison Size is not everything, LLaMA is a much better model as it has been trained much longer on much bigger dataset (1T tokens) Context length: 1k-100k tokens (working memory) Pre-training Training Process Inputs to the transformer of the shape (B, T) tensor B is the batch size, T is the maximum context length Training sequences are laid out as rows, delimited by special \u0026lt;|endoftext|\u0026gt; tokens Training Curve - loss decreases over time Base Models Learn Powerful, General Representations (GPT-1) Do pre-training then fine-tune on a particular task like sentiment classification Base Models can be Prompted into Completing Tasks (GPT-2) Started the era of prompting as these models can be tricked to do question-answering tasks using clever prompting Base Models in the Wild GPT-4 (base model not released), GPT-3 (available via API), GPT-2 (weights are released), LLaMA (not commercially licensed) Base Models are NOT ‘Assistants’ They are basically document completers, though they can be tricked into being AI Assistants using some clever prompting Supervised Fine-tuning (SFT) - require less compute than pre-training (1-100 GPUs) SFT Dataset - low quantity compared to Pre-training, but high quality QA prompt responses Reinforcement Learning from Human Feedback (RLHF) - still research/experimental territory Reward Modeling (RM) - rank outputs of a model RM Dataset RM Training Reinforcement Learning (RL) RL Training Why RLHF? - it just works better Mode Collapse Not strictly an improvement on the base models, they lose some entropy (outputs of the model lose some diversity) Assistant Models in the Wild Best Models: GPT-4, Claude 1, … Applications Human Text Generation vs. LLM Text Generation For GPT’s, it’s just a sequence of tokens, each chunk is roughly the same amount of computation work for each token Transformers are just like token simulators They do have a lot of storage (10B parameters), and a relatively large working memory (context length or window) Anything that fits into the context window is immediately available (direct access) to the Transformer through the self-attention mechanism Chain of Thought “Transformers need tokens to think” Few shot prompting - give examples that shows the Transformer that it should show its work Can elicit this behavior by adding “Let’s think step by step” in the prompt Conditions the Transformer to show its work Spread out the reasoning over many tokens Ensemble Multiple Attempts Self-Consistency - sample multiple answers as sometimes it can get unlucky with its generation of output Ask for Reflection Ask the model if it achieved the target of your prompt (works for GPT-4) Recreate Our ‘System 2’ - slower, deliberate planning part of our brain Tree of Thoughts paper - maintaining multiple completions for any given prompt, scoring them along the way and keeping the ones that are going well Chains / Agents ReAct paper AutoGPT - allow an LLM to sort of keep a task list and continue to recursively break down tasks Condition on Good Performance “LLMs don’t want to succeed. They want to imitate. You want to succeed, and you should ask for it.” Ask the model to pretend that its a leading expert in something “Let’s work this out in a step by step way to be sure we have the right answer” Conditions it on getting a right answer Tool Use / Plugins - calculator Retrieval-Augmented LLMs Load related information into the model’s working memory (context window) Recipe Take relevant documents, split them up into chunks, embed all of them, and basically get embedding vectors that represent that data Store the embeddings and chunks of text in the vector store At test time, query your vector store, fetch chunks that might be relevant to your task, and stuff them into the prompt, and the model generates the response Constrained Prompting Techniques for enforcing a certain template in the outputs of LLMs Guidance by Microsoft Fine-tuning Parameter Efficient Finetuning Techniques (PEFT) like LORA Training only small, sparse pieces of your model while most of the base model is kept clamped Works pretty well, empirically, and is cheaper RLHF still experimental Default Recommendations Achieve your top possible performance Optimize costs Use Cases - GPTs/LLMs as Copilots Use in low-stakes applications, combine with human oversight Source of inspiration, suggestions Copilots over autonomous agents GPT-4 \u0026amp; Looking Forward OpenAI API State of GPT (2023) #ANDREJ KARPATHY:
Hi, everyone. I’m happy to be here to tell you about the state of GPT. And more generally, about the rapidly growing ecosystem of large language models. So I would like to partition the talk into two parts.
In the first part, I would like to tell you about how we train GPT assistants. And then in the second part, we are going to take a look at how we can use these assistants effectively for your applications.
GPT Assistant Training Pipeline #So first, let’s take a look at the emerging recipe for how to train these assistants. And keep in mind that this is all very new and so rapidly evolving. But so far, the recipe looks something like this.
Now, this is kind of a complicated slide, so I’m going to go through it piece by piece. But roughly speaking, we have four major stages: pre-training, supervised fine tuning, reward modeling, reinforcement learning. They follow each other serially.
Now, in each stage we have a dataset that powers that stage. We have an algorithm that for our purposes will be an objective for training a neural network. And then we have a resulting model. And then there’s some notes on the bottom.
Pre-training #So the first stage we’re going to start with is the pre-training stage. Now, this stage is kind of special in this diagram, and this diagram is not to scale because this stage is where all of the computational work basically happens. This is 99% of the training compute time and also flops.
And so this is where we are dealing with internet-scale datasets with thousands of GPUs in the supercomputer and also months of training, potentially. The other three stages are fine tuning stages that are much more along the lines of some few number of GPUs and hours or days.
So let’s take a look at the pre-training stage to achieve a base model.
Data Collection #First, we’re going to gather a large amount of data. Here’s an example of what we call a data mixture that comes from this paper that was released by Meta, where they released this LLaMA-based model.
Now, you can see roughly the kinds of datasets that enter into these collections. So we have Common Crawl, which is just a web scrape, C4, which is also a Common Crawl, and then some high-quality datasets as well.
So for example, GitHub, Wikipedia, Books, Archive, Stock Exchange and so on. These are all mixed up together and then they are sampled according to some given proportions, and that forms the training sets for the neural net for the GPT.
Tokenization #Now, before we can actually train on this data, we need to go through one more pre-processing step, and that is tokenization. And this is basically a translation of the raw text that we scraped from the internet into sequences of integers, because that’s the native representation over which GPT functions.
Now, this is a lossless kind of translation between pieces of text and tokens and integers, and there are a number of algorithms for the stage. Typically, for example, you could use something like byte pair encoding which iteratively merges little text chunks and groups them into tokens.
And so here I’m showing some example chunks of these tokens, and then this is the raw integer sequence that will actually feed into a transformer.
2 Example Models #Now, here I’m showing two sort of like examples for hyperparameters that govern the stage. GPT-4, we did not release too much information about how it was trained and so on. So I’m using GPT-3’s numbers, but GPT-3 is of course a little bit old by now, about three years ago. But LLaMA is a fairly recent model from Meta.
So these are roughly the orders of magnitude that we’re dealing with when we’re doing pre-training. The vocabulary size is usually 10,000 tokens. The context length is usually something like 2,000, 4,000, or nowadays, even 100,000. And this governs the maximum number of integers that the GPT will look at when it’s trying to predict the next integer in a sequence.
You can see that roughly the number of parameters is, say, 65 billion for LLaMA. Now, even though LLaMA has only 65 parameters compared to GPT-3’s 175 billion parameters, LLaMA is a significantly more powerful model and intuitively that’s because the model is trained for significantly longer, in this case, 1.4 trillion tokens instead of just 300 billion tokens. You shouldn’t judge the power of a model just by the number of parameters that it contains.
Below, I’m showing some tables of a number of hyperparameters that typically go into specifying the transformer neural network. So the number of heads, the dimension size, number of layers and so on.
And on the bottom, I’m showing some training hyperparameters. For example, to train the 65B model, Meta used 2,000 GPUs, roughly 21 days of training, and roughly several million dollars. And so that’s the rough orders of magnitude that you should have in mind for the pre-training stage.
Pre-training #Now, when we’re actually pre-training, what happens? Roughly speaking, we are going to take our tokens and we’re going to lay them out into data batches. We have these arrays that will feed into the transformer, and these arrays are B, the batch size, and these are all independent examples stacked up in rows, and B x T, T being the maximum context length.
So in my picture I only have 10, but this is the context length, and so this could be 2,000, 4,000, et cetera. These are extremely long rows, and what we do is we take these documents, and we pack them into rows, and we delimit them with these special end-of-text tokens, basically telling the transformer where a new document begins. And so here I have a few examples of documents and then I stretched them out into into this input.
Now, we’re going to feed all of these numbers into transformer. And let me let me just focus on a single particular cell, but the same thing will happen at every every cell in this diagram.
So let’s look at the green cell. The green cell is going to take a look at all of the tokens before it, so all of the tokens in yellow. And we’re going to feed that entire context into the transformer neural network. And the transformer is going to try to predict the next token in the sequence, in this case in red.
Now, the transformer, I don’t have too much time to unfortunately, go into the full details of this neural network architecture, but it is just a large blob of neural net stuff for our purposes, and it’s got several – 10 billion parameters typically, or something like that.
And of course, as they tune these parameters, you’re getting slightly different predicted distributions for every single one of these cells. And so, for example, if our vocabulary size is 50,257 tokens, then we’re going to have that many numbers because we need to specify a probability distribution for what comes next, so that we basically have a probability for whatever may follow.
Now, in this specific example for the specific cell, 513 will come next. And so we can use this as a source of supervision to update our transformer weights. And so we’re applying this basically on every single cell in the parallel. And we keep swapping batches and we’re trying to get the transformer to make the correct predictions over what token comes next in a sequence.
Training Process #So let me show you more concretely what this looks like when you train one of these models. This is actually coming from the New York Times, and they trained a small GPT on Shakespeare. And so here’s a small snippet of Shakespeare, and they trained their GPT on it.
Now, in the beginning at initialization, the GPT starts with completely random weights, so you’re just getting completely random outputs as well. But over time, as you train the GPT longer and longer, you are getting more and more coherent and consistent sorts of samples from the model.
And the way you sample from it, of course, is you predict what comes next. You sample from that distribution and you keep feeding that back into the process and you can basically sample large sequences.
And so by the end, you see that the transformer has learned about words and where to put spaces and where to put commas and so on. And so we’re making more and more consistent predictions over time.
Training Curve #These are the kinds of plots that you’re looking at when you’re doing model pre-training. Effectively, we’re looking at the loss function over time as you train. And low loss means that our transformer is predicting the correct – is giving a higher probability to the correct next integer in a sequence.
Base Models Learn Powerful, General Representations #Now, what are we going to do with this model once we’ve trained it after a month? Well, the first thing that we noticed, we in the field, is that these models are basically in the process of language modeling, learn very powerful general representations, and it’s possible to very efficiently fine tune them for any arbitrary downstream task you might be interested in.
So as an example, if you’re interested in sentiment classification, the approach used to be that you collect a bunch of positives and negatives and then you train some kind of an NLP model for for that. But the new approach is to ignore sentiment, classification, go off and do large language model pre-training, train the large transformer and then you can only – you may only have a few examples and you can very efficiently fine tune your model for that task.
And so this works very well in practice. And the reason for this is that basically the transformer is forced to multitask a huge amount of tasks in the language modeling task, because just just in terms of predicting the next token, it’s forced to understand a lot about the structure of the text and all the different concepts therein.
So that was GPT-1.
Base Models can be Prompted into Completing Tasks #Now, around the time of GPT-2, people noticed that actually even better than fine tuning, you can actually prompt these models very effectively. So these are language models and they want to complete documents, and so you can actually trick them into performing tasks just by arranging these fake documents.
So in this example, for example, we have some passage and then we sort of like do, \u0026ldquo;QA, QA, QA,\u0026rdquo; and this is called few-shot prompt, and then we do Q, and then as the transformer is trying complete the document, it’s actually answering our question. And so this is an example of prompt engineering a base model, making it belief that it’s sort of imitating a document and it’s getting it to perform a task.
And so this kicked off, I think, the era of, I would say, prompting over fine tuning and seeing that this actually can work extremely well on a lot of problems, even without training any neural networks, fine tuning or so on.
Base Models in the Wild #Now, since then, we’ve seen an entire evolutionary tree of base models that everyone has trained. Not all of these models are available. For example, the GPT-4 base model was never released. The GPT-4 model that you might be interacting with over API is not a base model. It’s an assistant model and we’re going to cover how to get those in a bit.
The GPT-3 base model is available via the API under the named DaVinci and the GPT-2 base model is available even as weights on our GitHub repo. But currently the best available base model probably is the LLaMA series from Meta, although it is not commercially licensed.
Base Models are NOT ‘Assistants’ #Now, one thing to point out is base models are not assistants. They don’t want to make answers to your questions. They just want to complete documents. So if you tell them, \u0026ldquo;Write a poem about the bread and cheese,\u0026rdquo; it will just – you know, it will answer questions with more questions. It’s just completing what it thinks is a document.
However, you can prompt them in a specific way for base models that is more likely to work. So as an example, here’s a poem about bread and cheese, and in that case it will autocomplete correctly.
You can even trick base models into being assistants. And the way you would do this is you would create like a specific few-shot prompt that makes it look like there’s some kind of a document between a human and assistant, and they’re exchanging sort of information.
And then at the bottom you sort of put your query at the end, and the base model will sort of like condition itself into being like a helpful assistant and kind of answer. But this is not very reliable and doesn’t work super well in practice, although it can be done.
Supervised Fine-tuning (SFT) #So instead we have a different path to make actual GPT assistants, not just base model document completers. And so that takes us into supervised fine tuning. So in the supervised fine-tuning stage, we are going to collect small, but high-quality datasets. And in this case, we’re going to ask human contractors to gather data of the form prompt and ideal response. And we’re going to collect lots of these, typically tens of thousands or something like that.
And then we’re going to still do language modeling on this data. So nothing changed algorithmically. We’re just swapping out a training set. So it used to be internet documents, which is a high-quantity/low-quality, for basically QA prompt response kinds of data, and that is low-quantity/high-quality.
So we would still do language modeling. And then after training we get an SFT model, and you can actually deploy these models, and they are actual assistants, and they work to some extent.
SFT Dataset #Let me show you what an example demonstration might look like. So here’s something that a human contractor might come up with. Here’s some random prompt, \u0026ldquo;Can you write a short introduction about the relevance of the term monopsony,\u0026rdquo; or something like that? And then the contractor also writes out an ideal response.
And when they write out these responses, they are following extensive labeling documentations and they’re being asked to be helpful, truthful and harmless. These are the labeling instructions here. You probably can’t read it, and neither can I, but they’re long and this is just people following instructions and trying to complete these prompts.
So that’s what the dataset looks like. And you can train these models and this works to some extent.
Reinforcement Learning from Human Feedback (RLHF) #Reward Modeling (RM) #Now, you can actually continue the pipeline from here on and go into, RLHF, reinforcement learning from human feedback, which consists of both reward modeling and reinforcement learning.
So let me cover that and then I’ll come back to why you may want to go through the extra steps and how that compares to just SFT models.
RM Dataset #So in the reward modeling step, what we’re going to do is we’re now going to shift our data collection to be of the form of comparisons. So here’s an example of what our dataset will look like. I have the same prompt, identical prompt on the top, which is asking the assistant to write a program or a function that checks if a given string is a palindrome.
And then what we do is we take the SFT model, which we’ve already trained, and we create multiple completions. So in this case we have three completions that the model has created. And then we ask people to rank these completions.
So if you stare at this for a while, and by the way, these are very difficult things to do to compare some of these predictions, and this can take people even hours for single prompt completion pairs. But let’s say we decided that one of these is much better than the others and so on, and so we rank them. We can then follow that with something that looks very much kind of like a binary classification on all the possible pairs between these completions.
RM Training #So what we do now is we lay out our prompt in rows and the prompts is identical across all three rows here. So it’s all the same prompt, but the completion is varied, and so the yellow tokens are coming from the SFT model.
Then what we do is we append another special reward readout token at the end, and we basically only supervise the transformer at this single green token, and the transformer will predict some reward for how good that completion is for that prompt.
And so it basically makes a guess about the quality of each completion. And then once it makes a guess for every one of them, we also have the ground truth, which is telling us the ranking of them. And so we can actually enforce that some of these numbers should be much higher than others and so on. We formulate this into a loss function, and we train our model to make reward predictions that are consistent with the ground truth coming from the comparisons from all these contractors.
So this is how we train our reward model, and that allows us to score how good a completion is for a prompt.
Reinforcement Learning (RL) #Once we have a reward model, we can’t deploy this because this is not very useful as an assistant by itself, but it’s very useful for the reinforcement learning stage that follows now. Because we have a reward model, we can score the quality of any arbitrary completion for any given prompt.
So what we do during reinforcement learning is we basically get, again, a large collection of prompts and now we do reinforcement learning with respect to the reward model.
RL Training #Here’s what that looks like. We take a single prompt, we lay it out in rows, and now we use the SFT model. We use basically the model we’d like to train, which is initialized as SFT model, to create some completions in yellow. And then we append the reward token again, and we read off the reward according to the reward model, which is now kept fixed. It doesn’t change anymore.
And now, the reward model tells us the quality of every single completion for these prompts. And so, what we can do is we can now just basically apply the same language modeling loss function, but we’re currently training on the yellow tokens. And we are weighing the language modeling objective by the rewards indicated by the reward model.
As an example, in the first row, the reward model said that this is a fairly high scoring completion. And so, all the tokens that we happen to sample on the first row are going to get reinforced, and they’re going to get higher probabilities for the future. Conversely, on the second row, the reward model really did not like this completion, -1.2. And so, therefore, every single token that we sampled in that second row is going to get a slightly higher probability for the future. And we do this over and over on many prompts, on many batches. And basically, we get a policy which creates yellow tokens here, and it’s basically all of them, all of the completions here will score high according to the reward model that we trained in the previous stage.
That’s how we train. That’s what the RLHF pipeline is. And then at the end, you get a model that you could deploy. And so, as an example, ChatGPT is an RLHF model, but some other models that you might come across, for example, of the (inaudible) and so on, these are SFT models. We have base models, SFT models, and RLHF models, and that’s kind of like the state of things there.
Why RLHF? #Now why would you want to do RLHF? One answer that is kind of not that exciting is that it just works better. This comes from the instruct GPT paper. According to these experiments a while ago now, these PPO models are RLHF. And we see that they are basically just preferred in a lot of comparisons, when we give them to humans. Humans just prefer basically tokens that come from RLHF models, compared to SFT models, compared to base model that is prompted to be an assistant. And so, it just works better.
But you might ask why? Why does it work better? And I don’t think that there’s a single amazing answer that the community has really agreed on, but I will just offer one reason, potentially, and it has to do with the asymmetry between how easy computationally it is to compare versus generate.
Let’s take an example of generating a haiku. Suppose I ask a model to write a haiku about paperclips. If you’re a contractor trying to give training data, then imagine being a contractor collecting basically data for the SFT. How are you supposed to create a nice haiku for a paperclip? You might just not be very good at that, but if I give you a few examples of haikus, you might be able to appreciate some of these haikus a lot more than others. And so, judging which one of these is good is much easier task. And so, basically this asymmetry makes it so that comparisons are a better way to potentially leverage yourself, as a human and your judgment to create a slightly better model.
Mode Collapse #Now, RLHF models are not strictly an improvement on the base models, in some cases. In particular, we’ve noticed, for example, that they lose some entropy. That means that they give more (PT?) results. They can output lower variations. They can output samples with lower variation than the base model. Base model has lots of entropy and will give lots of diverse outputs.
For example, one kind of place where I still prefer to use a base model is in the setup where you basically have n things and you want to generate more things like it. And so, here is an example that I just cooked up. I want to generate cool Pokémon names. I gave it seven Pokémon names, and I asked the base model to complete the document. And it gave me a lot more Pokémon names. These are fictitious. I tried to look them up. I don’t believe there are actual Pokémons. And this is the kind of task that I think base model would be good at, because it still has lots of entropy and will give you lots of diverse, cool kind of more things that look like whatever you give it before.
Assistant Models in the Wild #Having said all that, these are kind of like the assistant models that are probably available to you at this point. There’s a team at Berkeley that ranked a lot of the available assistant models and gave them basically ELO ratings. Currently, some of the best models, of course, are GPT-4, by far, I would say, followed by Claude GPT 3.5 and then a number of models. Some of these might be available as weights, like the Kuna, Koala, etcetera. And the first three rows here, they are all RLHF models, and all of the other models, to my knowledge, are SFT models, I believe.
Applications #Okay, so that’s how we train these models on the high level. Now, I’m going to switch gears, and let’s look at how we can best apply the GPT assistant model to your problems.
Human Text Generation vs. LLM Text Generation #Now, I would like to work in something of a concrete example. Let’s work with the concrete example here. Let’s say that you are working on an article or a blog post, and you’re going to write this sentence at the end. “California’s population is 53 times that of Alaska.” For some reason, you want to compare the populations of these two states.
Think about the rich internal monologue and tool use, and how much work actually goes computationally in your brain to generate this one final sentence. Here’s maybe what that could look like in your brain.
Okay, for this next step, let me blog. Let me compare these two populations. Okay, first, obviously, I need to get both of these populations. Now, I know that I probably don’t know these populations off the top of my head, so I’m kind of like aware of what I know or don’t know of my self-knowledge, right? I do some tool use, and I go to Wikipedia and I look up California’s population and Alaska’s population.
Now I know that I should divide the two, but again, I know that dividing 39.2 by 0.74 is very unlikely to succeed. That’s not the kind of thing that I can do in my head. And so, therefore I’m going to rely on the calculator. I’m going to use a calculator, punch it in and see that the output is roughly 53. And then maybe I do some reflection and sanity checks in my brain, so that 53 makes sense. Well, that’s quite a large fraction, but then California has the most populous state, so maybe that looks okay.
Then I have all the information I might need, and now I get to the sort of creative portion of writing. I might start to write something like, “California has 53x times greater.” And then I think to myself, that’s actually really awkward phrasing. Let me actually delete that and let me try again. And so, as I’m writing, I have this separate process, almost inspecting what I’m writing and judging whether it looks good or not. And then maybe I delete and maybe I reframe it, and then maybe I’m happy with what comes out.
Basically, long story short, a ton happens under the hood in terms of your internal monologue when you create sentences like this. But what does a sentence like this look like when we are training a GPT on it?
From GPT’s perspective, this is just a sequence of tokens. GPT, when it’s reading or generating these tokens, it just goes chunk, chunk, chunk, chunk, and each chunk is roughly the same amount of computational work for each token. And these transformers are not very shallow networks. They have about 80 layers of reasoning, but 80 is still not too much. And so, this transformer is going to do its best to imitate, but of course, the process here looks very, very different from the process that you took.
In particular, in our final artifacts, in the dataset that we create and then eventually feed to LLMs, all of that internal dialog is completely stripped. And unlike you, the GPT will look at every single token and spend the same amount of compute on every one of them. And so, you can’t expect it to actually like – well, you can’t expect it to do sort of do too much work per token.
And also, in particular, basically these transformers are just like token simulators. They don’t know what they don’t know. They just imitate the next token. They don’t know what they’re good at or not good at. They just try their best to imitate the next token. They don’t reflect in the loop. They don’t sanity check anything. They don’t correct their mistakes along the way by default. They just sample token sequences. They don’t have separate inner monologue streams in their head, right? They are evaluating what’s happening.
Now, they do have some sort of cognitive advantages, I would say, and that is that they do actually have a very large fact-based knowledge across a vast number of areas because they have, say, several 10 billion parameters. It’s a lot of storage for a lot of facts, and they also, I think, have a relatively large and perfect working memory. Whatever fits into the context window is immediately available to the transformer through its internal self-attention mechanism. And so, it’s kind of like perfect memory, but it’s got that finite size. But the transformer has a very direct access to it. And so, it can lossless-ly remember anything that is inside its context window.
That’s kind of how I would compare those two. And the reason I bring all of this up is because I think to a large extent, prompting is just making up for this sort of cognitive difference between these two kind of architectures, like our brains here and LLM brains. You can look at it that way almost.
Chain of Thought #Here’s one thing that people found, for example, works pretty well in practice. Especially if your tasks require reasoning, you can’t expect the transformer to do too much reasoning per token. And so, you have to really spread out the reasoning across more and more tokens. For example, you can’t give a transformer a very complicated question and expect it to get the answer in a single token. There’s just not enough time for it. These transformers need tokens to think, quote/ unquote, I like to say sometimes.
And so, this is some of the things that work well. You may, for example, have a few short prompt that shows the transformer that it should show its work when it’s answering a question. And if you give a few examples, the transformer will imitate that template, and it will just end up working out better in terms of its evaluation.
Additionally, you can elicit this kind of behavior from the transformer by saying, let’s think step by step, because this conditions the transformer into sort of showing its work. And because it kind of snaps into a mode of showing its work, it’s going to do less computational work per token. And so, it’s more likely to succeed as a result, because it’s making slower reasoning over time.
Ensemble Multiple Attempts #Here’s another example. This one is called self-consistency. We saw that we had the ability to start writing, and then it didn’t work out. I can try again, and I can try multiple times and maybe select the one that worked best. In these kinds of approaches, you may sample not just once, but you may sample multiple times, and then have some process for finding the ones that are good, and then keeping just those samples or doing a majority vote, or something like that. Basically, these transformers in the process as they predict the next token, just like you, they can get unlucky. And they could sample not a very good token, and they can go down sort of like a blind alley in terms of reasoning.
And so, unlike you, they cannot recover from that. They are stuck with every single token they sample. And so, they will continue the sequence, even if they even know that this sequence is not going to work out. Give them the ability to look back, inspect or try to find, try to basically sample around it.
Ask for Reflection #Here’s one technique also. It turns out that, actually, LLMs, they know when they’ve screwed up. As an example, say you ask the model to generate a poem that does not rhyme, and it might give you a poem, but it actually rhymes. But it turns out that especially for the bigger models, like GPT-4, you can just ask it, did you meet the assignment? And actually, GPT-4 knows very well that it did not meet the assignment. It just kind of got unlucky in its sampling. And so, it will tell you, no, I didn’t actually meet the assignment. Here, let me try again.
But without you prompting it, it doesn’t even know. It doesn’t know to revisit, and so on. You have to make up for that in your prompts. You have to get it to check. If you don’t ask it to check, it’s not going to check by itself. It’s just a token simulator.
Recreate our ‘System 2’ #I think more generally, a lot of these techniques fall into the bucket of what I would say recreating our System 2. You might be familiar with the System 1, System 2 thinking for humans. System 1 is a fast, automatic process and I think kind of corresponds to an LLM just sampling tokens. And System 2 is the slower, deliberate planning sort of part of your brain.
And so, this is a paper actually from just last week, because the space is pretty quickly evolving. It’s called Tree of Thought, and in Tree of Thought, the authors of this paper proposed maintaining multiple completions for any given prompt. And then they are also scoring them along the way and keeping the ones that are going well, if that makes sense. And so, a lot of people are really playing around with kind of prompt engineering to basically bring back some of these abilities that we sort of have in our brain for LLMs.
Now, one thing I would like to note here is that this is not just a prompt. This is actually prompts that are, together, used with some Python glue code, because you actually have to maintain multiple prompts, and you also have to do some tree search algorithm here to like figure out which prompts to expand, etcetera. It’s a symbiosis of Python glue code and individual prompts that are called in a (wild?) loop or in a bigger algorithm.
I also think there’s a really cool parallel here to AlphaGo. AlphaGo has a policy for placing the next stone when it plays go, and this policy was trained originally by imitating humans. But in addition to this policy, it also does Monte-Carlo tree search. And basically, it will play out a number of possibilities in its head and evaluate all of them, and only keep the ones that work well. And so, I think this is kind of an equivalent of AlphaGo, but for text, if that makes sense.
Chains / Agents #Just like Tree of Thought, I think more generally, people are starting to really explore more general techniques of not just a simple question/answer prompts, but something that looks a lot more like Python glue code, stringing together many prompts.
On the right, I have an example from this paper called React, where they structure the answer to a prompt as a sequence of thought, action, observation, thought, action, observation. And it’s a full rollout, kind of a thinking process to answer the query. And in these actions, the model is also allowed to tool use.
On the left, I have an example of Auto GPT. And now, Auto GPT, by the way, is a project that I think got a lot of hype recently, but I think I still find it kind of inspirationally interesting. It’s a project that allows an LLM to sort of keep a task list and continue to recursively break down tasks. And I don’t think this currently works very well, and I would not advise people to use it in practical applications. I just think it’s something to generally take inspiration from in terms of where this is going, I think, over time.
That’s kind of like giving our model System 2 thinking.
Condition on Good Performance #The next thing that I find kind of interesting is this following sort of, I would say, almost psychological quirk of LLMs, is that LLMs don’t want to succeed. (Laughter.) They want to imitate. You want to succeed, and you should ask for it. (Laughter.) What I mean by that is when transformers are trained, they have training sets. And there can be an entire spectrum of performance qualities in their training data.
For example, there could be some kind of a prompt for some physics question or something like that, and there could be a student solution that is completely wrong, but there can also be an expert answer that is extremely right. And transformers can’t tell the difference between low – I mean, they know about low quality solutions and high quality solutions, but by default, they want to imitate all of it, because they’re just trained on language modeling. And so, at test time, you actually have to ask for a good performance.
In this example, in this paper, they tried various prompts, and let’s think step by step was very powerful, because it sort of like spread out the reasoning over many tokens. But what worked even better is, let’s work this out in a step by step way to be sure we have the right answer. And so, it’s kind of like a conditioning on getting a right answer. And this actually makes the transformer work better, because the transformer doesn’t have to now hedge its probability mass on low quality solutions, as ridiculous as that sounds.
And so, basically, feel free to ask for a strong solution. Say something like, you are a leading expert on this topic, pretend you have IQ 120, etcetera. But don’t try to ask for too much IQ because if you ask for IQ of like 400, you might be out of data distribution, or even worse, you could be in data distribution for some sci-fi stuff, and it will start to take on some sci-fi role playing or something like that. (Laughter.) You have to find like the right amount of IQ, I think. It’s got some U-shaped curve there.
Tool Use / Plugins #Next up, as we saw, when we are trying to solve problems, we know what we are good at and what we’re not good at, and we lean on tools computationally. You want to do the same potentially with your LLMs. In particular, we may want to give them calculators, code interpreters and so on, the ability to do search, and there’s a lot of techniques for doing that.
One thing to keep in mind, again, is that these transformers, by default, may not know what they don’t know. You may even want to tell the transformer in the prompt, you are not very good at mental arithmetic. Whenever you need to do very large number addition, multiplication or whatever, instead, use this calculator. Here’s how you use the calculator. Use this token combination, etcetera, etcetera. You have to actually spell it out because the model, by default, doesn’t know what it’s good at or not good at, necessarily just like you and I might be.
Retrieval-Augmented LLMs #Next up, I think something that is very interesting is we went from a world that was retrieval only. All the way, the pendulum has swung to the other extreme, where it’s memory only in LLMs. But actually, there’s this entire space in between of these retrieval augmented models, and this works extremely well in practice.
As I mentioned, the context window of a transformer is its working memory. If you can load the working memory with any information that is relevant to the task, the model will work extremely well, because it can immediately access all that memory. And so, I think a lot of people are really interested in basically retrieval augmented generation. And on the bottom, I have an example of LAMA index, which has one sort of data connector to lots of different types of data. And you can you can index all of that data, and you can make it accessible to LLMs.
And the emerging recipe there is you take relevant documents, you split them up into chunks, you embed all of them, and you basically get embedding vectors that represent that data. You store that in the vector store, and then at test time, you make some kind of a query to your vector store. And you fetch chunks that might be relevant to your task, and you stuff them into the prompt, and then you generate. This can work quite well in practice.
This is, I think, similar to when you and I solve problems. You can do everything from your memory, and transformers have very large and extensive memory, but also, it really helps to reference some primary documents. Whenever you find yourself going back to a textbook to find something or whenever you find yourself going back to the documentation of a library to look something up, the transformers definitely want to do that, too. You have some memory over how some documentation of a library works, but it’s much better to look it up. Same applies here.
Constrained Prompting #Next, I wanted to briefly talk about constraint prompting. I also find this very interesting. This is basically techniques for enforcing a certain template in the outputs of LLMs. Guidance is one example from Microsoft, actually. And here we are, enforcing that the output from the LLM will be JSON. And this will actually guarantee that the output will take on this form, because they go in and they mess with the probabilities of all the different tokens that come out of the transformer, and they clamp those tokens. And then the transformer is only filling in the blanks here. And then you can enforce additional restrictions on what could go into those blanks.
This might be really helpful, and I think this kind of constraint sampling is also extremely interesting.
Fine-tuning #I also wanted to say a few words about finetuning. It is the case that you can get really far with prompt engineering, but it’s also possible to think about finetuning your models.
Now, finetuning models means that you are actually going to change the weights of the model. It is becoming a lot more accessible to do this in practice, and that’s because of a number of techniques that have been developed and have libraries for very recently.
For example, parameter efficient finetuning techniques like LoRA, make sure that you’re only training small, sparse pieces of your model. Most of the model is kept clamped at the base model and some pieces of it are allowed to change. And it still works pretty well, empirically, and makes it much cheaper to sort of tune only small pieces of your model. It also means that because most of your model is clamped, you can use very low precision inference for computing those parts, because they are not going to be updated by gradient descent. And so, that makes everything a lot more efficient as well.
And in addition, we have a number of open sourced, high quality based models currently, as I mentioned. And I think LAMA’s quite nice, although it is not commercially licensed, I believe, right now.
Something to keep in mind is that basically, finetuning is a lot more technically involved. It requires a lot more, I think, technical expertise to do right. It requires human data contractors for data sets and/or synthetic data pipelines that can be pretty complicated. This will definitely slow down your iteration cycle by a lot.
And I would say on a high level, SFT is achievable because you’re continuing the language modeling task. It’s relatively straightforward. But RLHF, I would say, is very much research territory and is even much harder to get to work. And so, I would probably not advise that someone just tries to roll their own RLHF implementation. These things are pretty unstable, very difficult to train, not something that is, I think, very beginner friendly right now. And it’s also potentially likely also to change pretty rapidly, still.
Default Recommendations* #I think these are my sort of default recommendations right now. I would break up your task into two major parts. Number one, achieve your top performance, and number two, optimize your performance in that order.
Number one, the best performance will currently come from GPT-4 model. It is the most capable by far. Use prompts that are very detailed. They have lots of task context, relevant information and instructions. Think along the lines of what would you tell a task contractor if they can’t e-mail you back? But then also keep in mind that a task contractor is a human, and they have inner monologue, and they’re very clever, etcetera. LLMs do not possess those qualities, so make sure to think through the psychology of the LLM, almost, and cater prompts to that.
Retrieve and add any relevant context and information to these prompts, basically refer to a lot of the prompt engineering techniques. Some of them are highlighted in the slides above, but also this is a very large space, and I would just advise you to look for prompt engineering techniques online. There’s a lot to cover there.
Experiment with a few short examples. What this refers to is you don’t just want to tell, you want to show whenever it’s possible. Give it examples of everything that helps it really understand what you mean, if you can.
Experiment with tools and plugins to offload a task that are difficult for LLMs natively, and then think about not just a single prompt and answer. Think about potential change and reflection, and how you glue them together, and how you could potentially make multiple samples, and so on.
Finally, if you think you’ve squeezed out prompt engineering, which I think you should stick with for a while, look at some potentially finetuning a model to your application, but expect this to be a lot more slower and involved. And then there’s an expert fragile research zone here, and I would say that is RLHF, which currently does work a bit better than SFT, if you can get it to work. But again, this is pretty involved, I would say. And to optimize your costs, try to explore lower capacity models or shorter prompts and so on.
Use Cases #I also wanted to say a few words about the use cases in which I think LLMs are currently well-suited for. In particular, note that there’s a large number of limitations to LLMs today. And so, I would keep that definitely in mind for all your applications. And this is, by the way, could be an entire talk, so I don’t have time to cover it in full detail.
Models may be biased. They may fabricate, hallucinate information. They may have reasoning errors. They may struggle an entire classes of applications. They have knowledge cutoffs, so they might not know any information about, say, September 2021. They are susceptible to a large range of attacks, which are sort of like coming out on Twitter daily, including prompt injection, jailbreak attacks, data poisoning attacks and so on.
My recommendation right now is use LLMs in low stakes applications, combine them always with human oversight, use them as a source of inspiration and suggestions, and think copilots instead of completely autonomous agents that are just performing a task somewhere. It’s just not clear that the models are there right now.
GPT-4 \u0026amp; Looking Forward #I wanted to close by saying that GPT-4 is an amazing artifact. I’m very thankful that it exists, and it’s beautiful. It has a ton of knowledge across so many areas that can do math, code, and so on. And in addition, there’s this thriving ecosystem of everything else that is being built and incorporated into the ecosystem, some of these things I talked about. And all of this power is accessible at your fingertips.
OpenAI API #Here’s everything that’s needed in terms of code to ask GPT-4 a question, to prompt it and get a response. In this case, I said, “Can you say something to inspire the audience of Microsoft Build 2023?” And I just punched this into Python, and verbatim, GPT-4 said the following.
And by the way, I did not know that they used this trick in the keynote, so I thought I was being clever. (Laughter.) But it is really good at this.
It says, “Ladies and gentlemen, innovators and trailblazers of Microsoft Build 2023, welcome to the gathering of brilliant minds like no other. You are architects of the future, the visionaries molding the digital realm in which humanity thrives. Embrace the limitless possibilities of technologies and let your ideas soar as high as your imagination. Together, let’s create a more connected, remarkable and inclusive world for generations to come. Get ready to unleash your creativity, canvass the unknown and turn dreams into reality. Your journey begins today.”
Thank you. (Applause.)
END
`}),e.add({id:10,href:"/notes/statistics/",title:"Statistics",description:"Statistics",content:""}),e.add({id:11,href:"/notes/statistics/introduction/",title:"Introduction",description:`Descriptive Statistics (Exploratory Data Analysis) #Summary Statistics Used to understand and define the sample data Measures of Central Tendency Mean - average, sensitive to outliers Median - middle point of the data, robust to outliers Mode - most frequent value Measures of Dispersion Variance - average of the squared distance of each point to the mean Standard Deviation - how much our data is spread out around the mean, most commonly used Range - difference between largest and smallest observation in the data, sensitive to outliers Interquartile Range - difference between the 25th and 75th percentile of the data Don’t allow us to make conclusions beyond the data we’ve analyzed or reach conclusions regarding any hypothesis we might have made Inferential Statistics #Used to make generalizations about a population Key Make sure samples are representative Selecting a good sample is critical for making inferences about the population Sampling error `,content:`Descriptive Statistics (Exploratory Data Analysis) #Summary Statistics Used to understand and define the sample data Measures of Central Tendency Mean - average, sensitive to outliers Median - middle point of the data, robust to outliers Mode - most frequent value Measures of Dispersion Variance - average of the squared distance of each point to the mean Standard Deviation - how much our data is spread out around the mean, most commonly used Range - difference between largest and smallest observation in the data, sensitive to outliers Interquartile Range - difference between the 25th and 75th percentile of the data Don’t allow us to make conclusions beyond the data we’ve analyzed or reach conclusions regarding any hypothesis we might have made Inferential Statistics #Used to make generalizations about a population Key Make sure samples are representative Selecting a good sample is critical for making inferences about the population Sampling error `}),e.add({id:12,href:"/notes/dsa/",title:"DSA",description:"Data Structures \u0026 Algorithms",content:""}),e.add({id:13,href:"/notes/dsa/data-structures-and-algorithms/",title:"Data Structures \u0026 Algorithms",description:"Data Structures \u0026 Algorithms",content:" Data Structures Algorithms Problems "}),e.add({id:14,href:"/notes/system-design/",title:"System Design",description:"System Design",content:""}),e.add({id:15,href:"/notes/system-design/concepts/",title:"Concepts",description:"System Design",content:""}),e.add({id:16,href:"/notes/system-design/concepts/basics/",title:"Basics",description:`“Everything is a trade-off” #Distributed System → a network of computers that work together to perform task(s) Characteristics of a Distributed System Availability → % uptime (ex: 99.99%) Reliability → remain operational despite component(s) failure, implies Availability Both achieved by Replication of data and Redundancy of services Scalability / Scaling → cope with increased demand without drop in performance Vertical Scaling - get a bigger machine, increase CPU, RAM, Storage, etc.`,content:`“Everything is a trade-off” #Distributed System → a network of computers that work together to perform task(s) Characteristics of a Distributed System Availability → % uptime (ex: 99.99%) Reliability → remain operational despite component(s) failure, implies Availability Both achieved by Replication of data and Redundancy of services Scalability / Scaling → cope with increased demand without drop in performance Vertical Scaling - get a bigger machine, increase CPU, RAM, Storage, etc. Horizontal Scaling - add more machines/nodes, require Load Balancing Efficiency Latency / Response Time Throughput / Bandwidth Manageability / Maintainability / Serviceability → simplicity and speed to repair system, impacts Availability Theorems Definitions Consistency → system maintains a consistent state across all nodes Availability → system should always respond to requests regardless of the current state of the system Partition Tolerance → system continues to function even if there is a network partition Latency → time taken for system to respond to requests CAP → in the presence of a network partition, system must choose between Availability and Consistency PACELC → extension of CAP theorem, in the absence of network partitions, system must choose between Latency (Lower) and Consistency (Strong) Consistency Patterns Weak Consistency Eventual Consistency → data replicated asynchronously, takes milliseconds Strong Consistency → data replicated synchronously Availability Patterns Complementary Patterns: Fail-Over and Replication Fail-Over Active-Passive → heartbeats sent between active and passive servers, if the heartbeat is interrupted, the passive server assumes the active server’s IP address and resumes service Active-Active → both servers are managing traffic, spreading the load between them Public Facing → DNS need to know IPs of servers Internal Facing → Application logic need to know IPs of servers Replication → improve reliability and fault tolerance Single Leader, Multi Leader, Leaderless Single Leader Replication → writes/reads go to leader, reads go to followers (read-replicas), suitable for read-heavy system Multi Leader Replication → both leaders serves reads and writes and coordinate with each other on writes Leaderless Replication Synchronous, Asynchronous, Semi-Synchronous Synchronous → Strong Consistency Asynchronous → Eventual Consistency Datastores Relational / SQL → structured data, stored in tables/rows/columns, has primary key(s) and foreign key(s), able to perform joins between tables, support transactions, ACID-compliant, require atomic reads/writes, normalization Non-relational / NoSQL → semi-structured and unstructured data, BASE-compliant, denormalization Key-value → like a HashMap In-Memory Key-value Document → similar to JSON Wide Column → use tables, rows and columns but names and format of the columns can vary from row to row in the same table Graph → model relationships, use nodes (entities) and edges (relationships), think of Adjacency List, Neo4j Data Consistency Models Trade-off between Consistency and Availability ACID (Atomicity, Consistency, Isolation, Durability) → associated with relational DB, support transactions and data integrity features, need strict consistency and isolation Offer stronger consistency guarantees but may be less available BASE (Basically Available, Soft state, Eventually consistent) → associated with non-relational DB, easily horizontally scalable, need high availability and scalability Offer weaker consistency guarantees but is more available Data Replication → process of making multiple copies of data and storing them on different servers, improves Availability and Durability of data Sharding or Data Partitioning → process of distributing data across a set of servers, improves the scalability and performance of the system Horizontal Partitioning / Sharding→ store rows in different nodes, use some key to partition data, key is an attribute of the data Range-based → ranges of a key Hash-based → pass key to hash function Directory-based → Lookup table Custom-based Vertical Partitioning → store columns in different nodes Hashing → hashing function that maps key to value Consistent Hashing → Distributed Hash Table (DHT) algorithm, only n / m keys need to be remapped when a node gets added/removed (n = no. of keys, m = no. of slots) Database Indexes Proxies Reverse Proxy → intermediary between Web Servers and the Internet Forward Proxy → intermediary between clients and the Internet Load Balancing/Balancer → distribute traffic across cluster of servers (traffic police) Redundant LB (active/passive) → eliminate single Point of Failure (PoF) Traffic Distribution Perform Health Checks Naive: Round Robin, Weighted Round Robin Better: Least Response Time Location: Between User and Web Server Web Server and Internal Platform Layer (Application Servers or Cache Servers) Internal Platform Layer and Database Cache → store data temporarily in-memory Time to Live (TTL) Cache Invalidation Write-through Cache Write-around Cache Write-back Cache Cache Eviction Policies Least Recently Used (LRU) Content Delivery Network (CDN) → geographically distributed cache servers, push content closer to the users, improve performance by reducing latency (shortened physical distance) Push CDN → push content to CDN whenever changes occur on your server Pull CDN → grab new content from your server Network Protocols → help transmit data over the internet Transmission (TCP) → transport-layer protocol, ensure that data is delivered to its destination without errors and in the correct order User Datagram Protocol (UDP) → transport-layer protocol, does not guarantee the delivery of data or the order in which it is delivered, used for real-time applications (online gaming or voice over IP), low latency is more important than reliability Hypertext Transfer Protocol (HTTP) → application-layer protocol, used to transfer data over the web HTTP1 → stateless protocol, request-response model, text-based encoding, built on top of TCP HTTP2 → similar to previous version but improved: supports multiplexing (able to send multiple requests over a single connection), binary encoding, support server push, header compression HTTP3 → stateful protocol, based on UDP, new binary encoding (QUIC), server push Communication Protocols Polling → AJAX/Fetch requests at regular intervals Long Polling → like Polling, but server holds on to the connection longer until new data is available (timeout if too long without response) WebSockets → full duplex (bidirectional) connection over TCP, establish “Websocket handshake” between client and server Server Sent Events → client establishes a persistent and long-term connection with server, server sends real-time data to client (unidirectional) Architecture Patterns Monolith → deploy as a single, large, stand-alone application (single codebase) Microservices → deploy application as a collection of small, independent services that communicate with each other over the network Event-Driven Architecture (EDA) → events to trigger and communicate between decoupled services, consist of Event Producers, Event Routers (Message Brokers), and Event Consumers (Workers) Serverless → Functions that respond to requests, event-driven Components of Web-based Distributed System Clients → Mobile, Desktop, etc. Domain Name System (DNS) → map domain names to IP addresses Content Delivery Network (CDN) → geographically distributed, store static content API Gateway → authentication/authorization, route to proper services Load Balancer → traffic police Web Server(s) → handle HTTP requests and responses only Message Queue → store and transmit requests for further processing by Application Server(s) Application Server(s) → execute business logic to application programs through any number of protocols File System → store and manage files Cache Server(s) → temporary, store hot DB rows in memory Database Server(s) → read and write data to disk Popular Services Relational Database → Amazon RDS/Aurora, Azure SQL Database, Google Spanner Non-relational Database → Amazon DynamoDB, Azure CosmosDB, Google Datastore Object Storage → AWS S3, Azure Blob Storage, Google Cloud Storage Cache (in-memory datastore) → Amazon ElastiCache, Azure Cache, Google Memorystore Redis, Memcached CDN → Amazon CloudFront, Azure CDN, Google CDN Serverless → AWS Lambda, Azure Functions, Google Cloud Functions `}),e.add({id:17,href:"/notes/system-design/concepts/20-concepts/",title:"20 Concepts",description:`Notes from 20 System Design Concepts Explained in 10 Minutes
Vertical Scaling → Add more resources like RAM or CPU Horizontal Scaling → Add replicas; each server can handle a subset of requests; can almost scale infinitely; don’t need beefy machines; adds redundancy and fault tolerance; eliminates single POF Load Balancer → a server known as Reverse Proxy; redirects incoming requests to the appropriate server; Round Robin (cycle through our pool of servers); Hashing incoming request ID; goal is to even the amount of traffic each server is getting; if servers are located all around the world, can route a request to the nearest location Content Delivery Networks → serving static files (images/videos/HTML/CSS/JS); network of servers located all around the world; does not run any application logic; work by taking files from the origin server and copying them into CDN servers; an either be pull or push basis; a technique for caching Caching → creating copies of data so that it can be refetched faster in the future; making network requests can be expensive, so our browsers sometimes cache data onto our disk, reading from disk can be expensive, so computer copies it into memory, but reading from memory can be expensive, so operating system will copy a subset of it into L1, L2 or L3 CPU cache IP Address → every computer assigned an IP address which uniquely identifies a device on a network TCP / IP → the Internet Protocol Suite, includes IP, TCP and UDP, TCP: files are broken down into individual packets and sent over the Internet, arrive at the destination, packets are numbered so they can be reassembled in the right order; if packets are missing, TCP ensures they will be resent as TCP is a reliable protocol; HTTP and WebSockets are built on top of TCP Domain Name System (DNS) → largely decentralized service that works to translate a domain to its IP address; when you buy a domain from a DNS registrar, you can create a DNA A record (stands for address), and then you can the enter the IP address of your server, so when you search and your computer makes a DNS query to get the IP address, it’ll use the A record mapping to get the address and then your operating system will cache it so that it does not need to make a DNS query every single time HTTP → use this protocol to view websites as TCP is too low level; it is an application layer protocol; client-server model, client initiates a request (includes 1.`,content:`Notes from 20 System Design Concepts Explained in 10 Minutes
Vertical Scaling → Add more resources like RAM or CPU Horizontal Scaling → Add replicas; each server can handle a subset of requests; can almost scale infinitely; don’t need beefy machines; adds redundancy and fault tolerance; eliminates single POF Load Balancer → a server known as Reverse Proxy; redirects incoming requests to the appropriate server; Round Robin (cycle through our pool of servers); Hashing incoming request ID; goal is to even the amount of traffic each server is getting; if servers are located all around the world, can route a request to the nearest location Content Delivery Networks → serving static files (images/videos/HTML/CSS/JS); network of servers located all around the world; does not run any application logic; work by taking files from the origin server and copying them into CDN servers; an either be pull or push basis; a technique for caching Caching → creating copies of data so that it can be refetched faster in the future; making network requests can be expensive, so our browsers sometimes cache data onto our disk, reading from disk can be expensive, so computer copies it into memory, but reading from memory can be expensive, so operating system will copy a subset of it into L1, L2 or L3 CPU cache IP Address → every computer assigned an IP address which uniquely identifies a device on a network TCP / IP → the Internet Protocol Suite, includes IP, TCP and UDP, TCP: files are broken down into individual packets and sent over the Internet, arrive at the destination, packets are numbered so they can be reassembled in the right order; if packets are missing, TCP ensures they will be resent as TCP is a reliable protocol; HTTP and WebSockets are built on top of TCP Domain Name System (DNS) → largely decentralized service that works to translate a domain to its IP address; when you buy a domain from a DNS registrar, you can create a DNA A record (stands for address), and then you can the enter the IP address of your server, so when you search and your computer makes a DNS query to get the IP address, it’ll use the A record mapping to get the address and then your operating system will cache it so that it does not need to make a DNS query every single time HTTP → use this protocol to view websites as TCP is too low level; it is an application layer protocol; client-server model, client initiates a request (includes 1. request header - metadata about the package, 2. request body: package contents) REST → API pattern; most popular; a standardization around HTTP APIs; making them stateless and following consistent guidelines GraphQL → API pattern; introduced by Facebook in 2015; idea is to only make a single request aka a Query, and you get to choose exactly which requests you want to fetch, means you can fetch multiple resources with a single request, and don’t end up overfetching data that is not needed gRPC → API pattern, though it’s really considered a framework; released by Google in 2016; meant as an improvement over REST APIs; RPC framework mainly used for server-to-server communication, but there is also gRPC Web which allows using gRPC from the browser; performance boost comes from protocol buffers (protobuf), comparing to JSON which REST APIs mainly use, upside: data is serialized into a binary format which is usually more storage efficient and sends less data over a network, downside: JSON is much more human readable since it’s just plain text WebSockets → application layer protocol; popular for real-time chat messaging’ avoids overhead with something like Polling; supports bi-directional communication, when you get a new message it’s immediately pushed to the receiver’s device and vice-versa SQL → relational database management systems (RDBMS) like MySQL or Postgres; more efficiently store data using data structures like B trees; fast retrieval of data using SQL queries; data is stored into rows and tables; RDBMS are ACID-compliant ACID → Durability: data is stored on disk; Isolation: different concurrent transactions will not interfere with each other; Atomicity: every transaction is “All or Nothing”; Consistency: foreign key and other constraints are other key constraints are enforced NoSQL → issue with consistency led to the creation of NoSQL; consistency makes databases harder to scale, so NoSQL databases drop this constraint and the idea of relations all together; there are many types of NoSQL databases: key-value stores, document stores, graph databases; if we don’t have to enforce any foreign key constraints, that means we can break up our database and scale it horizontally with different machines Sharding → technique to break up our database and scale it horizontally with different machines; have a shard key, decide which portion of the data to put on which machine Replication → to scale database reads, make read-only copies, called Leader-Follower replication, where every write gets sent to the Leader while every read can go either to a Leader or Follower; there’s also a Leader-Leader replication, where every replica can be used for read or write, but can result in inconsistent data, best to use it where you can have a replica for every region in the world for example, it can be complex to keep replicas in sync CAP Theorem → weigh trade-offs with replicated design; given a network partition in a database, we can only choose to favor either data consistency or data availability, “pick two of three”; note that consistency means something different than the one in ACID; more complete is the PACELC Theorem Message Queues → kind of like databases, have durable storage, can be replicated for redundancy, can be sharded for scalability; have many use cases; system receiving more data than it can process, good to introduce a message queue, so our data can be persisted before we are able to process it; obtain the added benefit that different parts of our application can become decoupled For more System Design concepts, click here.
`}),e.add({id:18,href:"/notes/system-design/concepts/25-golden-rules/",title:"25 Golden Rules",description:`Notes from 25 Golden Rules for System Design Interview
If we are dealing with a read-heavy system, it\u0026rsquo;s good to consider using a Cache
If we need low latency in the system, it\u0026rsquo;s good to consider using a Cache \u0026amp; CDN
If we are dealing with a write-heavy system, it\u0026rsquo;s good to use a Message Queue for Async processing
If we need a system to be ACID complaint, we should go for RDBMS or SQL Database`,content:`Notes from 25 Golden Rules for System Design Interview
If we are dealing with a read-heavy system, it\u0026rsquo;s good to consider using a Cache
If we need low latency in the system, it\u0026rsquo;s good to consider using a Cache \u0026amp; CDN
If we are dealing with a write-heavy system, it\u0026rsquo;s good to use a Message Queue for Async processing
If we need a system to be ACID complaint, we should go for RDBMS or SQL Database
If data is unstructured \u0026amp; doesn\u0026rsquo;t require ACID properties, we should go for NO-SQL Database
If the system has complex data in the form of videos, images, files etc, we should go for Blob/Object storage
If the system requires complex pre-computation like a news feed, we should use a Message Queue \u0026amp; Cache
If the system requires searching data in high volume, we should consider using a search index, tries or a search engine like Elasticsearch
If the system requires to Scale SQL Database, we should consider using Database Sharding
If the system requires High Availability, Performance, \u0026amp; Throughput, we should consider using a Load Balancer
If the system requires faster data delivery globally, reliability, high availability, \u0026amp; performance, we should consider using a CDN
If the system has data with nodes, edges, and relationships like friend lists, \u0026amp; road connections, we should consider using a Graph Database
If the system needs scaling of various components like servers, databases, etc, we should consider using Horizontal Scaling
If the system requires high-performing database queries, we should use Database Indexes
If the system requires bulk job processing, we should consider using Batch Processing \u0026amp; Message Queues
If the system requires reducing server load and preventing DOS attacks, we should use a Rate Limiter
If the system has Microservices, we should consider using an API Gateway (Authentication, SSL Termination, Routing etc)
If the system has a single point of failure, we should implement Redundancy in that component
If the system needs to be fault-tolerant, \u0026amp; durable, we should implement Data Replication (creating multiple copies of data on different servers)
If the system needs user-to-user communication (bi-directional) in a fast way, we should use WebSockets
If the system needs the ability to detect failures in a distributed system, we should implement a Heartbeat
If the system needs to ensure data integrity, we should use Checksum Algorithm
If the system needs to transfer data between various servers in a decentralized way, we should go for the Gossip Protocol
If the system needs to scale servers with add/removal of nodes efficiently, with no hotspots, we should implement Consistent Hashing
If the system needs anything to deal with a location like maps, nearby resources, we should consider using Quadtree, Geohash, etc
`}),e.add({id:19,href:"/notes/system-design/concepts/key-steps/",title:"Key Steps",description:`Steps #Requirements / Clarifications / Minimum Viable Product (MVP) / Goals Functional Requirements Basic functionalities of the app Non-functional Requirements Availability, Consistency, Latency, Scalability… High Availability, High Reliability, Low Latency, Highly Scalable Can consistency take a hit in favor of availability/lower latency? Low latency in … Extended Requirements (recommended if you have more time) Estimation and Constraints / Back of the Envelope Calculation / Rough Estimates Read heavy? Write Heavy?`,content:`Steps #Requirements / Clarifications / Minimum Viable Product (MVP) / Goals Functional Requirements Basic functionalities of the app Non-functional Requirements Availability, Consistency, Latency, Scalability… High Availability, High Reliability, Low Latency, Highly Scalable Can consistency take a hit in favor of availability/lower latency? Low latency in … Extended Requirements (recommended if you have more time) Estimation and Constraints / Back of the Envelope Calculation / Rough Estimates Read heavy? Write Heavy? Read to Write Ratio System will focus on the more common Traffic Total Users Daily Active Users (DAU) Queries Per Second (QPS) / Requests Per Second (RPS) Bandwidth (manage traffic and balance load between servers) Storage Memory / Cache Data Model Design / Database Design Relations Type of Database Is the data relational? Require joins? Schema / Table API Design Function Signatures High Level Component Design Identify components that are needed API Gateway, Load Balancers, Multiple Application Servers Separate read and write servers Datastores Database Distributed File Storage System (photos and videos) / CDN Detailed Component Design No right answer, consider trade-offs How will we partition our data to distribute it to multiple databases? How do we handle hot active users? Storing most recent data, should we store our data in such a way that is optimized for scanning latest data How much and at which layer should we introduce caching? What components need better load balancing? Identify and Resolve Bottlenecks Is there a Single Point of Failure Enough Data Replication? Enough Copies of Services? Performance Monitoring Universal Tricks #Introduce Cache For read-heavy system Reduce load on database Multiple instances and replicas of our globally distributed cache Redis / Memcached Cache Eviction: LRU Pareto Principle: 80-20 rule CDN for static assets Geographically distributed Address latency Cache Eviction: LRU Pareto Principle: 80-20 rule Redundancy and Replication Introduce Load Balancers For horizontal scaling Consistent Hashing - useful strategy for distributed caching system and distributed hash tables Initially, RR then something Dynamic Consistent Hashing Uniformly distribute requests among different nodes such that we should be able to add or remove nodes with minimal effort High Reliability Multiple copies Multiple Datastores Relational Database Sharding / Horizontal Partitioning Multiple Read Replicas (part of handling heavy reads) Non-relational Database Easily scalable (horizontal scaling) Relational Database Read-heavy system Indexing for faster search makes columns faster to query by creating pointers to where data is stored within a database Popular Services
SQL Azure SQL Database (MySQL, PostgreSQL) NoSQL Apache Cassandra Amazon RDS Google Cloud Datastore Key-Value Store Amazon DynamoDB Key Object Store Amazon S3 (Simple Storage Service) Azure Blob Storage Google Cloud Storage Graph Database Neo4j CDN Amazon CloudFront Azure CDN Google CDN Cache Redis Memcached Search ElasticSearch Not commonly talked about
Security Other Outlines: https://github.com/jguamie/system-design/blob/master/notes/system-design-outline.md
Miscellaneous #1 byte = 8 bits
1 KB = 10^3 byte
1 MB = 10^6 byte
1 GB = 10^9 byte
1 TB = 10^12 byte
1 PB = 10^15 byte
1 EB = 10^18 byte
In UTF-8, 1 char can range from 1-4 bytes
`}),e.add({id:20,href:"/notes/system-design/design/",title:"Design",description:"System Design",content:""}),e.add({id:21,href:"/notes/system-design/design/url-shortener/",title:"URL Shortener",description:`Notes from grokking system design
Similar: TinyURL, bitly
Requirements / Goals #Functional Generate shorter and unique alias of URL, redirect users to the original URL Option to choose custom short link Short links will expire after a default/certain timespan Non-functional High availability Minimal latency (URL redirection) Alias should not be predictable Extended Analytics - total number of redirects occurred Capacity Estimation / Constraints #100:1 read to write ratio (Read-heavy system) Reads = redirection requests Writes = new URL shortenings Scale?`,content:`Notes from grokking system design
Similar: TinyURL, bitly
Requirements / Goals #Functional Generate shorter and unique alias of URL, redirect users to the original URL Option to choose custom short link Short links will expire after a default/certain timespan Non-functional High availability Minimal latency (URL redirection) Alias should not be predictable Extended Analytics - total number of redirects occurred Capacity Estimation / Constraints #100:1 read to write ratio (Read-heavy system) Reads = redirection requests Writes = new URL shortenings Scale? Thousands, millions, billions? Traffic Estimates Assume 500 M new URL shortenings / month (write) Therefore, 100 * 500 M = 50 B redirections (read) QPS New URL shortenings 500 M / (30 days * 24 hrs * 3600 s) = ~ 200 URL / s (write) URL redirections 100 * 200 URL / s = 20 K / s (read) Storage Estimates Assume storing for 5 years 500 M * 5 yrs * 12 months = 30 B objects 30 B objects * 500 bytes = 15 TB Bandwidth Estimates Write 200 new URL shortenings / s * 500 bytes = 100 KB/s Read 20 K * 500 bytes = ~10 MB/s Cache Memory Estimates 80-20 rule, 20% of URLs will generate 80% of traffic Cache 20% of requests 0.2 * 1.7 Billion * 500 bytes = ~170 GB Summary
Value New URLs (write) 200 /s URL redirections (read) 20 K/s Incoming Data (write) 100 KB/s Outgoing Data (read) 10 MB/s Storage for 5 Years 15 TB Memory for Cache 170 GB System APIs #REST service
create_url(api_dev_key: string, original_url: string, custom_alias: string, user_name: string, expire_date: string) returns shortened URL or error view_url() delete_url(api_dev_key: string, url_key: string) API key to control usage and prevent abuse Database Design #Observations Need to store billions of records Read-heavy Each record is small (\u0026lt;1000) No relationships between each record, except for storing which user created the short link Schema URL Hash varchar PK OriginalURL varchar CreationDate date ExpirationDate date User UserID int PK Name varchar Email varchar CreationDate date LastLogin date Best choice: NoSQL key-value store (DynamoDB / Cassandra / Riak), easier to scale Store billions of rows with no relationships between objects Basic System Design and Algorithm #Base62 Encode (a-zA-Z0-9), Base64 includes ‘+’ and ‘/’ Base62 Base62 62^6 = ~56 Billion Possible Strings 64^6 = ~68.7 Billion Possible Strings 62^7 = 3.5 Trillion Possible Strings 64^7 = ~281 Trillion Possible Strings MD5 / base62 encode MD5(long URL) → base62(128-bit hash value) → 20+ characters Key Generation Service (KGS) Generate unique random keys of length 6 offline and store them in a database (key-DB) Retrieve hash from key-DB No need to worry about collisions No need to perform encoding Concurrency Issues Multiple servers reading keys concurrently KGS can use two tables (key-DB): unused keys, used keys As soon as KGS hands a key to an application server, it moves it to the used keys table KGS can also have keys in memory, it can quickly provide them whenever a servers needs them As we load them in memory, move it to the used table Ensures each server gets unique keys If KGS dies before assigning all the loaded keys to some server, we will be wasting those keys - fine since we have a lot of keys Make sure not to give same key to multiple servers Must synchronize (or get a lock on) the data structure holding the keys before removing keys from it and giving them to a server Size of key-DB Base64 encoding, 68.7 N unique six letter keys Assume 1 byte to store 1 alpha-numeric character 6 characters * 68.7 B = 412 GB KGS Single PoF? Standby replica of KGS, primary server dies, standby take over Data Partitioning and Replication #Store billions of URLs Two ways to partition the DB, store data in different servers Range-based Store in partitions based on first letter of hash key Drawback: unbalanced DB servers, unequal load Hash-based Take a hash of the object we are storing, then calculate which partition to use based on the hash Can take the hash of the ‘key’ or the actual URL to determine the partition Hashing function randomly distribute URLs into different partitions (map any key to a number between 1-256), number would represent the partition in which we store our object Drawback: overloaded partitions Solution: Consistent Hashing Cache #Memcached/Redis to store full URLs with respective hashes 80-20% rule Memory Requirements 170 GB to cache 20% of daily traffic Can easily fit into one machine Alternatively, can use a couple of small servers to store all these hot URLs Eviction Policy: Least Recently Used (LRU) Use a Linked Hash Map or similar data structure Replicate cache servers to distribute load between them Whenever there is a cache miss, servers would hit the database Update the cache and pass new entry to all cache replicas Load Balancing #In three places: Between clients, application servers, database servers and cache servers Start: Round Robin (RR) Drawback: overloaded servers More intelligent load balancer solution required to avoid server overload Least Connection / Least Response / Least Bandwidth Purging or DB Cleanup #Reached expiration time Lazy cleanup Whenever a user tries to access an expired link, we delete the link and return an error Separate Cleanup Service Rune periodically removing expired links from storage and cache Considered lightweight, can run when traffic is low After removing expired link, put the key back in key-DB to be reused Telemetry #Statistics to track Country of visitor Date and time of access Web page that refers the click / Platform where page was accessed Security and Permissions #Create Private URLs or allow a particular set of users to access a URL? Store permission level (public/private) with each URL in the DB Create a separate table to store UserIDs that have permission to see a specific URL Send 401 error if no access `}),e.add({id:22,href:"/notes/system-design/design/short-media-sharing-platform/",title:"Short Media Sharing Platform",description:`Notes from grokking system design
Services: Instagram, TikTok
System Requirements / Goals #Functional Requirements Upload, download, view photos and short videos Video or photo metadata search Able to follow other users Non-functional Requirements High availability Consistency can take a hit (in the interest of availability), if a user doesn’t see a photo/video for a while, still fine High reliability (media is never lost) Acceptable latency (loading media) Capacity Estimation / Constraints #Read-heavy system Users Total Users: 500 M Daily Active Users (DAU): 1 M Storage For 1 Day: 2 M Uploaded Media * 200 KB average file size = 400 GB / day For 10 years: 400 GB / day * 365 days * 10 years = ~1425 TB High Level System Design #(Write) Upload media service (Read) View/Search media service Object storage servers - store media Database servers - store media metadata and users metadata Database Schema #Media MediaID int PK MediaPath varchar CreationDate datetime User UserID int PK Name varchar Email varchar DateOfBirth datetime CreationDate datetime LastLogin datetime UserFollow UserID1 int PK UserID2 int PK Relational Database Require joins Challenge to scale the application NoSQL Database Distributed key-value store Schema Media User UserFollow Apache Cassandra Maintain replicas for high reliability Media Data Store Distributed File Storage (HDFS) Distributed Object Storage (Amazon S3) Component Design #Split reads and writes to separate servers, avoid overload, scale independently (Microservices) Web servers have a connection limit (Assume 500, therefore can’t have more concurrent reads/writes more than this) (Write) Upload media service Slow, reads from disk (Read) View media service Faster, especially served from cache Reliability and Redundancy #High Availability and Reliability - Store multiple copies of media Multiple copies of the services, system will run even if one instance of a service dies Eliminate single PoF Data Sharding #Metadata Sharding (Options) Based on UserID (Not recommended) Based on MediaID Generate unique MediaIDs first and then find a shard number through MediaID % 10 servers Generate MediaIDs Dedicate a separate database instance to generate auto-incrementing IDs Define a table containing only an ID field As new media added into the system, insert a new row in this table and take that ID to be our MediaID of the new media Single PoF?`,content:`Notes from grokking system design
Services: Instagram, TikTok
System Requirements / Goals #Functional Requirements Upload, download, view photos and short videos Video or photo metadata search Able to follow other users Non-functional Requirements High availability Consistency can take a hit (in the interest of availability), if a user doesn’t see a photo/video for a while, still fine High reliability (media is never lost) Acceptable latency (loading media) Capacity Estimation / Constraints #Read-heavy system Users Total Users: 500 M Daily Active Users (DAU): 1 M Storage For 1 Day: 2 M Uploaded Media * 200 KB average file size = 400 GB / day For 10 years: 400 GB / day * 365 days * 10 years = ~1425 TB High Level System Design #(Write) Upload media service (Read) View/Search media service Object storage servers - store media Database servers - store media metadata and users metadata Database Schema #Media MediaID int PK MediaPath varchar CreationDate datetime User UserID int PK Name varchar Email varchar DateOfBirth datetime CreationDate datetime LastLogin datetime UserFollow UserID1 int PK UserID2 int PK Relational Database Require joins Challenge to scale the application NoSQL Database Distributed key-value store Schema Media User UserFollow Apache Cassandra Maintain replicas for high reliability Media Data Store Distributed File Storage (HDFS) Distributed Object Storage (Amazon S3) Component Design #Split reads and writes to separate servers, avoid overload, scale independently (Microservices) Web servers have a connection limit (Assume 500, therefore can’t have more concurrent reads/writes more than this) (Write) Upload media service Slow, reads from disk (Read) View media service Faster, especially served from cache Reliability and Redundancy #High Availability and Reliability - Store multiple copies of media Multiple copies of the services, system will run even if one instance of a service dies Eliminate single PoF Data Sharding #Metadata Sharding (Options) Based on UserID (Not recommended) Based on MediaID Generate unique MediaIDs first and then find a shard number through MediaID % 10 servers Generate MediaIDs Dedicate a separate database instance to generate auto-incrementing IDs Define a table containing only an ID field As new media added into the system, insert a new row in this table and take that ID to be our MediaID of the new media Single PoF? Two DB instance, one generating even numbers, one generating odd numbers Design can be extended for Users, Media-Comments Cache and Load Balancing #Need large-scale media delivery system (Global) Pareto Principle: 80-20 rule 20% of daily read volume for media is generating 80% of traffic Cache 20% of media files and metadata CDN for static assets Geographically distributed cache servers Push content closer to the user Cache for Metadata Cache hot database rows Memcached Application servers check cache first before hitting DB Cache Eviction Policy: LRU `}),e.add({id:23,href:"/notes/system-design/design/video-streaming-platform/",title:"Video Streaming Platform",description:`Notes from grokking system design
Services: Youtube, Netflix
Requirements #Functional Upload, view and share videos Search videos from titles/description Record stats (likes, number of views) Comment on videos Non-functional High Availability Consistency can take a hit in the interest of availability/lower latency, fine if an uploaded video takes a while to be available High Reliability (uploaded videos are never lost) Minimal Latency (while watching videos) Capacity Estimation / Constraints #Read-heavy system Read (View) to Write (Upload) Ratio 200:1 Traffic Total Users: 1.`,content:`Notes from grokking system design
Services: Youtube, Netflix
Requirements #Functional Upload, view and share videos Search videos from titles/description Record stats (likes, number of views) Comment on videos Non-functional High Availability Consistency can take a hit in the interest of availability/lower latency, fine if an uploaded video takes a while to be available High Reliability (uploaded videos are never lost) Minimal Latency (while watching videos) Capacity Estimation / Constraints #Read-heavy system Read (View) to Write (Upload) Ratio 200:1 Traffic Total Users: 1.5 B Daily Active Users (DAU): 800 M Assume: A user watches 5 videos / day (Read) 800 M DAU * 5 videos / day * 1 day / 86400 s = 46 K videos / s (Write) 46 K videos / s * 1/200 = 230 videos / s Storage Assume 500 hr worth of videos are uploaded / min On average, 1 min video needs 50 MB storage (videos need to be stored in multiple formats) 500 hr of videos uploaded / min * 60 min / hr * 50 MB = 1500 GB / min = 25 GB / s Ignored video compression and replication Bandwidth - 10 MB / min (Write / Ingoing) 500 hr of videos uploaded / min * 60 min / hr * 10 MB / min = 300 GB / min = 5 GB / s (Read / Outgoing) 200 * 5 GB / s = 1 TB / s System APIs #upload_video(api_dev_key, video_contents, video_title, video_description, tags[], category_id, default_language, recording_details) Parameters: api_dev_key (string): limit users video_contents (stream): video to be uploaded video_title (string) video_description (string) tags (string[]) category_id (string) default_language (string) recording_details (string): Location info Returns: result (JSON): status: successful upload returns HTTP 202 (request accepted), if successful, can send email notification as well with link link: access to the video search_video(api_dev_key, search_query, user_location, maximum_videos_to_return, page_token) Parameters: api_dev_key (string) search_query (string): search term user_location (string) maximum_videos_to_return (number) page_token (string): specify a page in the result set that should be returned Returns: result (JSON): contains information about the list of video resources matching the search query videos (array): each has video title, a thumbnail, a video creation date, and a view count stream_video(api_dev_key, video_id, offset, codec, resolution) Parameters: api_dev_key (string) video_id (string) offset (number): time offset in seconds from the beginning of video codec (string) \u0026amp; resolution (string): to support play/pause from multiple devices Returns: stream Media stream (video chunk) from given offset Database Schema #Video Metadata - SQL Video VideoID (PK), Title, Description, Size, Thumbnail, Uploader/UserID, TotalNumberOfLikes, TotalNumberOfViews Comment CommentID (PK), VideoID (FK), UserID (FK), Comment, CreationDate User Metadata - SQL UserID (PK), Name, Email, Address, Age, RegistrationDetails High Level Design #Processing Queue - videos to be dequeued for encoding, thumbnail generation and storage Encoder - encode uploaded videos into multiple formats Thumbnails Generator Video and Thumbnail Storage - distributed file storage / object storage User Datastore Video Metadata Datastore Detailed Component Design #Read-heavy system - more views than uploads Video stored using a Distributed File Storage System (HDFS) or Distributed Object Storage (S3) Maintain multiple copies of videos Microservices Architecture - separate the services, scale them independently Read Service LB - distribute traffic to different servers since we have multiple copies of videos Write Service Single Leader Replication Can cause some staleness in data, but still acceptable (takes some milliseconds to update) Video Uploads - Support resuming from the same point if connection fails, store uploaded videos on server Video Encoding - New task added to processing queue to encode video into multiple formats Thumbnails Read-heavy - Higher thumbnail views than video views Watching one video, there can be multiple other thumbnails of other videos Each thumbnail is small in size (5-10 KB) Store in Bigtable Combines multiple files into one block to store on the disk Efficient in reading a small amount of data Cache hot thumbnails to improve latencies Metadata Sharding #Based on UserID - store all data for a specific user on one server Pass UserID to hash function which will map the user to a database server Query videos for a particular user will be straightforward, pass UserID to the hash function To search videos by titles, query all servers and each server will return a set of videos, a centralized server will then aggregate and rank these results before returning them to the user Issues Performance bottleneck if a user becomes popular (a lot of queries on the server holding the user) Nonuniform distribution of data as some users may be storing a lot more that others Solution - use Consistent Hashing to balance the load between servers or repartition/redistributed the data Based on VideoID - hash function map each VideoID to a random server where we will store that video’s metadata To find videos of a user, query all servers and each server will return a set of videos, a centralized server will then aggregate and rank these results before returning them to the user This approach solves our problem of popular users but shifts it to popular videos Improve performance by caching hot videos Video Deduplication #Perform depuplication early in the process Run matching algorithms (Block Matching, Phase Correlation, etc.) to find video duplications Replace existing video if If new video higher quality If new video is longer, or existing video is subpart of new video Intelligently divide the video into smaller chunks, only upload the new parts Load Balancing #Use Consistent Hashing among cache servers Cache \u0026amp; Content Delivery Network #Cache hot database rows for metadata servers Eviction Policy: LRU, discard least recently viewed row first Pareto Principle: 80-20 rule 20% is generating 80% of traffic Cache the 20% CDN - Geographically distributed cache servers Move popular videos to CDNs CDNs replicate content in multiple places - videos will stream from a friendlier network CDN machines make heavy use of caching and can mostly serve videos out of memory Fault Tolerance #Use Consistent Hashing for distribution among DB servers Help replace dead servers Help distribute load among servers Other Details #Youtube was originally built using a relational database (MySQL) Discovered new ways to scale it, resulted in Vitess `}),e.add({id:24,href:"/notes/",title:"Notes",description:"Personal Notes",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()